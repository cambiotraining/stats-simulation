---
title: "Simulation basics"
output: html_document
---

```{r, source file}
#| echo: false
#| message: false
#| results: false
source(file = "setup_files/setup.R")
```

```{python, source file2}
#| echo: false
#| message: false
#| results: false
exec(open('setup_files/setup.py').read())
import shutup;shutup.please()
```

This introductory chapter teaches a number of key programming skills, and a handful of important functions, that will be required throughout the course.

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

```{r, required libraries}
#| eval: false

library(tidyverse)
```

## Python

```{python, import libraries}
#| eval: false

import pandas as pd
import numpy as np
import random
import statistics
import matplotlib.pyplot as plt
from plotnine import *
```
:::
:::

## Drawing samples from distributions

The first thing we need to get comfortable with is random sampling, i.e., drawing a number of data points from an underlying distribution with known parameters.

Parameters are features of a distribution that determine its shape. For example, the normal distribution has two parameters: mean and variance (or standard deviation).

This random sampling is what we're (hopefully) doing when we collect a sample in experimental research. The underlying distribution of the response variable, i.e., the "ground truth" in the world, has some true parameters that we're hoping to estimate. We collect a random subset of individual observations that have come from that underlying distribution, and use the sample's statistics to estimate the true population parameters.

### Sampling from the normal distribution

::: {.panel-tabset group="language"}
## R

This is the `rnorm` function, which we'll be using a lot in this course. It takes three arguments; the first is the number of data points (`n`) that you'd like to draw. The second and third arguments are the two important parameters that describe the shape of the underlying distribution: the mean and the standard deviation.

```{r, rnorm for the first time}
rnorm(n = 100, mean = 0, sd = 1)
```

Without any further instruction, R simply prints us a list of length `n`; this list contains numbers that have been pulled randomly from a normal distribution with mean 0 and standard deviation 1. 

This is a sample or dataset, drawn from an underlying population, which we can now visualise.

We'll use the base R `hist` function for this for now, just to keep things simple:

```{r, rnorm piped to hist}
rnorm(100, 0, 1) %>%
  hist()
```

## Python

The typical default method in Python for sampling from a normal distribution is via `numpy.random.normal`.

It takes three arguments: `loc` (mean), `scale` (standard deviation) and `size` (sample size):

```{python, np.random.normal first time}
norm_data = np.random.normal(loc = 0, scale = 1, size = 100)
print(norm_data)
```

The output is an array of numbers, of length `size`.

This is a sample or dataset, drawn from an underlying population, which we can now visualise.

For this first example, we'll show how to use both `matplotlib` and `plotnine` to create histograms. We'll use the `matplotlib` version for speed as we go through the course, but if you're transitioning over from R (or `ggplot`), you might find `plotnine` friendlier.

```{python, plot hist matplot}
plt.hist(norm_data)
plt.show()
```

If plotting multiple histograms, you can use `plt.clf()` to clear the figure, or `plt.close()` to close the plotting window entirely.

If using `plotnine`, you have to convert the array to a data frame before you can visualise it:

```{python, plot hist plotnine}
norm_df = pd.DataFrame({"values":norm_data})

norm_df_hist = (
  ggplot(norm_df, aes(x = "values")) +
  geom_histogram()
)

print(norm_df_hist)
```
:::

### Sampling from other distributions

The normal/Gaussian distribution might be the most famous of the distributions, but it is not the only one that exists - nor the only one that we'll care about on this course.

For example, we'll also be sampling quite regularly from the uniform distribution.

The uniform distribution is flat: inside the range of possible values, all values are equally likely, and outside that range, the probability density drops to zero. This means the only parameters we need to set are the minimum and maximum of that range of possible values, like so:

::: {.panel-tabset group="language"}
## R

```{r, runif piped to hist}
runif(n = 100, min = 0, max = 1) %>%
  hist(xlim = c(-0.5, 1.5))
```

## Python

```{python, uniform dist histogram}
unif_data = np.random.uniform(low = 0, high = 1, size = 100)

plt.clf() # clear existing plot, if applicable
plt.hist(unif_data)
plt.show()
```
:::

The underlying shape of the distribution that we just sampled from looks like this - square and boxy. The probability density is zero outside of the 0-1 range, and flat inside it:

```{r, visualise unif dist}
#| echo: false
x <- seq(-0.5, 1.5, length=2000)
y <- dunif(x, min = 0, max = 1)
plot(x, y, type = 'l', col='#0072CF', lwd = 3,
     xlab='x', ylab='Probability density', main='The uniform distribution')
```

Later in this course, we will sample from the binomial, negative binomial and Poisson distributions as well.

## Setting a seed

What happens if you run this block of code over and over again?

::: {.panel-tabset group="language"}
## R

```{r, rnorm piped to hist again}
rnorm(100, 0, 1) %>%
  hist()
```

## Python
```{python, hist of multiple norm}
plt.clf()
plt.hist(np.random.normal(0, 1, 100))
plt.show()
```
:::

Each time you run the code, you are sampling a unique random subset of data points.

It's very helpful that we can do that - later on in this course, we'll exploit this to sample many different datasets from the same underlying population.

However, sometimes it's useful for us to be able to sample the *exact* set of data points more than once.

::: {.panel-tabset group="language"}
## R

To achieve this, we can use the `set.seed` function.

Run the following code several times in a row, and you'll see the difference:

```{r, rnorm with seed}
set.seed(20)

rnorm(100, 0, 1) %>%
  hist()
```

## Python

To achieve this, we can use the `np.random.seed` function.

Run the following code several times in a row, and you'll see the difference:

```{python, np.norm with np.seed}
np.random.seed(20)

plt.clf()
plt.hist(np.random.normal(0, 1, 100))
plt.show()
```

:::

Notice how each time, the exact same dataset and histogram are produced?

You can choose any number you like for the seed. All that matters is that you return to that same seed number, if you want to recreate that dataset.

## Soft-coding parameters

In the code above, we have "hard-coded" our parameters by putting them directly inside the `rnorm` or `np.random.normal` functions.

However, from here on, we will start "soft-coding" parameters or other values that we might want to change.

This is considered good programming practice. It's also sometimes called "dynamic coding".

::: {.panel-tabset group="language"}
## R

```{r, softcode rnorm}
n <- 100
mean_n <- 0
sd_n <- 1

rnorm(n = n, mean = mean_n, sd = sd_n) %>%
  hist()
```

## Python

```{python, softcode np.random.norm}
n = 100
mean = 0
sd = 1

plt.clf()
plt.hist(np.random.normal(loc = mean, scale = sd, size = n))
plt.show()
```
:::

This might look like we're writing out more code, but it will be helpful in more complex simulations where we use the same parameter more than once.

We're also separating out the bits of the code that might need to be edited, which are all at the top where we can more easily see them, versus the bits we can leave untouched. 

## Loops

In programming, a loop is a chunk of code that is run repeatedly, until a certain condition is satisfied.

There are two broad types of loop: the for loop and the while loop. For the purposes of this course, we'll only really worry about the for loop. For loops run for a pre-specified number of iterations before stopping.

### For loop syntax

::: {.panel-tabset group="language"}
## R

In R, the syntax for a for loop looks like this:

```{r, simple for loop r}
for (i in 1:5) {
  
  print(i)

  }
```

Here, `i` is our loop variable. It will take on the values in `1:5`, one at a time.

For each value of our loop variable, the code inside the loop body - defined by `{}` curly brackets in R - will run.

## Python

```{python, simple for loop py}
for i in range(1, 6):
    print(i)
```

Here, `i` is our loop variable. It will take on the values in `1:5`, one at a time.

(Note that `range(1, 6)` does **not** include `6`, the endpoint.)

For each value of our loop variable, the code inside the loop body - defined by indentation in Python - will run.
:::

In this case, all we are asking our loop to do is print `i`. It'll do this 5 times, increasing the value of `i` each time for each new iteration of the loop.

But, we can ask for more complex things than this on each iteration, and we don't always have to interact with `i`. 

### Visualising means with for loops {#sec-exm_mean-for-loop}

You might've guessed based on context clues, but we can use for loops to perform repeated simulations using the same starting parameters (in fact, we'll do that a lot in this course).

In this loop, we sample 3 unique datasets, each made up of 20 random data points, from a normal distribution with mean 4 and standard deviation 0.5.

Then, we produce a histogram for each dataset, overlaying the mean value each time.

::: {.panel-tabset group="language"}
## R

```{r, less simple for loop}
for (i in 1:3) {
  
  n <- 20
  mean_n <- 4
  sd_n <- 0.5
  
  data <- rnorm(n, mean_n, sd_n) 

  hist(data, xlim = c(1, 7))
  abline(v = mean(data), col = "red", lwd = 3)

  }
```

## Python

```{python, less simple loop with np.r.n}
plt.close()

for i in range(1, 4):
  n = 20
  mean = 4
  sd = 0.5
  data = np.random.normal(mean, sd, n)
  plt.figure()
  plt.hist(data)
  plt.axvline(x = statistics.mean(data), color = 'r')
  plt.show()
```
:::

We can see that the means in each case are mostly hovering around 4, which is reassuring, since we know that's the true population mean.

## Exercises

### Revisiting Shapiro-Wilk {#sec-exr_shapiro}

Now that you know how to perform random sampling, let's link it back to a specific statistical test, the Shapiro-Wilk test.

As a reminder: the Shapiro-Wilk test is used to help us decide whether a sample has been drawn from a normal distribution or not. It's one of the methods we have for checking the normality assumption.

However, it is also itself a null hypothesis test. The null hypothesis is that the underlying distribution is normal, so a significant p-value is usually interpreted as evidence that the normality assumption is violated.

::: {.callout-exercise}

{{< level 1 >}}

In this exercise, using the template code provided as a starting point:

1. Try a variety of different seeds (hint: `20` might be interesting...)
2. Sample from a uniform distribution instead
3. While sampling from both normal and uniform distributions, try a variety of sample sizes (including `n < 10`)
4. Create normal QQ plots, to compare them to the Shapiro-Wilk results (use the `qqnorm` function)

Try to create:

- A false positive error
- A false negative error

What does this teach you about the nature of the Shapiro-Wilk test, and null hypothesis significance tests in general?

::: {.panel-tabset group="language"}
## R

Template code:

```{r, template for SW exercise}
set.seed(200)
n <- 100
Mean <- 0
SD <- 1

data <- rnorm(n, Mean, SD) 

data %>% hist()

data %>% shapiro.test()
```

:::

:::

## Summary

This chapter covers some key coding basics that will be important in later chapters, such as functions for random sampling, "soft-coding" of variables, and for loops.

A key simulation concept - sampling from distributions with known parameters - was also introduced. This is central to all of the simulating we will do in the remaining chapters.

::: {.callout-tip}
#### Key Points

-   There are a suite of functions for sampling data points from distributions with known parameters
-   Each distribution has its own function, with different parameters that we need to specify
-   Good coding practice (soft-coding of variables, setting a seed, and using loops effectively) all help us to simulate more efficiently
:::

