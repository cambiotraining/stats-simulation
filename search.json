[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical simulations",
    "section": "",
    "text": "Overview\nThis course is designed for those with some statistical knowledge and programming skills, but who are brand new to data simulation.\nThe focus for this course is on simulation for experimental design and statistics; the materials do not cover complex mechanistic simulations e.g., for modelling economics or epidemiology.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistical simulations",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nExplore statistical and experimental design concepts, introduced in other courses, via simulation\n\nUnderstand the impact of factors such as sample size, confounds and collinearity on statistical analyses\nGain a more intuitive sense of core statistical concepts such as significance and power\n\nSimulate life sciences-inspired datasets\n\nSimulate a variety of predictor variables including continuous and categorical main effects and interactions\nSimulate different types of response variable, including continuous, binary/proportional and count response variables\n\nUse simulation to perform statistical analyses\n\nPerform power analysis via simulation\nKnow how to use simulation for hypothesis testing, e.g., via resampling and/or bootstrapping techniques\n\n\n\n\n\nTarget Audience\nThis course is intended for researchers (postgraduate and postdoctoral) in the life sciences, including clinical settings.\n\n\nPrerequisites\nThis course is designed to follow on from our existing programme of statistical courses.\nAttendees must be familiar with the statistical concepts introduced in the Core statistics course. They should also be comfortable using the R programming language.\nIt is recommended that attendees are also familiar with more complex statistical models, such as generalised linear models and mixed effects models, to get the most out of this course.\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#citation-authors",
    "href": "index.html#citation-authors",
    "title": "Statistical simulations",
    "section": "Citation & Authors",
    "text": "Citation & Authors\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in YourReferenceHere”.\n\nYou can cite these materials as:\n\nHodgson, V. (2025). Statistical simulations. https://cambiotraining.github.io/quarto-course-template/\n\nOr in BibTeX format:\n@misc{YourReferenceHere,\n  author = {Hodgson, Vicki},\n  month = {5},\n  title = {Statistical simulations},\n  url = {https://cambiotraining.github.io/quarto-course-template/},\n  year = {2025}\n}\nAbout the authors:\nVicki Hodgson  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: writing - original draft; conceptualisation",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical simulations",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSome of the code examples provided in these course materials were written with the aid of a large language model (ChatGPT-4o). All code was reviewed and edited by the (human) authors, and AI was not used to generate any of the written content.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & setup",
    "section": "",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop to follow along with the materials.\nDownload",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & setup</span>"
    ]
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "Data & setup",
    "section": "Software",
    "text": "Software\n\nR and RStudio\n\n\nWindows\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & setup</span>"
    ]
  },
  {
    "objectID": "materials/01-drawing-samples.html",
    "href": "materials/01-drawing-samples.html",
    "title": "3  Drawing samples from distributions",
    "section": "",
    "text": "3.1 Libraries and functions\nThe first thing we need to get comfortable with is random sampling, i.e., drawing a number of data points from an underlying distribution with known parameters.\nParameters are features of a distribution that determine its shape. For example, the normal distribution has two parameters: mean and variance (or standard deviation).\nThis random sampling is what we’re (hopefully) doing when we collect a sample in experimental research. The underlying distribution of the response variable, i.e., the “ground truth” in the world, has some true parameters that we’re hoping to estimate. We collect a random subset of individual observations that have come from that underlying distribution, and use the sample’s statistics to estimate the true population parameters.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Drawing samples from distributions</span>"
    ]
  },
  {
    "objectID": "materials/01-drawing-samples.html#libraries-and-functions",
    "href": "materials/01-drawing-samples.html#libraries-and-functions",
    "title": "3  Drawing samples from distributions",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(tidyverse)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nfrom plotnine import *",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Drawing samples from distributions</span>"
    ]
  },
  {
    "objectID": "materials/01-drawing-samples.html#sampling-from-the-normal-distribution",
    "href": "materials/01-drawing-samples.html#sampling-from-the-normal-distribution",
    "title": "3  Drawing samples from distributions",
    "section": "3.2 Sampling from the normal distribution",
    "text": "3.2 Sampling from the normal distribution\n\nRPython\n\n\nThis is the rnorm function, which we’ll be using a lot in this course. It takes three arguments; the first is the number of data points (n) that you’d like to draw. The second and third arguments are the two important parameters that describe the shape of the underlying distribution: the mean and the standard deviation.\n\nrnorm(n = 100, mean = 0, sd = 1)\n\n  [1] -1.22940988 -0.78175473 -0.05941649  0.43406066  0.78549053  0.56536106\n  [7] -0.63515430  0.10417513 -2.08135144 -0.21772407 -0.36095027  1.25353619\n [13] -1.31314589  0.96765067 -0.92081289 -0.76175924  0.73932670  1.10017197\n [19]  0.12156430 -2.09315168  2.33844957 -0.22713777 -1.15659441 -0.50859259\n [25] -0.35160684 -0.12091276  1.00008825 -0.90091916 -0.56267685  0.23064728\n [31] -0.31785121 -0.58964684 -1.81563776  0.23640526 -1.20897085  1.12029491\n [37] -0.38821684  1.13408684 -1.04606759  0.92615178 -0.09701480 -0.53382837\n [43] -0.41569846 -0.25972174 -0.26645389  0.19264407 -0.91187099  0.42361118\n [49]  1.42616047 -1.67701722 -1.14529965 -0.84394270  1.46987867 -0.01068597\n [55]  0.49853952  2.47578333  0.05834657  0.33363704 -0.08329676  0.09074863\n [61]  0.67496907  1.14087753 -1.04401976  0.14448116 -0.95568251 -0.06110808\n [67]  0.04990981 -0.37881064  0.89040968 -0.50300805  0.90501630 -0.39382809\n [73]  1.68724592  0.98484438 -0.41111495 -0.65825626  0.56220031 -1.06151990\n [79]  0.80127211 -1.09972058  1.52686100  1.91600738  1.75376795 -0.46469303\n [85]  0.28610037  1.19647158 -0.33322253  0.04652775  2.05441549  0.97647477\n [91] -0.51554070  0.82428873 -0.62834756 -1.55123585  1.43185246 -0.16385640\n [97] -0.15364102  2.47732635  0.45012856  0.92003779\n\n\nWithout any further instruction, R simply prints us a list of length n; this list contains numbers that have been pulled randomly from a normal distribution with mean 0 and standard deviation 1.\nThis is a sample or dataset, drawn from an underlying population, which we can now visualise.\nWe’ll use the base R hist function for this for now, just to keep things simple:\n\nrnorm(100, 0, 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\n\n\n\nThe typical default method in Python for sampling from a normal distribution is via numpy.random.normal.\nIt takes three arguments: loc (mean), scale (standard deviation) and size (sample size):\n\nnorm_data = np.random.normal(loc = 0, scale = 1, size = 100)\nprint(norm_data)\n\n[ 1.18790953  1.38723105 -0.75600814  0.8410843  -0.57766845 -1.64258659\n -0.50886713  1.5197107   0.86489165  0.3051659   0.68563204 -1.46101353\n -0.74638774  0.31405489  0.10245238  0.88659415 -0.92750301 -0.16047263\n  0.96703509 -0.12210535 -1.21487207  0.35922202  0.49859833 -0.99115405\n  0.57701065  0.47599325  0.56958299  0.30291505  0.0988164   0.63402371\n  1.68415555  0.80163562  0.57098047 -1.04267593  0.76374138 -0.64495585\n  1.71271037 -1.10353423 -0.15160839 -0.2881727   0.40208074  0.18689907\n  2.50543842  0.84248958  0.11523662  0.13546877  0.63422016 -0.05847853\n  0.25498864 -0.61147325 -1.57169603 -1.29311285  0.42553885  0.93507036\n -1.12242325 -0.11015332  0.88560299 -0.01718516  0.42783246  1.6490974\n -1.01240318 -0.09587259 -0.26182404 -0.48902448 -1.23146388  1.37848657\n -0.45180123  0.28357726 -0.47183027 -0.49051111 -2.5622498  -1.56173666\n  0.28948143  0.55626578 -0.57596352 -1.98400261  0.08957272 -1.10548904\n  0.27674145  0.00781173  1.81832078  1.10086291 -0.57059829 -0.2551952\n  1.00554561  2.11844184 -1.5444488  -0.79271778 -0.34570661 -0.32839502\n  1.25159258 -1.0947154   0.85876553  0.39134558 -0.79129755  0.46582172\n -2.03891307  0.67608781 -2.73679991  1.21440739]\n\n\nThe output is an array of numbers, of length size.\nThis is a sample or dataset, drawn from an underlying population, which we can now visualise.\nFor this first example, we’ll show how to use both matplotlib and plotnine to create histograms. We’ll use the matplotlib version for speed as we go through the course, but if you’re transitioning over from R (or ggplot), you might find plotnine friendlier.\n\nplt.hist(norm_data)\nplt.show()\n\n\n\n\n\n\n\n\nIf plotting multiple histograms, you can use plt.clf() to clear the figure, or plt.close() to close the plotting window entirely.\nIf using plotnine, you have to convert the array to a data frame before you can visualise it:\n\nnorm_df = pd.DataFrame({\"values\":norm_data})\n\nnorm_df_hist = (\n  ggplot(norm_df, aes(x = \"values\")) +\n  geom_histogram()\n)\n\nprint(norm_df_hist)\n\n&lt;ggplot: (640 x 480)&gt;",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Drawing samples from distributions</span>"
    ]
  },
  {
    "objectID": "materials/01-drawing-samples.html#sampling-from-other-distributions",
    "href": "materials/01-drawing-samples.html#sampling-from-other-distributions",
    "title": "3  Drawing samples from distributions",
    "section": "3.3 Sampling from other distributions",
    "text": "3.3 Sampling from other distributions\nThe normal/Gaussian distribution might be the most famous of the distributions, but it is not the only one that exists - nor the only one that we’ll care about on this course.\nFor example, we’ll also be sampling quite regularly from the uniform distribution.\nThe uniform distribution is flat: inside the range of possible values, all values are equally likely, and outside that range, the probability density drops to zero. This means the only parameters we need to set are the minimum and maximum of that range of possible values, like so:\n\nRPython\n\n\n\nrunif(n = 100, min = 0, max = 1) %&gt;%\n  hist(xlim = c(-0.5, 1.5))\n\n\n\n\n\n\n\n\n\n\n\nunif_data = np.random.uniform(low = 0, high = 1, size = 100)\n\nplt.clf() # clear existing plot, if applicable\nplt.hist(unif_data)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe underlying shape of the distribution that we just sampled from looks like this - square and boxy. The probability density is zero outside of the 0-1 range, and flat inside it:\n\n\n\n\n\n\n\n\n\nLater in this course, we will sample from the binomial, negative binomial and Poisson distributions as well.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Drawing samples from distributions</span>"
    ]
  },
  {
    "objectID": "materials/01-drawing-samples.html#setting-a-seed",
    "href": "materials/01-drawing-samples.html#setting-a-seed",
    "title": "3  Drawing samples from distributions",
    "section": "3.4 Setting a seed",
    "text": "3.4 Setting a seed\nWhat happens if you run this block of code over and over again?\n\nRPython\n\n\n\nrnorm(100, 0, 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\nplt.hist(np.random.normal(0, 1, 100))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nEach time you run the code, you are sampling a unique random subset of data points.\nIt’s very helpful that we can do that - later on in this course, we’ll exploit this to sample many different datasets from the same underlying population.\nHowever, sometimes it’s useful for us to be able to sample the exact set of data points more than once.\n\nRPython\n\n\nTo achieve this, we can use the set.seed function.\nRun the following code several times in a row, and you’ll see the difference:\n\nset.seed(20)\n\nrnorm(100, 0, 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\n\n\n\nTo achieve this, we can use the np.random.seed function.\nRun the following code several times in a row, and you’ll see the difference:\n\nnp.random.seed(20)\n\nplt.clf()\nplt.hist(np.random.normal(0, 1, 100))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNotice how each time, the exact same dataset and histogram are produced?\nYou can choose any number you like for the seed. All that matters is that you return to that same seed number, if you want to recreate that dataset.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Drawing samples from distributions</span>"
    ]
  },
  {
    "objectID": "materials/01-drawing-samples.html#exercises",
    "href": "materials/01-drawing-samples.html#exercises",
    "title": "3  Drawing samples from distributions",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\n3.5.1 Revisiting Shapiro-Wilk\nNow that you know how to perform random sampling, let’s link it back to a specific statistical test, the Shapiro-Wilk test.\nAs a reminder: the Shapiro-Wilk test is used to help us decide whether a sample has been drawn from a normal distribution or not. It’s one of the methods we have for checking the normality assumption.\nHowever, it is also itself a null hypothesis test. The null hypothesis is that the underlying distribution is normal, so a significant p-value is usually interpreted as evidence that the normality assumption is violated.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nLevel: \nIn this exercise, using the template code provided as a starting point:\n\nTry a variety of different seeds (hint: 20 might be interesting…)\nSample from a uniform distribution instead\nWhile sampling from both normal and uniform distributions, try a variety of sample sizes (including n &lt; 10)\nCreate normal QQ plots, to compare them to the Shapiro-Wilk results (use the qqnorm function)\n\nTry to create:\n\nA false positive error\nA false negative error\n\nWhat does this teach you about the nature of the Shapiro-Wilk test, and null hypothesis significance tests in general?\n\nRPython\n\n\nTemplate code:\n\nset.seed(200)\nn &lt;- 100\nMean &lt;- 0\nSD &lt;- 1\n\ndata &lt;- rnorm(n, Mean, SD) \n\ndata %&gt;% hist()\n\n\n\n\n\n\n\ndata %&gt;% shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.97978, p-value = 0.1279\n\n\n\n\nTemplate code:\n\nimport pingouin as pg # needed for the pg.normality function\n\nnp.random.seed(200)\nn = 100\nmean = 0\nsd = 1\n\ndata = np.random.normal(mean, sd, n)\n\nplt.clf()\nplt.hist(data)\nplt.show()\n\n\n\n\n\n\n\npg.normality(data)\n\n          W      pval  normal\n0  0.986137  0.382255    True",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Drawing samples from distributions</span>"
    ]
  },
  {
    "objectID": "materials/01-drawing-samples.html#summary",
    "href": "materials/01-drawing-samples.html#summary",
    "title": "3  Drawing samples from distributions",
    "section": "3.6 Summary",
    "text": "3.6 Summary\nThis chapter introduced a key simulation concept: sampling from distributions with known parameters. This is central to all of the simulating we will do in the remaining chapters.\n\n\n\n\n\n\nKey Points\n\n\n\n\nThere are a suite of functions for sampling data points from distributions with known parameters\nEach distribution has its own function, with different parameters that we need to specify\nYou can set a seed to make sure you sample the exact same set of values each time",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Drawing samples from distributions</span>"
    ]
  },
  {
    "objectID": "materials/02-loops.html",
    "href": "materials/02-loops.html",
    "title": "4  Loops",
    "section": "",
    "text": "4.1 Libraries and functions\nIn the previous chapter, we learned how to sample random datasets from known distributions.\nHere, we’ll combine that with a fundamental programming technique: loops.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loops</span>"
    ]
  },
  {
    "objectID": "materials/02-loops.html#libraries-and-functions",
    "href": "materials/02-loops.html#libraries-and-functions",
    "title": "4  Loops",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(tidyverse)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nfrom plotnine import *",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loops</span>"
    ]
  },
  {
    "objectID": "materials/02-loops.html#soft-coding-parameters",
    "href": "materials/02-loops.html#soft-coding-parameters",
    "title": "4  Loops",
    "section": "4.2 Soft-coding parameters",
    "text": "4.2 Soft-coding parameters\nBefore we get onto loops, we have one quick detour to take.\nIn the last chapter, we “hard-coded” our parameters by putting them directly inside the rnorm or np.random.normal functions.\nHowever, from here on, we will start “soft-coding” parameters or other values that we might want to change.\nThis is considered good programming practice. It’s also sometimes called “dynamic coding”.\n\nRPython\n\n\n\nn &lt;- 100\nmean_n &lt;- 0\nsd_n &lt;- 1\n\nrnorm(n = n, mean = mean_n, sd = sd_n) %&gt;%\n  hist()\n\n\n\n\n\n\n\n\n\n\n\nn = 100\nmean = 0\nsd = 1\n\nplt.clf()\nplt.hist(np.random.normal(loc = mean, scale = sd, size = n))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis might look like we’re writing out more code, but it will be helpful in more complex simulations where we use the same parameter more than once.\nWe’re also separating out the bits of the code that might need to be edited, which are all at the top where we can more easily see them, versus the bits we can leave untouched.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loops</span>"
    ]
  },
  {
    "objectID": "materials/02-loops.html#loops",
    "href": "materials/02-loops.html#loops",
    "title": "4  Loops",
    "section": "4.3 Loops",
    "text": "4.3 Loops\nIn programming, a loop is a chunk of code that is run repeatedly, until a certain condition is satisfied.\nThere are two broad types of loop: the for loop and the while loop. For the purposes of this course, we’ll only really worry about the for loop. For loops run for a pre-specified number of iterations before stopping.\n\n4.3.1 For loop syntax\n\nRPython\n\n\nIn R, the syntax for a for loop looks like this:\n\nfor (i in 1:5) {\n  \n  print(i)\n\n  }\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere, i is our loop variable. It will take on the values in 1:5, one at a time.\nFor each value of our loop variable, the code inside the loop body - defined by {} curly brackets in R - will run.\n\n\n\nfor i in range(1, 6):\n    print(i)\n\n1\n2\n3\n4\n5\n\n\nHere, i is our loop variable. It will take on the values in 1:5, one at a time.\n(Note that range(1, 6) does not include 6, the endpoint.)\nFor each value of our loop variable, the code inside the loop body - defined by indentation in Python - will run.\n\n\n\nIn this case, all we are asking our loop to do is print i. It’ll do this 5 times, increasing the value of i each time for each new iteration of the loop.\nBut, we can ask for more complex things than this on each iteration, and we don’t always have to interact with i.\n\n\n4.3.2 Visualising means with for loops\nYou might’ve guessed based on context clues, but we can use for loops to perform repeated simulations using the same starting parameters (in fact, we’ll do that a lot in this course).\nIn this loop, we sample 3 unique datasets, each made up of 20 random data points, from a normal distribution with mean 4 and standard deviation 0.5.\nThen, we produce a histogram for each dataset, overlaying the mean value each time.\n\nRPython\n\n\n\nfor (i in 1:3) {\n  \n  n &lt;- 20\n  mean_n &lt;- 4\n  sd_n &lt;- 0.5\n  \n  data &lt;- rnorm(n, mean_n, sd_n) \n\n  hist(data, xlim = c(1, 7))\n  abline(v = mean(data), col = \"red\", lwd = 3)\n\n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.close()\n\nfor i in range(1, 4):\n  n = 20\n  mean = 4\n  sd = 0.5\n  data = np.random.normal(mean, sd, n)\n  plt.figure()\n  plt.hist(data)\n  plt.axvline(x = statistics.mean(data), color = 'r')\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the means in each case are mostly hovering around 4, which is reassuring, since we know that’s the true population mean.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loops</span>"
    ]
  },
  {
    "objectID": "materials/02-loops.html#exercises",
    "href": "materials/02-loops.html#exercises",
    "title": "4  Loops",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\n4.4.1 Even numbers\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nLevel: \nWrite a for loop that prints out all of the even numbers between 1 and 100 (inclusive).\n\n\n\n\n\n\nWorked answer\n\n\n\n\n\nThere’s actually a couple of different ways you could do this:\n\nRPython\n\n\n\nMethod 1\n\nfor (i in 1:50) {\n  print(i*2)\n}\n\n\n\nMethod 2\nThis method uses another programming essential, the if-statement.\nWe won’t really use if-statements much in this course, but if you can get your head around the for loop syntax, you can definitely manage an if-statement, and it doesn’t hurt to know they exist!\n\nfor (i in 1:100) {\n  if (i%%2 == 0) {\n    print(i)\n  }\n}\n\n\n\n\n\nMethod 1\n\nfor i in range(1,51):\n  print(i*2)\n\nThe only thing to watch here is that Python uses zero-indexing, i.e., starts counting from 0 instead of 1.\nThis means that writing range(50) here doesn’t actually get you the right numbers!\n\n\nMethod 2\nThis method uses another programming essential, the if-statement.\nWe won’t really use if-statements much in this course, but if you can get your head around the for loop syntax, you can definitely manage an if-statement, and it doesn’t hurt to know they exist!\n\nfor i in range(1,101):\n  if i % 2 == 0:\n    print(i)\n\nAgain, watch for zero-indexing!\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Fizzbuzz\nLet’s do something a little different, to really show the power of loops.\nFor some context, for those of you who’ve never played: Fizzbuzz is a silly parlour game that involves taking it in turns to count, adding 1 each time.\nThe trick is that all multiples of 3 must be replaced with the word “fizz”, and all multiples of 5 with the word “buzz” (and multiples of both with “fizzbuzz”).\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nLevel: \nIn this exercise, write a for loop that will play the Fizzbuzz game all by itself, up to the number 100.\nNote: you will need to make if-else statements within your loop for this to work!\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nRPython\n\n\n\nfor (i in 1:100){\n  \n  if (i%%3 == 0 & i%%5 == 0) {\n    print(\"fizzbuzz\")\n  } else if (i%%5 &gt; 0 & i%%3 == 0) {\n    print(\"fizz\")\n  } else if (i%%5 == 0 & i%%3 &gt; 0) {\n    print(\"buzz\")\n  } else {\n    print(i)\n  }\n  \n}\n\n\n\n\nfor i in range(1,101):\n  if i%3 == 0 and i%5 == 0: print(\"fizzbuzz\")\n  elif i%3 == 0 and i%5 &gt; 0: print(\"fizz\")\n  elif i%5 == 0 and i%3 &gt; 0: print(\"buzz\")\n  else: print(i)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loops</span>"
    ]
  },
  {
    "objectID": "materials/02-loops.html#summary",
    "href": "materials/02-loops.html#summary",
    "title": "4  Loops",
    "section": "4.5 Summary",
    "text": "4.5 Summary\nThis chapter covers some key coding practices that will be important later, including “soft-coding” of variables and for loops.\n\n\n\n\n\n\nKey Points\n\n\n\n\nSoft-coding, or dynamic coding, means assigning key parameters at the top of a script rather than within functions, so that they can more easily be changed later\nFor loops in programming are chunks of code that are executed for a pre-specified number of iterations",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Simulation basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Loops</span>"
    ]
  },
  {
    "objectID": "materials/03-central-limit-theorem.html",
    "href": "materials/03-central-limit-theorem.html",
    "title": "5  The central limit theorem",
    "section": "",
    "text": "5.1 Libraries and functions\nNow that you have the programming basics, we’re going to use them to make sense of a famous statistical concept: the central limit theorem.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The central limit theorem</span>"
    ]
  },
  {
    "objectID": "materials/03-central-limit-theorem.html#libraries-and-functions",
    "href": "materials/03-central-limit-theorem.html#libraries-and-functions",
    "title": "5  The central limit theorem",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nfrom scipy.stats import t",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The central limit theorem</span>"
    ]
  },
  {
    "objectID": "materials/03-central-limit-theorem.html#estimating-parameters-from-datasets",
    "href": "materials/03-central-limit-theorem.html#estimating-parameters-from-datasets",
    "title": "5  The central limit theorem",
    "section": "5.2 Estimating parameters from datasets",
    "text": "5.2 Estimating parameters from datasets\nThere are two main forms of statistical inference, from dataset to population.\n\nEstimating parameters\nTesting hypotheses\n\nWe’ll get onto the second form later on, but for this section, we’re going to focus on estimating parameters.\nIn other words - can we do a good job of recreating the distribution of the underlying population, just from the sample we’ve taken from that population?\nOur ability to do this well often hinges on the quality of our sample. This includes both the size of the sample, and whether it is biased in any way.\nIf our sample is biased, there’s not much we can do about it apart from a) scrapping it and starting again, or b) acknowledging the uncertainty/narrowed focus of our conclusions.\nWe do, however, often have some control over the sample size. Let’s look at how sample size affects our parameter estimates.\n\n5.2.1 Mean\nWhen simulating data, we have an unusual level of knowledge and power (in the normal sense of the word, not the statistical one!):\nWe know exactly what the true population parameters are, because we specified them.\nIn the code below, we know that the actual true population mean is 4, with no uncertainty. We have set this to be the “ground truth”.\nThis code is similar to the for loop introduced in Section 4.3.2, but we’ve increased the sample size.\nNotice how this:\n\nDecreases variance (our histograms are “squished” inward - pay attention to the x axis scale)\nIncreases the consistency of our mean estimates (they are more similar to one another)\nIncreases the accuracy of our mean estimates (they are, overall, closer to the true value of 4)\n\n\nRPython\n\n\n\nfor (i in 1:3) {\n  \n  n &lt;- 200\n  mean_n &lt;- 4\n  sd_n &lt;- 0.5\n  \n  data &lt;- rnorm(n, mean_n, sd_n) \n  \n  hist(data, xlim = c(1, 7)); abline(v = mean(data), col = \"red\", lwd = 3)\n\n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in range(1, 4):\n  n = 200\n  mean = 4\n  sd = 0.5\n  data = np.random.normal(mean, sd, n)\n  plt.figure()\n  plt.hist(data)\n  plt.axvline(x = statistics.mean(data), color = 'r')\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a larger sample size, we are able to do a better job of “recovering” or recreating the original parameter we specified.\n\n\n\n\n\n\nThe average of averages\n\n\n\nWhen we take multiple samples and average across them, we do an even better job of recovering the true population mean.\nIn other words, if we sampled a bunch of datasets, and then took the average of all of their respective means, we’d probably get pretty close to 4.\nThe section later on in this chapter, on the central limit theorem, pushes this idea a little further.\n\n\n\n\n5.2.2 Variance\nIn the code above, we also know what the true population variance is, because we set it (by setting standard deviation, the square root of variance).\n\n\n\n\n\n\nWhat do we mean by “variance”?\n\n\n\nVariance is calculated by measuring the difference from each data point and the mean, squaring them, adding those squares up, and then dividing by the number of data points. The mathematical formula looks like this:\n\\[\n\\text{Var}(X) = \\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 } {n}\n\\]\nHowever, when we estimate population variance from a sample, we actually tweak this formula a bit. We divide instead by \\(n - 1\\):\n\\[\n\\text{Var}(X) = \\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 } {n - 1}\n\\]\nWhy?\nWell, I could give you a theoretical explanation. Or, we can use the power of simulation, and see for ourselves more intuitively.\nFor the next section, the word variance will crop up in different forms:\n\nEstimated sample variance, calculated by dividing by \\(n - 1\\), from a dataset\nEstimated variance, calculated by dividing by \\(n\\), from a dataset\nThe true population variance, which isn’t calculated; it’s known/specified by us\n\n\n\nIn the for loop below, we are using the same simulation parameters as above, and calculating (manually) two types of variance.\nWe’ll set the mean to 4 and the variance to 1:\n\nRPython\n\n\n\nresults &lt;- data.frame(var=numeric(),\n                      sample_var=numeric())\n\nfor (i in 1:30) {\n  \n  n &lt;- 10\n  mean_n &lt;- 4\n  sd_n &lt;- 1\n  \n  data &lt;- rnorm(n, mean_n, sd_n) \n  \n  v &lt;- sum((data - mean(data))^2)/n\n  samplev &lt;- sum((data - mean(data))^2)/(n-1)\n\n  results &lt;- rbind(results, data.frame(var = v, sample_var = samplev))\n  \n}\n\nTo break down this code further:\nWe’ve initialised a results table with our desired columns first. On each iteration of the loop (30 total), we sample a dataset, calculate the estimated variance estv and sample variance samplev using slightly different formulae, and then add them to our results table.\n\n\n\nrows = []\n\nfor i in range(30):\n    n = 10\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    v = np.sum((data - np.mean(data))**2) / n\n    samplev = np.sum((data - np.mean(data))**2) / (n - 1)\n    \n    rows.append({'var': v, 'sample_var': samplev})\n\nresults = pd.DataFrame(rows)\n\nTo break down this code further:\nOn each iteration of the loop (30 total), we sample a dataset, calculate the estimated variance estv and sample variance samplev using slightly different formulae. These values are collated by our rows object, which we finally convert to a pandas dataframe (a results table).\n\n\n\nNow, let’s look at those results and what they show us.\nFirst, we’ll create a new column in our results object that contains the difference between our estimates in each case.\n\nRPython\n\n\n\nresults &lt;- results %&gt;%\n  mutate(diff_var = sample_var - var)\n\n\n\n\nresults['diff_var'] = results['sample_var'] - results['var']\n\n\n\n\nWhen we look at the final results file, we see that the estimated sample variance is, on average, a bit bigger than the estimated variance. This makes sense, because we’re dividing by a smaller number when we calculate the sample variance (in other words, \\(n - 1 &lt; n\\)).\nNow, let’s look at the average variance and sample variance, and see which of them is doing the best job of recreating our true population variance (which we know is 1, because we built the simulation that way).\n\nRPython\n\n\n\nresults %&gt;%\n  summarise(mean(var), mean(sample_var), mean(diff_var))\n\n  mean(var) mean(sample_var) mean(diff_var)\n1 0.9183643         1.020405      0.1020405\n\n\n\n\n\nsummary = results[['var', 'sample_var', 'diff_var']].mean()\nprint(summary)\n\nvar           0.918773\nsample_var    1.020859\ndiff_var      0.102086\ndtype: float64\n\n\n\n\n\nHere, we’ve taken the mean of each column. This means we’re looking at the average estimated variance and the average estimated sample variance, across all 10 of our random datasets.\n\n\n\n\n\n\nRemember, each of these numbers is an average\n\n\n\nYes, now we’re taking the mean of the variance. Yes, we could also measure the variance of the variance if we wanted. Yes, this does start to get confusing the more you think about it.\nMore on this in the central limit theorem section!\n\n\nAs we can see from these results, when the sample size is small, our estimated variance (on average) underestimates the true value.\nThis is because, the smaller the sample, the less likely it is to contain values from the edges or tails of the normal distribution, so we don’t get a good picture of the true spread.\nThe sample variance accounts for this by dividing by n - 1 instead, so the estimate is larger and less of an underestimate.\nTo get an intuition for this, let’s repeat all the code above, but with a larger sample size:\n\nRPython\n\n\n\nresults &lt;- data.frame(var=numeric(),\n                      sample_var=numeric())\n\nfor (i in 1:30) {\n  \n  n &lt;- 20\n  mean_n &lt;- 4\n  sd_n &lt;- 1\n  \n  data &lt;- rnorm(n, mean_n, sd_n) \n  \n  v &lt;- sum((data - mean(data))^2)/n\n  sample &lt;- sum((data - mean(data))^2)/(n-1)\n\n  results &lt;- rbind(results, data.frame(var = v, sample_var = sample))\n  \n}\n\nresults &lt;- results %&gt;%\n  mutate(diff_var = sample_var - var)\n\nresults %&gt;%\n  summarise(mean(var), mean(sample_var), mean(diff_var))\n\n  mean(var) mean(sample_var) mean(diff_var)\n1 0.9507581         1.000798      0.0500399\n\n\n\n\n\nrows = []\n\nfor i in range(30):\n    n = 20\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    v = np.sum((data - np.mean(data))**2) / n\n    samplev = np.sum((data - np.mean(data))**2) / (n - 1)\n    \n    rows.append({'var': v, 'sample_var': samplev})\n\nresults = pd.DataFrame(rows)\n\nresults['diff_var'] = results['sample_var'] - results['var']\n\nsummary = results[['var', 'sample_var', 'diff_var']].mean()\nprint(summary)\n\nvar           0.953684\nsample_var    1.003878\ndiff_var      0.050194\ndtype: float64\n\n\n\n\n\nWhen n is larger, notice how the estimated variance is now closer to the true population variance, on average?\nWith a larger sample size, we are more likely to sample the full “spread” of the distribution.\nThe estimated sample variance, however, is about as close to the true population variance as it was before, and so the difference between the estimated variance and estimated sample variance has shrunk.\n\n\n\n\n\n\nWhy sample variance is even cleverer than you might think\n\n\n\n\n\nDividing by n - 1 has a bigger impact when the sample is small, where 1 will be a relatively larger fraction of n.\nThis is great, because these smaller samples are also the place where we need this adjustment most: they’re less likely to contain values from the tails of the distribution, and therefore will underestimate the true population variance more.\nIn contrast, when our sample is much bigger, it’s going to be more representative/less noisy, and we see much less of an underestimation and will need less adjustment. Happily, we will automatically get less of an adjustment anyway, since the 1 is now a smaller fraction of n.\nIn other words: the impact of dividing by n - 1 scales naturally with both the size of n, and with the amount of underestimation we need to account for.\nIn fact, when the sample is infinitely large, we should see no difference between the estimated sample variance and the estimated variance at all, because n-1 = n at infinity.\nYou can test this intuition (except for the infinity part, you kinda just have to trust me on that) by continuing to mess around with the value of n in the code above.\n\n\n\nSince sample variance is the most effective way to estimate the true population variance, functions in R and Python will default to this.\n\nRPython\n\n\nThe var function in R specifically calculates the sample variance. We can see that we get identical results using the function or doing it manually, by adapting the loop above:\n\nresults &lt;- data.frame(var=numeric(),\n                      sample_var=numeric(),\n                      r_var=numeric())\n\nfor (i in 1:30) {\n  \n  n &lt;- 20\n  mean_n &lt;- 4\n  sd_n &lt;- 1\n  \n  data &lt;- rnorm(n, mean_n, sd_n) \n  \n  v &lt;- sum((data - mean(data))^2)/n\n  sample &lt;- sum((data - mean(data))^2)/(n-1)\n  \n  # Add an extra column containing the results of var(data)\n  results &lt;- rbind(results, data.frame(var = v, sample_var = sample, r_var = var(data)))\n  \n}\n\nresults %&gt;%\n  summarise(mean(var), mean(sample_var), mean(r_var))\n\n  mean(var) mean(sample_var) mean(r_var)\n1 0.9507581         1.000798    1.000798\n\n\nNotice how mean(sample_var) and mean(r_var) are identical?\n\n\nThe numpy.var function specifically calculates sample variance. We can see that we get identical results using the function or doing it manually, by adapting the loop above:\n\nrows = []\n\nfor i in range(30):\n    n = 20\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    # Estimated variance\n    v = np.sum((data - np.mean(data))**2) / n\n    # Sample variance\n    sample = np.sum((data - np.mean(data))**2) / (n - 1)\n    # numpy sample variance\n    np_var = np.var(data, ddof=1)\n\n    rows.append({'var': v, 'sample_var': sample, 'np_var': np_var})\n\nresults = pd.DataFrame(rows)\n\nsummary = results[['var', 'sample_var', 'np_var']].mean()\nprint(summary)\n\nvar           0.953684\nsample_var    1.003878\nnp_var        1.003878\ndtype: float64\n\n\nNotice how sample_var and np_var are identical?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The central limit theorem</span>"
    ]
  },
  {
    "objectID": "materials/03-central-limit-theorem.html#central-limit-theorem",
    "href": "materials/03-central-limit-theorem.html#central-limit-theorem",
    "title": "5  The central limit theorem",
    "section": "5.3 Central limit theorem",
    "text": "5.3 Central limit theorem\nIn the section above, we used for loops to simulate multiple datasets, measure certain statistics from them, and then averaged those statistics across the datasets.\nSo, in some cases, we were looking at the mean of the means, or the mean of the variances. We’re going to unpack that a bit further now.\nSpecifically, we’re going to talk about the central limit theorem: the idea that, across multiple samples taken from the same distribution, the estimates/statistics we calculate from them will themselves follow a normal distribution.\n\n5.3.1 An example: the mean\nYou will recognise all the code below from previous sections, but here we’re using it to show us a slightly different distribution.\nInstead of producing separate histograms for each of the datasets (i.e., one per loop), we are instead simply collecting the mean value from each of our datasets.\nThen, we will treat the set of means as a sample in itself, and visualise its distribution.\n\nRPython\n\n\n\nmeans &lt;- c()\n\nfor (i in 1:40) {\n  \n  n &lt;- 200\n  mean_n &lt;- 4\n  sd_n &lt;- 1\n  \n  means[i] &lt;- mean(rnorm(n, mean_n, sd_n))\n\n}\n\nhist(means)\nabline(v = mean(means), col = \"red\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\n\nmeans = []\n\nfor i in range(40):\n  n = 200\n  mean = 4\n  sd = 1\n  \n  est_mean = np.random.normal(mean, sd, n).mean()\n\n  means.append(est_mean)\n\nplt.clf()\nplt.hist(means)\nplt.axvline(x = statistics.mean(means), color = 'r')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe set of means from our i datasets, follow a normal distribution. The mean of this normal distribution is approximately the true population mean (which we know to be 4).\nIf we increase the number of iterations/loops, we will sample more datasets, with more means.\nIf we think of our set of sample means as a sample in itself, then doing this is effectively increasing our sample size. And, as we know from the first section of this chapter, that means that the mean of our distribution should be a better estimate of the true population value.\nThis is exactly what happens:\n\nRPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo produce the plot above, the number of iterations was set to i = 1000.\nTry setting it to something between 40 and 1000, or even more than 1000, and see how that changes things.\n\n\n\n\n\n\nStandard error of the mean\n\n\n\n\n\nYou may have come across the concept of the standard error of the mean in the past (especially when constructing error bars for plots). But now, you should be in a better position to really understand what it is.\nIn the histogram above, we’ve calculated the mean of the sample means. But around that mean of sample means, there is some spread or noise.\nWe can quantify that spread by measuring the standard deviation of the distribution of sample means - and if we do, we’ve calculated the standard error.\nOf course, in classical statistics we usually only have one dataset, rather than 1000, to help us figure out that standard error. So, like with everything else we calculate from a dataset, we are only ever able to access an estimate of that standard error.\n\n\n\n\n\n5.3.2 It’s always normal\nThe really quirky thing about the central limit theorem is that it doesn’t actually matter what distribution you pulled the original samples from. In other words, the results we got above aren’t just because we were using the normal distribution for our simulations.\nTo prove that, the code here has been adapted to pull each of our 1000 samples from a uniform distribution instead, and estimate the mean.\n\nRPython\n\n\nNote the use of runif instead of rnorm:\n\nmeans &lt;- c()\n\nfor (i in 1:1000) {\n  \n  n &lt;- 200\n  min_n &lt;- 1\n  max_n &lt;- 7\n  \n  means[i] &lt;- mean(runif(n, min_n, max_n))\n\n}\n\nhist(means)\nabline(v = mean(means), col = \"red\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\nNote the use of np.random.uniform instead of np.random.normal:\n\nmeans = []\n\nfor i in range(1000):\n  n = 200\n  lower = 1\n  upper = 7\n  est_mean = np.random.uniform(mean, sd, n).mean()\n  means.append(est_mean)\n\nplt.clf()\nplt.hist(means)\nplt.axvline(x = statistics.mean(means), color = 'r')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAlthough each of the individual samples would have a flat histogram, that’s not what we’re plotting here. Here, we’re looking at the set of means that summarise each of those individual samples.\nThe nature of the underlying population distribution doesn’t matter - the distribution of the parameter estimates is still normal, which we can see clearly with a sufficient number of simulations.\nNo wonder the normal distribution enjoys such special status in statistics.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The central limit theorem</span>"
    ]
  },
  {
    "objectID": "materials/03-central-limit-theorem.html#exercises",
    "href": "materials/03-central-limit-theorem.html#exercises",
    "title": "5  The central limit theorem",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\n5.4.1 t-statistic under CLT\nAll statistics obey the central limit theorem. This includes not just descriptive statistics like the mean, median, standard deviation etc., but the test statistics that we use for hypothesis testing.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nLevel: \nTo demonstrate this to yourself, generate 1000 t-statistics from one-sample t-tests, and plot them on a histogram.\n\nWhat happens to the distribution as you change mu?\nHow does the distribution of 1000 t-statistics, compare to the t-statistic distribution? Why are they different?\n\nYou can refer back to the code in Section 5.3.1 to help you.\n\n\n\n\n\n\nCode tips\n\n\n\n\n\nIf you’re struggling to extract the t-statistics, you might find the below code snippets to be helpful as tips/hints!\n\nRPython\n\n\nIf you’re using the base R t.test function:\n\nt.result &lt;- t.test(rnorm(n, mean_n, sd_n), mu = 3)\n  \nt.values[i] &lt;- unname(t.result$statistic)\n\nThis first method is probably quicker/easier.\nIf you’re using t_test from rstatix (which you are likely familiar with, if you took the Core statistics course before this one):\n\n# The data must be saved as a dataframe to use t_test\ndata &lt;- rnorm(n, mean_n, sd_n) %&gt;%\n  data.frame()\n\nt_result &lt;- t_test(data, .~1)\n\nt_values[i] &lt;- unname(t_result$statistic)\n\nIf you’re trying to sample from the t-distribution, note that the function requires different parameters - specifically, we specify the degrees of freedom df (and optionally, a non-centrality parameter ncp):\n\nrt(n = 1000, df = 99)\n\n\n\nThere are a few different functions for running t-tests in Python. If you took the Core statistics course, you’re likely familiar with the ttest function from pingouin.\nFor ease of use in this exercise, however, it’s easier to extract the t-statistic on each loop by indexing the output from the ttest_mean from statsmodels:\n\nsms.DescrStatsW(data).ttest_mean()[0]\n\nTo sample from the t-distribution, use np.random.standard_t:\n\nnp.random.standard_t(df = 99, size = 1000)\n\nYou will need to provide two arguments: the degrees of freedom df, and the sample size.\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Distributions all the way down\nThose of you with imagination might be wondering: if one for loop can construct a single histogram, giving us the distribution of the set of sample means, why can’t we run multiple for loops and look at the distribution of the set of means of the set of means?\nThe short answer is: we can!\n\n\n\nIt’s distributions all the way down; image source\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nLevel: \nYour mission, should you choose to accept it, is to plot a single histogram that captures the distribution of the set of means of the set of means.\nAdapt the code in Section 5.3.1 by nesting one for loop inside the other, to produce a single histogram. It should represent the set of means of the set of sample means.\nBefore you do, consider:\n\nWhat shape will the histogram take, if you run enough iterations?\nIf someone asks what you did on this course, how on earth will you explain this to them?!\n\n\n\n\n\n\n\nWorked answer\n\n\n\n\n\nTo make the code a little easier to parse and eliminate any clashes in the environment, the outside for loop uses j for indexing instead of i.\nWe’ve also kept the number of iterations low-ish. You might notice, if you use a large value like 1000 for each of the loops, it takes a while to run (because you’re actually asking for 1000000 total iterations!)\n\nRPython\n\n\n\nmeans_j &lt;- c()\n\nfor (j in 1:300) {\n\n  means_i &lt;- c()\n\n  for (i in 1:300) {\n  \n    n &lt;- 20\n    min_n &lt;- 1\n    max_n &lt;- 7\n  \n    means_i[i] &lt;- mean(runif(n, min_n, max_n))\n\n  }\n  \n  means_j[j] &lt;- mean(means_i)\n  \n}\n\nhist(means_j)\nabline(v = mean(means_j), col = \"red\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\n\nmeans_j = []\n\nfor j in range(300):\n  \n  means_i = []\n  \n  for i in range(300):\n    n = 20\n    lower = 1\n    upper = 7\n    imean = np.random.uniform(lower, upper, n).mean()\n    means_i.append(imean)\n  \n  jmean = np.mean(means_i)\n  means_j.append(jmean)\n  \nplt.clf()\nplt.hist(means_j)\nplt.axvline(x = np.mean(means_j), color = 'r')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nIt’s still normal. If this is what you guessed, well done.\nIf you were to take this further, and add an infinite number of nested loops, you’d get a normal distribution each time.\nAnd it doesn’t even matter what distribution we sample our individual datasets from, as we showed above. In this worked example code we’ve used the uniform distribution, but you can try swapping it out for any other distribution - you’ll still get a normal distribution at each “layer”, which gets clearer and clear the more iterations you run.\nNow, statistics may not be considered the “coolest” subject in the world - but I think that’s pretty awesome, don’t you?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The central limit theorem</span>"
    ]
  },
  {
    "objectID": "materials/03-central-limit-theorem.html#summary",
    "href": "materials/03-central-limit-theorem.html#summary",
    "title": "5  The central limit theorem",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nSimulation is a great way to help get your head around more difficult or abstract statistical concepts, without needing to worry about the mathematical formulae.\nThe central limit theorem is incredibly boring to explain via equations, and much less easy to get your head around - so why not just simulate?\n\n\n\n\n\n\nKey Points\n\n\n\n\nWe are more likely to accurately and precisely “recover” the true population parameters when our sample is large and unbiased\nThe central limit theorem shows that, across a number of samples, the set of estimates of a given parameter will follow an approximately normal distribution\nWhen the number of samples = \\(\\infty\\), it will be perfectly normal\nThis is true regardless of the original distribution that the individual samples come from",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The central limit theorem</span>"
    ]
  },
  {
    "objectID": "materials/04-confidence-intervals.html",
    "href": "materials/04-confidence-intervals.html",
    "title": "6  Confidence intervals",
    "section": "",
    "text": "6.1 Libraries and functions\nIn this chapter, we’ll use simulation to answer the deceptively simple question: what are confidence intervals, and how should they be interpreted?\nConfidence intervals, like p-values, are misunderstood surprisingly often in applied statistics. It’s understandable that this happens, because a single set of confidence intervals by itself might not mean very much, unless you understand more broadly where they come from.\nSimulation is a fantastic way to gain this understanding.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "materials/04-confidence-intervals.html#libraries-and-functions",
    "href": "materials/04-confidence-intervals.html#libraries-and-functions",
    "title": "6  Confidence intervals",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nfrom scipy.stats import t",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "materials/04-confidence-intervals.html#extracting-confidence-intervals-for-a-single-sample",
    "href": "materials/04-confidence-intervals.html#extracting-confidence-intervals-for-a-single-sample",
    "title": "6  Confidence intervals",
    "section": "6.2 Extracting confidence intervals for a single sample",
    "text": "6.2 Extracting confidence intervals for a single sample\nLet’s start by showing how we can calculate confidence intervals from just one dataset.\nWe’re still using one-dimensional data, so our confidence intervals in this case are for the mean.\n\nRPython\n\n\n\nMethod 1\nMethod 1 in R uses the t.test function. This is simpler in our current one-dimensional situation.\nThe confidence intervals are a standard part of the t-test, and we can use the $ syntax to extract them specifically from the output:\n\nset.seed(21)\n\nn &lt;- 40\nmean_n &lt;- 6\nsd_n &lt;- 1.5\n\ndata &lt;- rnorm(n, mean_n, sd_n)\n\nconf.int &lt;- t.test(data, conf.level = 0.95)$conf.int\n\nprint(conf.int)\n\n[1] 5.647455 6.678777\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nMethod 2\nMethod 2 in R is via the confint function, which takes an lm object as its first argument.\nWe’ve had to do a bit of extra work to be able to use the lm function in this case (making our data into a dataframe), but in future sections of the course where we simulate multi-dimensional data, we’ll have to do this step anyway - so this method may work out more efficient later on.\n\nset.seed(21)\n\nn &lt;- 40\nmean_n &lt;- 6\nsd_n &lt;- 1.5\n\ndata &lt;- rnorm(n, mean_n, sd_n) %&gt;%\n  data.frame()\n\nlm_data &lt;- lm(. ~ 1, data)\n\nconfint(lm_data, level = 0.95)\n\n               2.5 %   97.5 %\n(Intercept) 5.647455 6.678777\n\n\n\n\n\nThere are a few options for extracting confidence intervals in Python, but perhaps the most efficient is via statsmodels.stats.api:\n\nnp.random.seed(20)\n\nn = 40\nmean = 6\nsd = 1.5\n\ndata = np.random.normal(mean, sd, n)\n\nsms.DescrStatsW(data).tconfint_mean()\n\n(5.241512557961966, 6.341197862942866)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "materials/04-confidence-intervals.html#sec-exm_loop-confint",
    "href": "materials/04-confidence-intervals.html#sec-exm_loop-confint",
    "title": "6  Confidence intervals",
    "section": "6.3 Extracting multiple sets of confidence intervals",
    "text": "6.3 Extracting multiple sets of confidence intervals\nNow, let’s use a for loop to extract and save multiple sets of confidence intervals.\nWe’ll stick to the same parameters we used above:\n\nRPython\n\n\n\nset.seed(21)\n\n# Soft-code the number of iterations\niterations &lt;- 100\n\n# Soft-code the simulation parameters\nn &lt;- 40\nmean_n &lt;- 6\nsd_n &lt;- 1.5\n\n# Initialise a dataframe to store the results of the iterations\nintervals &lt;- data.frame(mean = numeric(iterations),\n                        lower = numeric(iterations),\n                        upper = numeric(iterations))\n\n# Run simulations\nfor (i in 1:iterations) {\n  \n  data &lt;- rnorm(n, mean_n, sd_n) %&gt;%\n    data.frame()\n\n  lm_data &lt;- lm(. ~ 1, data)\n  \n  # Extract mean and confidence intervals as simple numeric objects\n  est_mean &lt;- unname(coefficients(lm_data)[1])\n  est_lower &lt;- confint(lm_data, level = 0.95)[1]\n  est_upper &lt;- confint(lm_data, level = 0.95)[2]\n  \n  # Update appropriate row of empty intervals object, with values from this loop\n  intervals[i,] &lt;- data.frame(mean = est_mean, lower = est_lower, upper = est_upper)\n  \n}\n\nhead(intervals)\n\n      mean    lower    upper\n1 6.163116 5.647455 6.678777\n2 6.112842 5.601488 6.624196\n3 5.771928 5.313615 6.230240\n4 5.926402 5.489371 6.363432\n5 5.916349 5.481679 6.351019\n6 6.180240 5.742182 6.618298\n\n\n\n\n\nnp.random.seed(30)\n\niterations = 100\n\nn = 40\nmean = 6\nsd = 1.5\n\nrows = []\n\nfor i in range(iterations):\n  data = np.random.normal(mean, sd, n)\n  \n  estmean = statistics.mean(data)\n  lower = sms.DescrStatsW(data).tconfint_mean()[0]\n  upper = sms.DescrStatsW(data).tconfint_mean()[1]\n  \n  rows.append({'mean': estmean, 'lower': lower, 'upper': upper})\n\nintervals = pd.DataFrame(rows)\n\nintervals.head()\n\n       mean     lower     upper\n0  5.787213  5.223520  6.350906\n1  5.760226  5.256582  6.263870\n2  6.176261  5.716103  6.636418\n3  6.214928  5.795727  6.634128\n4  5.988544  5.452712  6.524377\n\n\n\n\n\nJust by looking at the first few sets of intervals, we can see - as expected - that our set of estimated means are varying around the true population value (in an approximately normal manner, according to the central limit theorem, as we now know).\nWe can also see that our confidence intervals are approximately following the mean estimate in each case. When the mean estimate is a bit high or a bit low relative to the true value, our confidence intervals are shifted up or down a bit, such that the estimated mean sits in the middle of the confidence intervals for each individual dataset.\nIn other words: each confidence interval is a property of its dataset.\nTo get a clearer picture, let’s visualise them:\n\nRPython\n\n\n\nintervals %&gt;%\n  ggplot(aes(x = 1:iterations)) +\n    geom_point(aes(y = mean)) +\n    geom_segment(aes(y = lower, yend = upper)) +\n    geom_hline(yintercept = mean_n, colour = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(iterations):\n    ax.plot([i + 1, i + 1], [intervals['lower'][i], intervals['upper'][i]], color='black')\n    ax.plot(i + 1, intervals['mean'][i], 'o', color='black')\n\nax.axhline(y=mean, color='red', linestyle='--', label='True Mean')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFrom this plot, with the true population mean overlaid, we can see that most of the confidence intervals are managing to capture that true value. But a small proportion aren’t.\nWhat proportion of the intervals are managing to capture the true population mean?\nWe can check like so:\n\nRPython\n\n\n\nmean(mean_n &lt;= intervals$upper & mean_n &gt;= intervals$lower)\n\n[1] 0.95\n\n\n\n\n\ncontains_true = (intervals['lower'] &lt;= mean) & (intervals['upper'] &gt;= mean)\ncontains_true.mean()\n\n0.95\n\n\n\n\n\nGiven that we set our confidence intervals at the 95% level, this is exactly what the simulation should reveal: approximately (in this case, exactly) 95 of our 100 confidence intervals contain the true population mean.\n\n\n\n\n\n\nThe confidence is about the intervals, not the parameter\n\n\n\nThe very definition of 95% confidence intervals is this: we expect that the confidence intervals from 95% of the samples drawn from a given population with a certain parameter, to contain that true population parameter.\nThis is not equivalent to saying that there is a 95% chance that the true population value falls inside a given interval. This is a common misconception, but there is no probability associated with the true population value - it just is what it is (even if we don’t know it).\nAs with p-values, the probability is associated with datasets/samples when talking about confidence intervals, not with the underlying population.\n\n\n\n\n\n\n\n\nCalculating confidence intervals manually\n\n\n\n\n\nFor those of you who are curious about the underlying mathematical formulae for confidence intervals, and how to calculate them manually, it’s done like so:\n\nCalculate the sample mean\nCalculate the (estimated) standard error of the mean\nFind the t-score* that corresponds to the confidence level (e.g., 95%)\nCalculate the margin of error and construct the confidence interval\n\n*You can use z-scores, but t-scores tend to be more appropriate for small samples.\n\nRPython\n\n\nLet’s start by simulating a simple dataset.\n\nset.seed(21)\n\nn &lt;- 40\nmean_n &lt;- 6\nsd_n &lt;- 1.5\n\ndata &lt;- rnorm(n, mean_n, sd_n)\n\n\nStep 1: Calculate the sample mean\n\nsample_mean &lt;- mean(data)\nprint(sample_mean)\n\n[1] 6.163116\n\n\n\n\nStep 2: Calculate the standard error of the mean\nWe do this by dividing the sample standard deviation by the square root of the sample size, \\(\\frac{s}{\\sqrt{N}}\\).\n\nsample_se &lt;- sd(data)/sqrt(n)\nprint(sample_se)\n\n[1] 0.2549382\n\n\n\n\nStep 3: Calculate the t-score corresponding to the confidence level\nThis step also gives a clue as to how the significance threshold (or \\(\\alpha\\)) is associated with confidence level (they add together to equal 1).\n\nalpha &lt;- 0.05\n\nsample_df &lt;- n - 1\n\nt_score = qt(p = alpha/2, df = sample_df, lower.tail = FALSE)\nprint(t_score)\n\n[1] 2.022691\n\n\n\n\nStep 4: Calculate the margin of error and construct the confidence interval\n\n# How many standard deviations away from the mean, is the margin of error?\nmargin_error &lt;- t_score * sample_se\n\n# Calculate upper & lower bounds around the mean\nlower_bound &lt;- sample_mean - margin_error\nupper_bound &lt;- sample_mean + margin_error\n\nprint(c(lower_bound,upper_bound))\n\n[1] 5.647455 6.678777\n\n\nIf we compare that to what we would’ve gotten, if we’d used a function to do it for us:\n\ndata %&gt;%\n  data.frame() %&gt;%\n  lm(data = ., formula = . ~ 1, ) %&gt;%\n  confint(level = 0.95)\n\n               2.5 %   97.5 %\n(Intercept) 5.647455 6.678777\n\n\n… we can indeed see that we get exactly the same values.\n\n\n\nLet’s start by simulating a simple dataset.\n\nnp.random.seed(20)\n\nn = 40\nmean = 6\nsd = 1.5\n\ndata = np.random.normal(mean, sd, n)\n\n\nStep 1: Calculate the sample mean and standard deviation\n\nsample_mean = np.mean(data)\nsample_sd = np.std(data)\nprint(sample_mean, sample_sd)\n\n5.791355210452416 1.69762282725244\n\n\n\n\nStep 2: Calculate the standard error of the mean\nWe do this by dividing the sample standard deviation by the square root of the sample size, \\(\\frac{s}{\\sqrt{N}}\\).\n\nsample_se = sample_sd/np.sqrt(n)\nprint(sample_se)\n\n0.26841773710061373\n\n\n\n\nStep 3: Calculate the t-score corresponding to the confidence level\nThis step also gives a clue as to how the significance threshold (or \\(\\alpha\\)) is associated with confidence level (they add together to equal 1).\n\nfrom scipy.stats import t\n\nalpha = 0.05\nsample_df = n-1\n\nt_crit = t.ppf(1-alpha/2, sample_df)\nprint(t_crit)\n\n2.022690911734728\n\n\n\n\nStep 4: Calculate the margin of error and construct the confidence interval\nFirst, we find the margin of error: how many standard deviations away from the mean is our cut-off?\nThen, we use that to find the upper and lower bounds, around the mean.\n\nmargin_error = t_crit * sample_se\n\nci = (sample_mean - margin_error, sample_mean + margin_error)\n\nprint(ci)\n\n(5.248429093070603, 6.334281327834229)\n\n\nIf we compare that to what we would’ve gotten, if we’d used a function to do it for us:\n\nsms.DescrStatsW(data).tconfint_mean()\n\n(5.241512557961966, 6.341197862942866)\n\n\n… we can indeed see that we get the same interval, plus or minus some tiny differences in numerical precision from the different functions used.\n\n\n\n\nIf you think about things in a maths-ier way, it can be helpful to know how something is calculated - but you will probably always use existing functions when actually coding this stuff!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "materials/04-confidence-intervals.html#exercises",
    "href": "materials/04-confidence-intervals.html#exercises",
    "title": "6  Confidence intervals",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\n6.4.1 Width of confidence intervals\nThere are multiple factors that will affect the width of confidence intervals.\nIn this exercise, you’ll test some of them, to get an intuition of how (and hopefully why).\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nLevel: \nUse the code in Section 6.3 as a starting point.\nVary the following parameters, and look at the impact on the width of the confidence intervals:\n\nThe sample size of each individual sample\nThe standard deviation of the underlying population\nThe confidence level (e.g., 95%, 99%, 50%)\n\nThink about the following questions:\n\nDoes the confidence interval get wider or narrower as these parameters increase? Why?\nWhat would happen (theoretically) if we set our desired confidence level to 100%, or our sample size to \\(n = \\infty\\)?\n\n\n\n\n\n\n\nPay attention to the y-axis!\n\n\n\nIf you keep the same seed while changing other parameters, you might get a series of plots that look identical.\nBut if you look more closely at the y-axis, you will sometimes notice the scale changing.\nTo combat this, you can manually set the y-axis limits, if you’d like.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "materials/04-confidence-intervals.html#summary",
    "href": "materials/04-confidence-intervals.html#summary",
    "title": "6  Confidence intervals",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nConfidence intervals are commonly misunderstood, which is really easy to do when you’re only thinking about one dataset.\nHowever, simulation allows us to look at a massive number of datasets that all come from the same underlying population, meaning we can look at multiple sets of confidence intervals - which puts the real interpretation of confidence intervals into context!\n\n\n\n\n\n\nKey Points\n\n\n\n\nIf you construct 100 sets of 95% confidence intervals, you should expect ~95 of them to contain the true population parameter\nThis is not the same as a 95% chance of the true population parameter being contained inside any individual set of confidence intervals\nThe probability is associated with the intervals, not with the parameter!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Statistical concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html",
    "href": "materials/05-continuous-predictors.html",
    "title": "7  Simulating continuous predictors",
    "section": "",
    "text": "7.1 Libraries and functions\nIn this and the next couple of chapters, we’re going to simulate a dataset about golden toads, an extinct species of amphibians.\nWe’ll simulate various predictor variables, and their relationship to the clutch size (number of eggs) produced by a given toad.\nIn this chapter, we’ll start by looking at continuous predictor variables.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#libraries-and-functions",
    "href": "materials/05-continuous-predictors.html#libraries-and-functions",
    "title": "7  Simulating continuous predictors",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\n# These packages will be used for evaluating the models we fit to our simulated data\nlibrary(performance)\nlibrary(ggResidpanel)\n\n# This package is optional/will only be used for later sections in this chapter\nlibrary(MASS)\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom patsy import dmatrix",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#step-1-set-seed-and-sample-size",
    "href": "materials/05-continuous-predictors.html#step-1-set-seed-and-sample-size",
    "title": "7  Simulating continuous predictors",
    "section": "7.2 Step 1: Set seed and sample size",
    "text": "7.2 Step 1: Set seed and sample size\nFirst, we set a seed and a sample size:\n\nRPython\n\n\n\nset.seed(20)\n\n# sample size\nn &lt;- 60\n\n\n\n\nnp.random.seed(25)\n\n# sample size\nn = 60",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#step-2-generate-values-of-predictor-variable",
    "href": "materials/05-continuous-predictors.html#step-2-generate-values-of-predictor-variable",
    "title": "7  Simulating continuous predictors",
    "section": "7.3 Step 2: Generate values of predictor variable",
    "text": "7.3 Step 2: Generate values of predictor variable\nThe next step is to generate our predictor variable.\nThere’s no noise or uncertainty in our predictor (remember that residuals are always in the y direction, not the x direction), so we can just produce the values by sampling from a distribution of our choice.\nOne of the things that can cause variation in clutch size is the size of the toad herself, so we’ll use that as our continuous predictor. This sort of biological variable would probably be normally distributed, so we’ll use rnorm to generate it.\nGoogle tells us that the average female golden toad was somewhere in the region of 42-56mm long, so we’ll use that as a sensible basis for our normal distribution for our predictor variable length.\n\nRPython\n\n\n\nlength &lt;- rnorm(n, 48, 3)\n\n\n\n\nlength = np.random.normal(48, 3, n)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#step-3-simulate-average-values-of-response-variable",
    "href": "materials/05-continuous-predictors.html#step-3-simulate-average-values-of-response-variable",
    "title": "7  Simulating continuous predictors",
    "section": "7.4 Step 3: Simulate average values of response variable",
    "text": "7.4 Step 3: Simulate average values of response variable\nNow, we need to simulate our response variable, clutchsize.\nWe’re going to do this by setting up the linear model. We’ll specify a y-intercept for clutchsize, plus a gradient that captures how much clutchsize changes as length changes.\n\nRPython\n\n\n\nb0 &lt;- 175\nb1 &lt;- 2\n\nsdi &lt;- 20\n\n\n\n\nb0 = 175\nb1 = 2\n\nsdi = 20\n\n\n\n\nWe’ve also added an sdi parameter. This captures the standard deviation around the model predictions that is due to other factors we’re not measuring. In other words, this will determine the size of our residuals.\nNow, we can simulate our set of predicted values for clutchsize.\n\nRPython\n\n\n\navg_clutch &lt;- b0 + b1*length\n\n\n\n\navg_clutch = b0 + b1*length\n\n\n\n\nYou’ll notice we’ve just written out the equation of our model.\n\nRPython\n\n\n\ntibble(length, avg_clutch) %&gt;%\n  ggplot(aes(x = length, y = avg_clutch)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe use the tibble function to combine our response and predictor variables together into a single dataset.\n\n\n\ntempdata = pd.DataFrame({'length': length, 'avg_clutch': avg_clutch})\n\ntempdata.plot.scatter(x = 'length', y = 'avg_clutch')\nplt.show()\n\n\n\n\n\n\n\n\nWe use the pd.DataFrame function to stitch our arrays together into a single dataframe object with multiple columns.\n\n\n\nWhen we visualise length and avg_clutch together, you see they perfectly form a straight line. That’s because avg_clutch doesn’t contain the residuals - that comes next.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#step-4-simulate-actual-values-of-response-variable",
    "href": "materials/05-continuous-predictors.html#step-4-simulate-actual-values-of-response-variable",
    "title": "7  Simulating continuous predictors",
    "section": "7.5 Step 4: Simulate actual values of response variable",
    "text": "7.5 Step 4: Simulate actual values of response variable\nThe final step is to simulate the actual values of clutch size.\nHere, we’ll be drawing from a normal distribution. We put avg_clutch in as our mean - this is because the set of actual clutch size values should be normally distributed around our set of predictions.\nOr, in other words, we want the residuals/errors to be normally distributed.\n\nRPython\n\n\n\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(clutchsize, length)\n\n\n\n\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'clutchsize': clutchsize})",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#step-5-checking-the-dataset",
    "href": "materials/05-continuous-predictors.html#step-5-checking-the-dataset",
    "title": "7  Simulating continuous predictors",
    "section": "7.6 Step 5: Checking the dataset",
    "text": "7.6 Step 5: Checking the dataset\nLet’s make sure our dataset is behaving the way we intended.\nFirst, we’ll visualise it:\n\nRPython\n\n\n\nggplot(goldentoad, aes(x = length, y = clutchsize)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\ngoldentoad.plot.scatter(x = 'length', y = 'clutchsize')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAnd then, we’ll construct a linear model - and check that our beta coefficients have been replicated to a sensible level of precision!\n\nRPython\n\n\n\nlm_golden &lt;- lm(clutchsize ~ length, goldentoad)\n\nsummary(lm_golden)\n\n\nCall:\nlm(formula = clutchsize ~ length, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.718 -10.973   1.094  11.941  41.690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 267.1395    38.4922   6.940  3.7e-09 ***\nlength        0.1065     0.8038   0.132    0.895    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.76 on 58 degrees of freedom\nMultiple R-squared:  0.0003025, Adjusted R-squared:  -0.01693 \nF-statistic: 0.01755 on 1 and 58 DF,  p-value: 0.8951\n\n\n\n\n\nmodel = smf.ols(formula= \"clutchsize ~ length\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.184\nModel:                            OLS   Adj. R-squared:                  0.170\nMethod:                 Least Squares   F-statistic:                     13.07\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           0.000630\nTime:                        11:43:20   Log-Likelihood:                -256.33\nNo. Observations:                  60   AIC:                             516.7\nDf Residuals:                      58   BIC:                             520.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    157.5257     30.614      5.146      0.000      96.245     218.806\nlength         2.2928      0.634      3.615      0.001       1.023       3.562\n==============================================================================\nOmnibus:                        0.516   Durbin-Watson:                   2.307\nProb(Omnibus):                  0.773   Jarque-Bera (JB):                0.492\nSkew:                          -0.207   Prob(JB):                        0.782\nKurtosis:                       2.839   Cond. No.                         649.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nNot bad at all. The linear model has managed to extract beta coefficients similar to the original b0 and b1 that we set.\nIf you’re looking to explore and understand this further, try exploring the following things in your simulation, and see how they affect the p-value and the precision of the beta estimates:\n\nVarying the sample size\nVarying the sdi\nVarying the b1 parameter",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#exercises",
    "href": "materials/05-continuous-predictors.html#exercises",
    "title": "7  Simulating continuous predictors",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\n\n7.7.1 A second continuous predictor\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nLevel: \nCreate at least one additional continuous predictor for the dataset (perhaps temperature or age of toad).\nFollow the same procedure described above:\n\nSet seed & sample size\nSet beta coefficients\nUse model equation to simulate average/expected values of clutchsize\nSimulate actual values of clutchsize, with random noise\nCheck the dataset\n\nDon’t worry about being biologically plausible - just pick some numbers. We’re focusing on the simulation here, not on perfect realism in the dataset!\n\n\n\n\n\n\n7.7.2 A brand new dataset\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nLevel: \nTo practice this process, set up another dataset of your own to simulate, containing at least one categorical predictor and a continuous response.\nYou’ll need to:\n\nSet the seed and sample size\nSet values of b0 and b1\nConstruct the predictor variable by sampling from some distribution\nSimulate the average and then actual values of the response\nConstruct a scatterplot and run a regression, to check your simulation ran as expected\n\nAdapt the code from the goldentoad example above to achieve this.\nFeel free to pick your own example, but if you’re looking for inspiration, here are a couple of ideas.\n(Remember - biological plausibility doesn’t actually matter, so don’t get too hung up on it!)\n\n\n\n\n\n\nInvasive plants\n\n\n\n\n\nContinuous response variable: Area invaded per year (m2/year)\nThe typical range will be between ~ 10-1000 m2/year.\nContinuous predictor: Annual rainfall (mm)\nA sensible range would be 400-1600mm. Many invasive plants will thrive in wetter environments.\nContinuous predictor: Soil pH\nA sensible range would be 4.5-8.5.\n\n\n\n\n\n\n\n\n\nGut microbiome & blood glucose\n\n\n\n\n\nContinuous response variable: Blood glucose level (mg/dL)\nSomewhere in the range 70-120 mg/dL would be typical (the upper end would be considered pre-diabetic).\nContinuous predictor: Fibre intake (g/day)\nWould usually be in the region of 5-50 g/day. Higher fibre would be associated with lower glucose, usually.\nContinuous predictor: Abundance of bacteroides (%)\nThe usual range would be 0-30%; higher levels sometimes correlate with better metabolism.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/05-continuous-predictors.html#summary",
    "href": "materials/05-continuous-predictors.html#summary",
    "title": "7  Simulating continuous predictors",
    "section": "7.8 Summary",
    "text": "7.8 Summary\nIn this chapter, we’ve simulated two-dimensional data for the first time.\nFirst, we construct a continuous variable to act as a predictor. Then, we can simulate our response variable as a function of the predictor variable(s) via a linear model equation, with residuals added.\nBy definition, the assumptions of the linear model will be always be met, because we are in control of the nature of the underlying population.\nHowever, our model may or may not do a good job of “recovering” the original beta coefficients we specified, depending on the sample size and the amount of error we introduce in our simulation.\n\n\n\n\n\n\nKey Points\n\n\n\n\nPredictor variables can be simulated from different distributions, with no errors associated\nResponse variables should be simulated with errors/residuals from the normal distribution\nTo do this, we need to specify the equation of the straight line, i.e., the intercept and slope beta coefficients, as parameters in the simulation",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html",
    "href": "materials/06-categorical-predictors.html",
    "title": "8  Simulating continuous predictors",
    "section": "",
    "text": "8.1 Libraries and functions\nThis chapter follows on closely from the previous chapter on continuous predictors.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#libraries-and-functions",
    "href": "materials/06-categorical-predictors.html#libraries-and-functions",
    "title": "8  Simulating continuous predictors",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\n# These packages will be used for evaluating the models we fit to our simulated data\nlibrary(performance)\nlibrary(ggResidpanel)\n\n# This package is optional/will only be used for later sections in this chapter\nlibrary(MASS)\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom patsy import dmatrix",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#the-pond-variable",
    "href": "materials/06-categorical-predictors.html#the-pond-variable",
    "title": "8  Simulating continuous predictors",
    "section": "8.2 The pond variable",
    "text": "8.2 The pond variable\nWe’re going to stick with our golden toads, and keep the continuous length predictor variable from the previous chapter, so that we can build up the complexity of our dataset gradually between chapters.\nTo add that complexity, we’re going to imagine that golden toads living in different ponds produce slightly different clutch sizes, and simulate some sensible data on that basis.\n\nClear previous simulation\nYou might find it helpful to delete variables/clear the global environment, so that nothing from your previous simulation has an unexpected impact on your new one:\n\nRPython\n\n\n\nrm(list=ls())\n\n\n\n\ndel(avg_clutch, clutchsize, goldentoad, length, lm_golden, model, tempdata)\n\n\n\n\n(This step is optional!)\n\n\nSimulating categorical predictors\nCategorical predictors are a tiny bit more complex to simulate, as the beta coefficients switch from being constants (gradients) to vectors (representing multiple means).\nHowever, we’re still going to follow the same workflow that we used with continuous predictors in the previous chapter:\n\nSet parameters (seed, sample size, beta coefficients and standard deviation)\nGenerate the values of our predictor variable\nSimulate average values of response variable\nSimulate actual values of response variable\nCheck the dataset",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#set-parameters",
    "href": "materials/06-categorical-predictors.html#set-parameters",
    "title": "8  Simulating continuous predictors",
    "section": "8.3 Set parameters",
    "text": "8.3 Set parameters\nExactly as we did before, we start by setting key parameters including:\n\nSeed, for reproducibility\nSample size n\nIndividual standard deviation sdi\nBeta coefficients\n\nWe need three betas now, since we’re adding a second predictor.\nTo generate the beta for pond, we need to specify a vector, instead of a single constant. Let’s keep things relatively simple and stick to just three ponds, which we’ll imaginatively call A, B and C.\nSince pond A will be our reference group, our beta does not need to contain any adjustment for that pond, hence the vector starts with a 0. The other two numbers then represent the difference in means between pond A and ponds B and C respectively.\n\nRPython\n\n\n\nset.seed(20)\n\nn &lt;- 60\nb0 &lt;- 175             # intercept\nb1 &lt;- 2               # main effect of length\nb2 &lt;- c(0, 30, -10)   # main effect of pond\n\nsdi &lt;- 20\n\n\n\n\nnp.random.seed(20)\n\nn = 60\nb0 = 175                        # intercept\nb1 = 2                          # main effect of length\nb2 = np.array([0, 30, -10])     # main effect of pond\n\nsdi = 20\n\n\n\n\nThis means that, in the reality we are creating here, the true mean clutch size for each of our three ponds is 175, 205 (175+30) and 165 (175-10).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#generate-values-for-predictor-variables",
    "href": "materials/06-categorical-predictors.html#generate-values-for-predictor-variables",
    "title": "8  Simulating continuous predictors",
    "section": "8.4 Generate values for predictor variable(s)",
    "text": "8.4 Generate values for predictor variable(s)\nNext, we need values for our length and pond predictors.\nWe already know how to generate length from the previous chapter.\nFor pond, we’re not going to sample from a distribution, because we need category names instead of numeric values. So, we’ll specify our category names and repeat them as appropriate:\n\nRPython\n\n\n\nlength &lt;- rnorm(n, 48, 3)\n\npond &lt;- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n\n\n\n\n\n\n\nrep and c functions\n\n\n\n\n\nThe rep and c functions in R (c short for concatenate) will come up quite a lot when generating predictor variables.\nThe rep function takes two arguments each time:\n\nFirst, an argument containing the thing that you want repeated (which can either be a single item, or a list/vector)\nSecond, an argument containing information about how many times to repeat the first argument (there are actually multiple options for how to phrase this second argument)\n\nThe c function is a little simpler: it combines, i.e., concatenates, all of the arguments you put into it into a single vector/list item.\nYou can then combine these two functions together in various ways to achieve what you want.\nFor example, these two lines of code both produce the same output:\n\nc(rep(\"A\", times = 3), rep(\"B\", times = 3))\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\"\n\nrep(c(\"A\", \"B\"), each = 3)\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\"\n\n\nThe first version asks for \"A\" to be repeated 3 times, \"B\" to be repeated 3 times, and then these two triplets to be concatenated together into one list.\nThe second version asks us to take a list of two items, \"A\", \"B\", and repeat each element 3 times each.\nMeanwhile, the code below will do something very different:\n\nrep(c(\"A\", \"B\"), times = 3)\n\n[1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\"\n\n\nThis line of code asks us to take the list \"A\", \"B\" and repeat it, as-is, 3 times. So we get a final result that alternates.\nThis shows that choosing carefully between times or each as your second argument for the rep function can be absolutely key to getting the right output!\nDon’t forget you can use ?rep or ?c to get more detailed information about how these functions work, if you would like it.\n\n\n\n\n\n\nlength = np.random.normal(48, 3, n)\n\npond = np.repeat([\"A\", \"B\", \"C\"], repeats = n//3)\n\n\n\n\n\n\n\nnp.repeat and np.tile functions\n\n\n\n\n\nWhen generating categorical variables, you are essentially repeating the category names multiple times.\nNote the difference between these two outputs, produced using two different numpy functions:\n\nnp.repeat([\"A\", \"B\", \"C\"], repeats = n//3)\n\narray(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',\n       'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B',\n       'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n       'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C',\n       'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'], dtype='&lt;U1')\n\n\n\nnp.tile([\"A\", \"B\", \"C\"], reps = n//3)\n\narray(['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A',\n       'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B',\n       'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C',\n       'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A',\n       'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'], dtype='&lt;U1')\n\n\nIn both functions, the first argument is an list of category names, with no duplication. Then, you specify the number of repeats with either repeats or reps (for np.repeats vs np.tile respectively).\nHowever, the two functions do slightly different things:\n\nnp.repeats takes each element of the list and repeats it a specified number of times, before moving on to the next element\nnp.tile takes the entire list as-is, and repeats it as a chunk for a desired number of times\n\nIn both cases we end up with an array of the same length, but the order of the category names within that list is very different.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#simulate-average-values-of-response",
    "href": "materials/06-categorical-predictors.html#simulate-average-values-of-response",
    "title": "8  Simulating continuous predictors",
    "section": "8.5 Simulate average values of response",
    "text": "8.5 Simulate average values of response\nNow, exactly as we did with the continuous predictor, we’re going to construct a linear model equation to calculate the average values of our response.\nHowever, including a categorical predictor in our model equation is a bit more complex.\nWe can no longer simply multiply our beta coefficient by our predictor, because our beta coefficient is a vector rather than a constant.\nInstead, we need to make use of something called a design matrix, and matrix multiplication, like so:\n\nRPython\n\n\n\nmodel.matrix(~0 + pond)\n\n   pondA pondB pondC\n1      1     0     0\n2      1     0     0\n3      1     0     0\n4      1     0     0\n5      1     0     0\n6      1     0     0\n7      1     0     0\n8      1     0     0\n9      1     0     0\n10     1     0     0\n11     1     0     0\n12     1     0     0\n13     1     0     0\n14     1     0     0\n15     1     0     0\n16     1     0     0\n17     1     0     0\n18     1     0     0\n19     1     0     0\n20     1     0     0\n21     0     1     0\n22     0     1     0\n23     0     1     0\n24     0     1     0\n25     0     1     0\n26     0     1     0\n27     0     1     0\n28     0     1     0\n29     0     1     0\n30     0     1     0\n31     0     1     0\n32     0     1     0\n33     0     1     0\n34     0     1     0\n35     0     1     0\n36     0     1     0\n37     0     1     0\n38     0     1     0\n39     0     1     0\n40     0     1     0\n41     0     0     1\n42     0     0     1\n43     0     0     1\n44     0     0     1\n45     0     0     1\n46     0     0     1\n47     0     0     1\n48     0     0     1\n49     0     0     1\n50     0     0     1\n51     0     0     1\n52     0     0     1\n53     0     0     1\n54     0     0     1\n55     0     0     1\n56     0     0     1\n57     0     0     1\n58     0     0     1\n59     0     0     1\n60     0     0     1\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$pond\n[1] \"contr.treatment\"\n\n\nThe model.matrix function produces a table of 0s and 1s, which is a matrix representing the design of our experiment. In this case, that’s three columns, one for each pond.\nThe number of columns in our model matrix matches the number of categories and the length of our b2 coefficient. Our b2 is also technically a matrix, just with one row.\nThis means we can use the %*% operator for matrix multiplication, to multiply these two things together:\n\nmodel.matrix(~0+pond) %*% b2\n\n   [,1]\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n11    0\n12    0\n13    0\n14    0\n15    0\n16    0\n17    0\n18    0\n19    0\n20    0\n21   30\n22   30\n23   30\n24   30\n25   30\n26   30\n27   30\n28   30\n29   30\n30   30\n31   30\n32   30\n33   30\n34   30\n35   30\n36   30\n37   30\n38   30\n39   30\n40   30\n41  -10\n42  -10\n43  -10\n44  -10\n45  -10\n46  -10\n47  -10\n48  -10\n49  -10\n50  -10\n51  -10\n52  -10\n53  -10\n54  -10\n55  -10\n56  -10\n57  -10\n58  -10\n59  -10\n60  -10\n\n\nThis gives us the adjustments we need to make, row by row, for our categorical predictor. For pond A we don’t need to make any, since that was the reference group.\n\navg_clutch &lt;- b0 + b1*length + model.matrix(~0+pond) %*% b2\n\n\n\n\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\n\nThe dmatrix function from patsy produces a table of 0s and 1s, which is a matrix representing the design of our experiment. In this case, it has three columns, which matches the number of categories and the length of our b2 coefficient.\nOur b2 is also technically a matrix, just with one row. We use np.dot to multiply these matrices together:\n\nnp.dot(Xpond, b2)\n\narray([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  30.,\n        30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,\n        30.,  30.,  30.,  30.,  30.,  30.,  30., -10., -10., -10., -10.,\n       -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,\n       -10., -10., -10., -10., -10.])\n\n\nThis gives us the adjustments we need to make, row by row, for our categorical predictor. For pond A we don’t need to make any, since that was the reference group.\nWe can then add all these adjustments to the rest of our model from the previous chapter, to produce our expected values:\n\navg_clutch = b0 + b1*length + np.dot(Xpond, b2)\n\n\n\n\nDon’t worry - you don’t really need to understand matrix multiplication to get used to this method. If that explanation was enough for you, you’ll be just fine from here.\nWe’ll use this syntax a few more times in this chapter, so you’ll learn to recognise and repeat the syntax!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#simulate-actual-values-of-response",
    "href": "materials/06-categorical-predictors.html#simulate-actual-values-of-response",
    "title": "8  Simulating continuous predictors",
    "section": "8.6 Simulate actual values of response",
    "text": "8.6 Simulate actual values of response\nThe last step is identical to the previous chapter.\nWe sample our actual values of clutchsize from a normal distribution with avg_clutch as the mean and with a standard deviation of sdi:\n\nRPython\n\n\n\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(clutchsize, length, pond)\n\n\n\n\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'pond': pond, 'clutchsize': clutchsize})",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#checking-the-dataset",
    "href": "materials/06-categorical-predictors.html#checking-the-dataset",
    "title": "8  Simulating continuous predictors",
    "section": "8.7 Checking the dataset",
    "text": "8.7 Checking the dataset\nOnce again, we’ll visualise and model these data, to check that they look as we suspected they would.\n\nRPython\n\n\n\nlm_golden &lt;- lm(clutchsize ~ length + pond, goldentoad)\n\nsummary(lm_golden)\n\n\nCall:\nlm(formula = clutchsize ~ length + pond, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.202 -11.751  -0.899  14.254  37.039 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 262.8111    38.6877   6.793  7.6e-09 ***\nlength        0.1016     0.8108   0.125    0.901    \npondB        39.2084     5.9116   6.632  1.4e-08 ***\npondC        -5.5279     5.9691  -0.926    0.358    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.69 on 56 degrees of freedom\nMultiple R-squared:  0.5481,    Adjusted R-squared:  0.5239 \nF-statistic: 22.64 on 3 and 56 DF,  p-value: 9.96e-10\n\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nmodel = smf.ols(formula= \"clutchsize ~ length + pond\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.354\nModel:                            OLS   Adj. R-squared:                  0.319\nMethod:                 Least Squares   F-statistic:                     10.23\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           1.80e-05\nTime:                        11:43:42   Log-Likelihood:                -263.47\nNo. Observations:                  60   AIC:                             534.9\nDf Residuals:                      56   BIC:                             543.3\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    196.9612     37.710      5.223      0.000     121.419     272.504\npond[T.B]     24.0615      6.419      3.749      0.000      11.204      36.919\npond[T.C]    -10.4082      6.426     -1.620      0.111     -23.282       2.465\nlength         1.5491      0.781      1.984      0.052      -0.015       3.114\n==============================================================================\nOmnibus:                        3.489   Durbin-Watson:                   2.017\nProb(Omnibus):                  0.175   Jarque-Bera (JB):                1.803\nSkew:                          -0.074   Prob(JB):                        0.406\nKurtosis:                       2.164   Cond. No.                         695.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nHas our model recreated “reality” very well? Would we draw the right conclusions from it?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#exercises",
    "href": "materials/06-categorical-predictors.html#exercises",
    "title": "8  Simulating continuous predictors",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\n\n8.8.1 A second categorical predictor\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nLevel: \nCreate at least one additional categorical predictor for this dataset (perhaps vegetation cover or presence of predators at the time the toad was laying her eggs).\nRemember that you don’t need to have values that are totally biologically plausible - this is just about simulation practice.\n\n\n\n\n\n\n8.8.2 Continuing your own simulation\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nLevel: \nLast chapter, the second exercise (Section 7.7.2) encouraged you to simulate a brand new dataset of your own, for practice.\nContinue with that simulation, this time adding a categorical predictor or two.\nYou’ll need to:\n\nSet the seed and sample size\nSet sensible beta coefficients (remember how reference groups work)\nConstruct the predictor variable (repeating instances of your group/category names)\nSimulate the average and then actual values of the response\nVisualise and model the data, to check your simulation ran as expected\n\nAdapt the code from the goldentoad example to achieve this.\nIf you used one of the inspiration examples last chapter, here’s some ideas of possible categorical predictors you could add:\n\n\n\n\n\n\nInvasive plants\n\n\n\n\n\nContinuous response variable: Area invaded per year (m2/year)\nThe typical range will be between ~ 10-1000 m2/year.\nCategorical predictor: Region (coastal, inland, mountains)\nOn average, the rate of invasion is probably faster in coastal regions, and slower in mountainous ones.\nCategorical predictor: Species (kudzu, knotweed, starthistle)\nKudzu spreads particularly fast!\n\n\n\n\n\n\n\n\n\nGut microbiome & blood glucose\n\n\n\n\n\nContinuous response variable: Blood glucose level (mg/dL)\nSomewhere in the range 70-120 mg/dL would be typical (the upper end would be considered pre-diabetic).\nPossible categorical predictors: Diet type (vegetarian, vegan, omnivore)\nCompared to “normal” omnivores, vegetarians/vegans will probably have lower blood glucose levels.\nCategorical predictor: Antibiotic use in the last 6 months (yes/no)\nRecent antibiotic use will probably disrupt the microbiome, and lead to poorer glucose metabolism/higher blood glucose levels.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/06-categorical-predictors.html#summary",
    "href": "materials/06-categorical-predictors.html#summary",
    "title": "8  Simulating continuous predictors",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nWhether our predictor is categorical or continuous, we still need to follow the same workflow to simulate the dataset.\nHowever, categorical predictors are a touch more complicated to simulate - we need a couple of extra functions, and it’s conceptually a little harder to get used to.\n\n\n\n\n\n\nKey Points\n\n\n\n\nCategorical predictors require vectors for their beta coefficients, unlike continuous predictors that just have constants\nThis means we need to use a design matrix to multiply our vector beta coefficient by our categorical variable\nOtherwise, the procedure is identical to simulating with a continuous predictor\nYou can include any number of continuous and categorical predictors in the same simulation",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulating continuous predictors</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html",
    "href": "materials/07-interactions.html",
    "title": "9  Simulating interactions",
    "section": "",
    "text": "9.1 Libraries and functions\nIn this chapter, we’re going to simulate a few different possible interactions for the goldentoad dataset we’ve been building.\nAs with main effects, categorical interactions are a little trickier than continuous ones, so we’ll work our way up.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html#libraries-and-functions",
    "href": "materials/07-interactions.html#libraries-and-functions",
    "title": "9  Simulating interactions",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\nlibrary(performance)\nlibrary(ggResidpanel)\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport pingouin as pg\n\nfrom patsy import dmatrix",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html#two-continuous-predictors",
    "href": "materials/07-interactions.html#two-continuous-predictors",
    "title": "9  Simulating interactions",
    "section": "9.2 Two continuous predictors",
    "text": "9.2 Two continuous predictors\nThe easiest type of interaction to simulate is a two-way interaction between continuous predictors.\nWe’ll use the length predictor from before, and add a new continuous predictor, temperature.\n(I don’t know whether temperature actually does predict clutch size in toads - remember, this is made up!)\n\nSet up parameters and predictors\nFirst, we set the important parameters. This includes the beta coefficients.\n\n\n\n\n\n\nYou might notice…\n\n\n\n\n\nthat we’ve increase the sample size from previous chapters, and tweaked beta0.\nThis is to give our model a fighting chance to recover some sensible estimates at the end, and also to keep the values of our final clutchsize variable within some sensible biological window.\nHowever, all of this is optional - the process of actually doing the simulation would work the same even with the old values!\n\n\n\n\nRPython\n\n\n\nset.seed(22)\n\nn &lt;- 120\nb0 &lt;- -30             # intercept\nb1 &lt;- 0.7               # main effect of length\nb2 &lt;- 0.5             # main effect of temperature\n\nb3 &lt;- 0.25             # interaction of length:temperature\n\nsdi &lt;- 12\n\n\n\n\nnp.random.seed(28)\n\nn = 120\nb0 = -30            # intercept\nb1 = 0.7            # main effect of length\nb2 = 0.5            # main effect of temperature\n\nb3 = 0.25           # interaction of length:temperature\n\nsdi = 12\n\n\n\n\nNotice that the beta coefficient for the interaction is just a single constant - this is always true for an interaction between continuous predictors.\nNext, we generate the values for length and temperature:\n\nRPython\n\n\n\nlength &lt;- rnorm(n, 48, 3)\n\ntemperature &lt;- runif(n, 10, 32)\n\n\n\n\nlength = np.random.normal(48, 3, n)\n\ntemperature = np.random.uniform(10, 32, n)\n\n\n\n\nJust for a bit of variety, we’ve sampled temperature from a uniform distribution instead of a normal one.\nIt won’t make any difference at all to the rest of the workflow, but if you’d like, you can test both ways to see whether it has an impact on the visualisation and model at the end!\n\n\nSimulate response variable\nThese steps should look familiar from previous chapters.\n\nRPython\n\n\n\navg_clutch &lt;- b0 + b1*length + b2*temperature + b3*length*temperature\n\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(length, temperature, clutchsize)\n\n\n\n\navg_clutch = b0 + b1*length + b2*temperature + b3*length*temperature\n\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'temperature': temperature, 'clutchsize': clutchsize})\n\n\n\n\n\n\nCheck the dataset\nFirst, let’s visualise the dataset.\nThis isn’t always easy, with two continuous variables, but one way that gives us at least some idea is to assign one of our continuous predictors to the colour aesthetic:\n\nRPython\n\n\n\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = temperature)) +\n  geom_point(size = 3) +\n  scale_colour_continuous(type = \"viridis\")\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\nplt.scatter(goldentoad[\"length\"], goldentoad[\"clutchsize\"], c=goldentoad[\"temperature\"],  \n    cmap=\"viridis\", s=40)          # optional - set colourmap and point size                \n# add labels\nplt.xlabel(\"Length\"); plt.ylabel(\"Clutch Size\"); plt.colorbar(label=\"Temperature\")\n\nText(0.5, 0, 'Length')\nText(0, 0.5, 'Clutch Size')\n&lt;matplotlib.colorbar.Colorbar object at 0x00000184E6146F90&gt;\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBroadly speaking, clutchsize is increasing with both length and temperature, which is good - we specified positive betas for both main effects.\nSince we specified a positive beta for the interaction, we would expect there to be a bigger increase in clutchsize per unit increase in length, for each unit increase in temperature.\nVisually, that should look like the beginnings of a “trumpet” or “megaphone” shape in the data; you’re more likely to see that with a larger sample size.\nNext, let’s fit the linear model and see if we can recover those beta coefficients:\n\nRPython\n\n\n\nlm_golden &lt;- lm(clutchsize ~ length * temperature, data = goldentoad)\n\nsummary(lm_golden)\n\n\nCall:\nlm(formula = clutchsize ~ length * temperature, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.480  -8.079   0.191   8.796  36.987 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -60.41655   71.12790  -0.849 0.397404    \nlength               1.23202    1.49139   0.826 0.410448    \ntemperature          0.11301    3.52109   0.032 0.974452    \nlength:temperature   0.26105    0.07404   3.526 0.000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.05 on 116 degrees of freedom\nMultiple R-squared:  0.9718,    Adjusted R-squared:  0.9711 \nF-statistic:  1332 on 3 and 116 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nmodel = smf.ols(formula= \"clutchsize ~ length * temperature\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.984\nModel:                            OLS   Adj. R-squared:                  0.984\nMethod:                 Least Squares   F-statistic:                     2370.\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):          7.23e-104\nTime:                        11:49:52   Log-Likelihood:                -458.42\nNo. Observations:                 120   AIC:                             924.8\nDf Residuals:                     116   BIC:                             936.0\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept            -16.2454     54.243     -0.299      0.765    -123.681      91.190\nlength                 0.3842      1.138      0.338      0.736      -1.870       2.639\ntemperature           -0.6613      2.488     -0.266      0.791      -5.589       4.267\nlength:temperature     0.2755      0.052      5.313      0.000       0.173       0.378\n==============================================================================\nOmnibus:                        1.282   Durbin-Watson:                   2.077\nProb(Omnibus):                  0.527   Jarque-Bera (JB):                0.818\nSkew:                           0.072   Prob(JB):                        0.664\nKurtosis:                       3.378   Cond. No.                     5.58e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.58e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\nNot bad. Not brilliant, but not terrible!\nOut of interest, let’s also fit a model that we know is incorrect - one that doesn’t include the interaction effect:\n\nRPython\n\n\n\nlm_golden &lt;- lm(clutchsize ~ length + temperature, data = goldentoad)\n\nsummary(lm_golden)\n\n\nCall:\nlm(formula = clutchsize ~ length + temperature, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.767  -9.325   0.651   9.986  37.804 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -301.4146    20.5996  -14.63   &lt;2e-16 ***\nlength         6.3030     0.4131   15.26   &lt;2e-16 ***\ntemperature   12.5066     0.2113   59.18   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.68 on 117 degrees of freedom\nMultiple R-squared:  0.9688,    Adjusted R-squared:  0.9682 \nF-statistic:  1815 on 2 and 117 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nmodel = smf.ols(formula= \"clutchsize ~ length + temperature\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.980\nModel:                            OLS   Adj. R-squared:                  0.980\nMethod:                 Least Squares   F-statistic:                     2872.\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):          3.65e-100\nTime:                        11:49:52   Log-Likelihood:                -471.49\nNo. Observations:                 120   AIC:                             949.0\nDf Residuals:                     117   BIC:                             957.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept    -289.7312     19.003    -15.247      0.000    -327.365    -252.097\nlength          6.1167      0.403     15.169      0.000       5.318       6.915\ntemperature    12.5306      0.179     69.955      0.000      12.176      12.885\n==============================================================================\nOmnibus:                        0.723   Durbin-Watson:                   2.145\nProb(Omnibus):                  0.697   Jarque-Bera (JB):                0.326\nSkew:                          -0.007   Prob(JB):                        0.850\nKurtosis:                       3.255   Cond. No.                         876.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nWithout the interaction term, our estimates are wildly wrong - or least, much more wrong than they were with the interaction.\nThis is a really nice illustration of how important it is to check for interactions when modelling data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html#one-categorical-one-continuous-predictor",
    "href": "materials/07-interactions.html#one-categorical-one-continuous-predictor",
    "title": "9  Simulating interactions",
    "section": "9.3 One categorical & one continuous predictor",
    "text": "9.3 One categorical & one continuous predictor\nThe next type of interaction we’ll look at is between one categorical and one continuous predictor. This is the type of interaction you’d see in a grouped linear regression.\nWe’ll use our two predictors from the previous chapters, length and pond.\n\nSet up parameters and predictors\nSince at least one of the variables in our interaction is a categorical predictor, the beta coefficient for the interaction will need to be a vector.\nThink of it this way: our model with an interaction term will consist of three lines of best fit, each with a different intercept and gradient. The difference in intercepts is captured by b2, and then the difference in gradients is captured by b3.\n\nRPython\n\n\n\nrm(list=ls()) # optional clean-up\n\nset.seed(20)\n\nn &lt;- 60\nb0 &lt;- 175                 # intercept\nb1 &lt;- 2                   # main effect of length\nb2 &lt;- c(0, 30, -10)       # main effect of pond\nb3 &lt;- c(0, 0.5, -0.2)     # interaction of length:pond\n\nsdi &lt;- 12\n\nlength &lt;- rnorm(n, 48, 3)\npond &lt;- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n\n\n\n\ndel(length, pond, goldentoad, clutchsize, avg_clutch) # optional clean-up\n\n\nnp.random.seed(23)\n\nn = 60\nb0 = 175                          # intercept\nb1 = 2                            # main effect of length\nb2 = np.array([0, 30, -10])       # main effect of pond\nb3 = np.array([0, 0.5, -0.2])     # interaction of length:pond\n\nsdi = 12\n\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\n\n\n\n\nSimulating the values for length and pond themselves is no different to how we did it in previous chapters.\n\n\nSimulate response variable\nOnce again, we use our two-step procedure to simulate our response variable.\nSince our interaction is categorical (i.e., contains a categorical predictor), we will need to create a design matrix for it.\n\nRPython\n\n\n\nmodel.matrix(~0+length:pond)\n\n   length:pondA length:pondB length:pondC\n1      51.48806      0.00000      0.00000\n2      46.24223      0.00000      0.00000\n3      53.35640      0.00000      0.00000\n4      44.00222      0.00000      0.00000\n5      46.66030      0.00000      0.00000\n6      49.70882      0.00000      0.00000\n7      39.33085      0.00000      0.00000\n8      45.39294      0.00000      0.00000\n9      46.61489      0.00000      0.00000\n10     46.33338      0.00000      0.00000\n11     47.93959      0.00000      0.00000\n12     47.54885      0.00000      0.00000\n13     46.11562      0.00000      0.00000\n14     51.96966      0.00000      0.00000\n15     43.43595      0.00000      0.00000\n16     46.68772      0.00000      0.00000\n17     50.91173      0.00000      0.00000\n18     48.08467      0.00000      0.00000\n19     47.74265      0.00000      0.00000\n20     49.16764      0.00000      0.00000\n21      0.00000     48.71006      0.00000\n22      0.00000     47.56668      0.00000\n23      0.00000     50.16669      0.00000\n24      0.00000     49.10972      0.00000\n25      0.00000     47.27380      0.00000\n26      0.00000     43.58381      0.00000\n27      0.00000     46.21152      0.00000\n28      0.00000     44.55990      0.00000\n29      0.00000     40.57609      0.00000\n30      0.00000     46.15947      0.00000\n31      0.00000     47.35107      0.00000\n32      0.00000     52.77044      0.00000\n33      0.00000     52.66843      0.00000\n34      0.00000     51.32535      0.00000\n35      0.00000     44.70797      0.00000\n36      0.00000     42.41818      0.00000\n37      0.00000     45.25926      0.00000\n38      0.00000     51.73671      0.00000\n39      0.00000     48.26356      0.00000\n40      0.00000     49.27045      0.00000\n41      0.00000      0.00000     45.54455\n42      0.00000      0.00000     43.37230\n43      0.00000      0.00000     49.66765\n44      0.00000      0.00000     46.89291\n45      0.00000      0.00000     44.85799\n46      0.00000      0.00000     48.05454\n47      0.00000      0.00000     50.64563\n48      0.00000      0.00000     50.64558\n49      0.00000      0.00000     51.07873\n50      0.00000      0.00000     46.85607\n51      0.00000      0.00000     51.29831\n52      0.00000      0.00000     47.90725\n53      0.00000      0.00000     48.57102\n54      0.00000      0.00000     52.00562\n55      0.00000      0.00000     50.19166\n56      0.00000      0.00000     48.16861\n57      0.00000      0.00000     51.98792\n58      0.00000      0.00000     46.77564\n59      0.00000      0.00000     45.54523\n60      0.00000      0.00000     49.07684\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$pond\n[1] \"contr.treatment\"\n\n\n\n\n\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\n\n\n\n\nYou’ll notice that this design matrix doesn’t contain 0s and 1s, like the design matrix for pond alone does.\nInstead, wherever there would be a 1, it has been replaced with the value of length for that row.\nThis means that when we multiply our design matrix by our b3, the following happens:\n\nRPython\n\n\n\nmodel.matrix(~0+length:pond) %*% b3\n\n         [,1]\n1    0.000000\n2    0.000000\n3    0.000000\n4    0.000000\n5    0.000000\n6    0.000000\n7    0.000000\n8    0.000000\n9    0.000000\n10   0.000000\n11   0.000000\n12   0.000000\n13   0.000000\n14   0.000000\n15   0.000000\n16   0.000000\n17   0.000000\n18   0.000000\n19   0.000000\n20   0.000000\n21  24.355031\n22  23.783340\n23  25.083345\n24  24.554860\n25  23.636901\n26  21.791905\n27  23.105761\n28  22.279950\n29  20.288045\n30  23.079737\n31  23.675533\n32  26.385219\n33  26.334215\n34  25.662676\n35  22.353987\n36  21.209091\n37  22.629632\n38  25.868353\n39  24.131782\n40  24.635223\n41  -9.108910\n42  -8.674459\n43  -9.933529\n44  -9.378583\n45  -8.971597\n46  -9.610908\n47 -10.129127\n48 -10.129117\n49 -10.215746\n50  -9.371214\n51 -10.259661\n52  -9.581450\n53  -9.714204\n54 -10.401124\n55 -10.038331\n56  -9.633721\n57 -10.397583\n58  -9.355128\n59  -9.109046\n60  -9.815367\n\n\n\n\n\nnp.dot(Xpond_length, b3)\n\narray([  0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n        23.69723922,  25.56805692,  24.80724295,  25.218178  ,\n        24.36165945,  22.5712357 ,  23.79559987,  25.90087231,\n        24.26045047,  22.16511784,  26.12297997,  24.68656647,\n        25.09331376,  26.9526521 ,  23.17831799,  22.98087259,\n        20.24065452,  24.22044074,  24.90929324,  23.96619166,\n        -9.60805335, -10.16156694,  -9.8523736 ,  -9.84697178,\n        -9.55720565,  -9.57273745, -10.22453158,  -9.54357916,\n        -9.34749363,  -9.26880686,  -9.52734147,  -9.71408482,\n        -9.90728244,  -9.67892308,  -9.40102973,  -8.62056823,\n        -9.97146844,  -9.00445573,  -9.50319217, -10.3154426 ])\n\n\n\n\n\nWe get no adjustments made for any of the measurements from pond A. This is what we wanted, because this pond is our reference group. The gradient between ‘clutchsize ~ length’ in pond A is therefore kept equal to our b2 value.\nWe do, however, get adjustments for ponds B and C. These generate a different gradient between clutchsize ~ length for our two non-reference ponds.\nWe then add this in to our model equation, like so:\n\nRPython\n\n\n\navg_clutch &lt;- b0 + b1*length + model.matrix(~0+pond) %*% b2 + model.matrix(~0+length:pond) %*% b3\n\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(clutchsize, length, pond)\n\n\n\n\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\n\navg_clutch = b0 + b1*length + np.dot(Xpond, b2) + np.dot(Xpond_length, b3)\n\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'clutchsize': clutchsize})\n\n\n\n\n\n\nCheck the dataset\n\nRPython\n\n\n\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nlm_golden &lt;- lm(clutchsize ~ length*pond, goldentoad)\n\nsummary(lm_golden)\n\n\nCall:\nlm(formula = clutchsize ~ length * pond, data = goldentoad)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.0456  -6.6815  -0.8263   7.7179  22.2570 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  232.7146    38.6258   6.025 1.56e-07 ***\nlength         0.7550     0.8125   0.929    0.357    \npondB         18.6008    53.4338   0.348    0.729    \npondC          1.2410    63.8124   0.019    0.985    \nlength:pondB   0.8565     1.1233   0.762    0.449    \nlength:pondC  -0.3744     1.3252  -0.283    0.779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.4 on 54 degrees of freedom\nMultiple R-squared:  0.9009,    Adjusted R-squared:  0.8917 \nF-statistic: 98.18 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWe’re going to use the seaborn package here, instead of just matlibplot, for efficiency.\n(If you’d prefer, however, you can just use plotnine and copy the R code from the other tab.)\n\nplt.clf()\nsns.lmplot(data=goldentoad, x=\"length\", y=\"clutchsize\", hue=\"pond\", height=5, aspect=1.3,\n           scatter_kws={\"s\": 40}, line_kws={\"linewidth\": 2}, ci=None)\n\n\n\n\n\n\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = smf.ols(formula= \"clutchsize ~ length * pond\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.900\nModel:                            OLS   Adj. R-squared:                  0.891\nMethod:                 Least Squares   F-statistic:                     97.36\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           9.33e-26\nTime:                        11:49:55   Log-Likelihood:                -233.42\nNo. Observations:                  60   AIC:                             478.8\nDf Residuals:                      54   BIC:                             491.4\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept          199.3514     46.822      4.258      0.000     105.480     293.223\npond[T.B]         -105.3064     65.251     -1.614      0.112    -236.126      25.513\npond[T.C]          -47.2483     82.637     -0.572      0.570    -212.925     118.428\nlength               1.5531      0.983      1.580      0.120      -0.418       3.524\nlength:pond[T.B]     3.2955      1.357      2.428      0.019       0.574       6.017\nlength:pond[T.C]     0.4078      1.721      0.237      0.814      -3.043       3.859\n==============================================================================\nOmnibus:                        4.125   Durbin-Watson:                   2.550\nProb(Omnibus):                  0.127   Jarque-Bera (JB):                3.455\nSkew:                           0.582   Prob(JB):                        0.178\nKurtosis:                       3.166   Cond. No.                     3.23e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.23e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\nWe can see that we get three separate lines of best fit, with different gradients and intercepts.\nHow do these map onto our beta coefficients?\n\nThe intercept and gradient for pond A are captured in b0 and b1\nThe differences between the intercept of pond A and the intercepts of ponds B and C are captured in b2\nThe differences in gradients between pond A and the gradients of ponds B and C are captured in b3\n\nUltimately, there are still 6 unique numbers; but, because of the format of the equation of a linear model, they’re split across 4 separate beta coefficients.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html#two-categorical-predictors",
    "href": "materials/07-interactions.html#two-categorical-predictors",
    "title": "9  Simulating interactions",
    "section": "9.4 Two categorical predictors",
    "text": "9.4 Two categorical predictors\nLast but not least: what happens if we have an interaction between two categorical predictors?\nWe’ll use a binary predictor variable, presence of predators (yes/no), as our second categorical predictor alongside pond.\n\nSet up parameters and predictors\nBy now, most of this should look familiar. We construct predators just like we do pond, by repeating the elements of a list/vector.\n(To keep things simple, we’ll drop our continuous length predictor for this example.)\n\nRPython\n\n\n\nrm(list=ls()) # optional clean-up\n\nset.seed(20)\n\nn &lt;- 60\nb0 &lt;- 165\nb1 &lt;- c(0, 30, -10)   # pond (A, B, C)\nb2 &lt;- c(0, 20)        # presence of predator (no, yes)\n\nsdi &lt;- 20\n\nlength &lt;- rnorm(n, 48, 3)\npond &lt;- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n\npredators &lt;- rep(c(\"yes\", \"no\"), times = n/2)\n\n\n\n\ndel(length, pond, goldentoad, clutchsize, avg_clutch) # optional clean-up\n\n\nnp.random.seed(23)\n\nn = 60\nb0 = 165\nb1 = np.array([0, 30, -10])   # pond (A, B, C)\nb2 = np.array([0, 20])        # presence of predator (no, yes)\n\nsdi = 20\n\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\n\npredators = np.tile([\"yes\",\"no\"], reps = n//2)\n\n\n\n\nYou’ll notice that we haven’t specified b3 yet - the next section is dedicated to this, since it gets a bit abstract.\n\n\nThe interaction coefficient\nWhat on earth does our b3 coefficient need to look like?\nWell, we know it needs to be a vector. Any interaction that contains at least one categorical predictor, requires a vector beta coefficient.\nLet’s look at the design matrix for the pond:predators interaction to help us figure that out.\n\nRPython\n\n\n\nmodel.matrix(~0 + pond:predators)\n\n\n\n  pondA:predatorsno pondB:predatorsno pondC:predatorsno pondA:predatorsyes\n1                 0                 0                 0                  1\n2                 1                 0                 0                  0\n3                 0                 0                 0                  1\n4                 1                 0                 0                  0\n5                 0                 0                 0                  1\n6                 1                 0                 0                  0\n  pondB:predatorsyes pondC:predatorsyes\n1                  0                  0\n2                  0                  0\n3                  0                  0\n4                  0                  0\n5                  0                  0\n6                  0                  0\n\n\n\n\n\nXpond_pred = dmatrix('0 + C(pond):C(predators)', data = {'pond': pond, 'predators': predators})\n\n\n\narray([[0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.]])\n\n\n\n\n\nThe matrix is 60 rows long (here we’re looking at just the top few rows) and has 6 columns.\nThose 6 columns represent our 6 possible subgroups, telling us that our b4 coefficient will need to be 6 elements long. Some of these elements will be 0s, but how many?\nThe short answer is this:\nThe first 4 elements will be 0s, because our b0, b1 and b2 coefficients already contain values for 4 of our subgroups. We then need two additional unique numbers for the remaining 2 subgroups.\n\n\n\n\n\n\nFor a longer answer:\n\n\n\n\n\nRemember that when fitting the model, our software will choose a group as a reference group.\nIn this example, b0 is the mean of our reference group, which here is pondA:predatorsno (determined alphabetically).\nOur other beta coefficients then represent the differences between this reference group mean, and our other group means.\n\nb0, the baseline mean of our reference group pondA:predatorsno\nb2, containing two numbers; these capture the difference between the reference group and pondB:predatorsno/pondC:predatorsno\nb3, containing one number; this captures the difference between the reference group and pondA:predatorsyes\n\nIf we didn’t include a b3 at all, we would get a dataset that looks like this:\n\n\n`summarise()` has grouped output by 'pond'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nHere, the difference in group means between pondA:predatorsno and pondA:predatorsyes is the exact same difference as we see within ponds B and C as well. That’s represented by the black lines, which are all identical in height.\nThis means that the only information we need to recreate the 6 group means is the 4 values from our b0, b1 and b2 coefficients.\nIf we include the interaction term, however, then that’s no longer the case. Within each pond, there can be a completely unique difference between when predators were and weren’t present:\n\n\n`summarise()` has grouped output by 'pond'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nNow, we cannot use the same b2 value (yes vs no predators) for each of our three ponds: we need unique differences.\nOr, to phrase it another way: each of our 5 non-reference subgroups will have a completely unique difference in means from our reference subgroup.\nThis means our simulation needs to provide 6 unique values across our beta coefficients. We already have the first 4, from b0, b1 and b2, so we just need two more.\n\n\n\nThis, therefore, is what our b3 looks like:\n\nRPython\n\n\n\nb3 &lt;- c(0, 0, 0, 0, -20, 40)\n\n\n\n\nb3 = np.array([0, 0, 0, 0, -20, 40])\n\n\n\n\nSince there are 6 subgroups, and the first 4 from our model design matrix are already dealt with, we only need two additional numbers. The other groups don’t need to be adjusted further.\n\n\nSimulate response variable & check dataset\nFinally, we simulate our response variable, and then we can check how well our model does at recovering these parameters.\n\nRPython\n\n\n\nb3 &lt;- c(0, 0, 0, 0, -20, 40)    # interaction pond:predators\n\navg_clutch &lt;- b0 + model.matrix(~0+pond) %*% b1 + model.matrix(~0+predators) %*% b2 + model.matrix(~0+pond:predators) %*% b3\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ntoads &lt;- tibble(clutchsize, length, pond, predators)\n\n# fit and summarise model\nlm_toads &lt;- lm(clutchsize ~ length + pond * predators, toads)\nsummary(lm_toads)\n\n\nCall:\nlm(formula = clutchsize ~ length + pond * predators, data = toads)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.890 -11.348  -0.284   9.929  37.223 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        241.8247    37.8165   6.395 4.24e-08 ***\nlength              -1.8881     0.7868  -2.400   0.0200 *  \npondB               48.0543     8.0604   5.962 2.08e-07 ***\npondC                2.5931     8.0643   0.322   0.7491    \npredatorsyes        40.9971     8.0570   5.088 4.86e-06 ***\npondB:predatorsyes -37.6927    11.4020  -3.306   0.0017 ** \npondC:predatorsyes  23.7371    11.4269   2.077   0.0426 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.01 on 53 degrees of freedom\nMultiple R-squared:  0.693, Adjusted R-squared:  0.6583 \nF-statistic: 19.94 on 6 and 53 DF,  p-value: 4.978e-12\n\n\n\n\n\nb3 = np.array([0, 0, 0, 0, -20, 40])    # interaction pond:predators\n\n# construct design matrices\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpred = dmatrix('0 + C(predators)', data = {'predators': predators})\nXpond_pred = dmatrix('0 + C(pond):C(predators)', data = {'pond': pond, 'predators': predators})\n\n# simulate response variable in two steps\navg_clutch = b0 + np.dot(Xpond, b1) + np.dot(Xpred, b2) + np.dot(Xpond_pred, b3)\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# collate dataset\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'clutchsize': clutchsize})\n\n# fit and summarise model\nmodel = smf.ols(formula= \"clutchsize ~ pond * predators\", data = goldentoad)\nlm_golden = model.fit()\nprint(lm_golden.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.473\nModel:                            OLS   Adj. R-squared:                  0.425\nMethod:                 Least Squares   F-statistic:                     9.704\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           1.19e-06\nTime:                        11:50:00   Log-Likelihood:                -265.96\nNo. Observations:                  60   AIC:                             543.9\nDf Residuals:                      54   BIC:                             556.5\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================================\n                                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nIntercept                    174.9817      6.787     25.781      0.000     161.374     188.589\npond[T.B]                     30.7008      9.599      3.198      0.002      11.457      49.945\npond[T.C]                    -25.7779      9.599     -2.686      0.010     -45.022      -6.534\npredators[T.yes]              10.4013      9.599      1.084      0.283      -8.843      29.646\npond[T.B]:predators[T.yes]   -22.1565     13.575     -1.632      0.108     -49.372       5.059\npond[T.C]:predators[T.yes]    44.0281     13.575      3.243      0.002      16.813      71.244\n==============================================================================\nOmnibus:                        1.332   Durbin-Watson:                   2.384\nProb(Omnibus):                  0.514   Jarque-Bera (JB):                1.357\nSkew:                           0.325   Prob(JB):                        0.507\nKurtosis:                       2.652   Cond. No.                         9.77\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html#three-way-interactions",
    "href": "materials/07-interactions.html#three-way-interactions",
    "title": "9  Simulating interactions",
    "section": "9.5 Three-way interactions",
    "text": "9.5 Three-way interactions\nThree-way interactions are rarer than two-way interactions, at least in practice, because they require a much bigger sample size to detect and are harder to interpret.\nHowever, they can occur, so let’s (briefly) look at how you’d simulate them.\n\nlength:temperature:pond\nThis three-way interaction involves two continuous (length and temperature) and one categorical (pond) predictors.\nIt will, by default, need a vector beta coefficient.\n\nRPython\n\n\n\nset.seed(22)\n\nn &lt;- 120\nb0 &lt;- -30               # intercept\nb1 &lt;- 0.7               # main effect of length\nb2 &lt;- c(0, 30, -10)     # main effect of pond\nb3 &lt;- 0.5               # main effect of temperature\n\nb4 &lt;- 0.25              # interaction of length:temperature\nb5 &lt;- c(0, 0.2, -0.1)   # interaction of length:pond\nb6 &lt;- c(0, 0.1, -0.25)  # interaction of temperature:pond\n\nb7 &lt;- c(0, 0.05, -0.1)  # interaction of length:temp:pond\n\nsdi &lt;- 6\n\n\n\n\nnp.random.seed(28)\n\nn = 120\nb0 = -30                        # intercept\nb1 = 0.7                        # main effect of length\nb2 = np.array([0, 30, -10])     # main effect of pond\nb3 = 0.5                        # main effect of temperature\n\nb4 = 0.25                       # interaction of length:temperature\nb5 = np.array([0, 0.2, -0.1])   # interaction of length:pond\nb6 = np.array([0, 0.1, -0.25])  # interaction of temperature:pond\n\nb7 = np.array([0, 0.05, -0.1])  # interaction of length:temp:pond\n\nsdi = 6\n\n\n\n\nThere are 12 unique/non-zero values across our 8 beta coefficients.\nOne helpful way to think about this: within each pond, we need 4 unique numbers/constants to describe the intercept, main effect of length, main effect of temperature, and two-way interaction between length:temperature.\nSince we’re allowing a three-way interaction, each of the three ponds will (or at least, can) have a completely unique set of 4 values.\n\nRPython\n\n\n\n# generate predictor variables\nlength &lt;- rnorm(n, 48, 3)\npond &lt;- rep(c(\"A\", \"B\", \"C\"), each = n/3)\ntemperature &lt;- runif(n, 10, 32)\n\n# generate response variable in two steps\navg_clutch &lt;- b0 + b1*length + model.matrix(~0+pond) %*% b2 + b3*temperature +\n  b4*length*temperature +\n  model.matrix(~0+length:pond) %*% b5 +\n  model.matrix(~0+temperature:pond) %*% b6 +\n  model.matrix(~0+length:temperature:pond) %*% b7\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\n# collate the dataset\ngoldentoad &lt;- tibble(length, pond, temperature, clutchsize)\n\n\n\n\n# generate predictor variables\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\ntemperature = np.random.uniform(10, 32, n)\n\n# create design matrices\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\nXpond_temp = dmatrix('0 + C(pond):temp', data = {'pond': pond, 'temp': temperature})\nXpond_temp_length = dmatrix('0 + C(pond):length:temp', data = {'pond': pond, 'length': length, 'temp': temperature})\n\n# generate response variable in two steps\navg_clutch = (\n  b0 + b1*length + np.dot(Xpond, b2) + b3*temperature \n  + b4*length*temperature + np.dot(Xpond_length, b5) \n  + np.dot(Xpond_temp, b6) + np.dot(Xpond_temp_length, b7)\n  )\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# collate the dataset\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'temperature': temperature, 'clutchsize': clutchsize})\n\n\n\n\nLet’s check whether these data look sensible by visualising them, and how well a linear model does at recovering the parameters:\n\nRPython\n\n\n\nggplot(goldentoad, aes(x = length, colour = temperature, y = clutchsize)) +\n  facet_wrap(~ pond) +\n  geom_point()\n\n\n\n\n\n\n\nlm_golden &lt;- lm(clutchsize ~ length * pond * temperature, data = goldentoad)\nsummary(lm_golden)\n\n\nCall:\nlm(formula = clutchsize ~ length * pond * temperature, data = goldentoad)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.6864  -4.4287  -0.0933   4.2699  17.4106 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -33.44912   59.94099  -0.558 0.577976    \nlength                     0.64795    1.26547   0.512 0.609683    \npondB                     26.82304   94.49979   0.284 0.777075    \npondC                    -58.61716   84.66914  -0.692 0.490230    \ntemperature                0.63337    2.98100   0.212 0.832142    \nlength:pondB               0.31095    1.96551   0.158 0.874591    \nlength:pondC               1.15449    1.79169   0.644 0.520711    \nlength:temperature         0.25152    0.06296   3.995 0.000119 ***\npondB:temperature         -0.67053    4.73427  -0.142 0.887633    \npondC:temperature          0.84409    4.13926   0.204 0.838798    \nlength:pondB:temperature   0.06567    0.09875   0.665 0.507463    \nlength:pondC:temperature  -0.13405    0.08779  -1.527 0.129706    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.51 on 108 degrees of freedom\nMultiple R-squared:  0.9972,    Adjusted R-squared:  0.9969 \nF-statistic:  3534 on 11 and 108 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWe’ll use seaborn to create the faceted plot (though, as always, if you’re used to ggplot/plotnine, you can toggle over to the R code and use that as a basis instead):\n\nplt.clf()\ng = sns.FacetGrid(goldentoad, col=\"pond\", hue=\"temperature\", height=4, aspect=1.2)\ng.map_dataframe(sns.scatterplot, x=\"length\", y=\"clutchsize\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd let’s fit and summarise the model:\n\nmodel = smf.ols(formula= \"clutchsize ~ length * pond * temperature\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.998\nModel:                            OLS   Adj. R-squared:                  0.998\nMethod:                 Least Squares   F-statistic:                     5432.\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):          1.02e-142\nTime:                        11:50:07   Log-Likelihood:                -370.91\nNo. Observations:                 120   AIC:                             765.8\nDf Residuals:                     108   BIC:                             799.3\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n================================================================================================\n                                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                      -66.5326     48.314     -1.377      0.171    -162.299      29.233\npond[T.B]                      107.8100     68.883      1.565      0.120     -28.729     244.349\npond[T.C]                       48.7665     67.082      0.727      0.469     -84.202     181.735\nlength                           1.4876      1.014      1.467      0.145      -0.523       3.498\nlength:pond[T.B]                -1.5205      1.443     -1.054      0.294      -4.381       1.340\nlength:pond[T.C]                -1.3506      1.407     -0.960      0.339      -4.139       1.438\ntemperature                      0.6124      2.123      0.288      0.774      -3.596       4.821\npond[T.B]:temperature           -0.9611      3.030     -0.317      0.752      -6.966       5.044\npond[T.C]:temperature           -1.9374      3.148     -0.615      0.540      -8.177       4.302\nlength:temperature               0.2466      0.044      5.556      0.000       0.159       0.335\nlength:pond[T.B]:temperature     0.0769      0.063      1.216      0.227      -0.048       0.202\nlength:pond[T.C]:temperature    -0.0646      0.065     -0.987      0.326      -0.194       0.065\n==============================================================================\nOmnibus:                        2.940   Durbin-Watson:                   2.011\nProb(Omnibus):                  0.230   Jarque-Bera (JB):                2.966\nSkew:                           0.065   Prob(JB):                        0.227\nKurtosis:                       3.759   Cond. No.                     2.14e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.14e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n\n\n\n\n\n\nTry fitting a purely additive model. Better or worse?\n\n\nlength:pond:predators\nNow, let’s look at a three-way interaction that contains two categorical predictors.\nWithin each pond, there are 4 key numbers that we need:\n\nThe intercept for the predator reference group\nThe length gradient for the predator reference group\nThe adjustment to the intercept for the predator non-reference group\nThe adjustment the length gradient for the predator non-reference group\n\n(It can really help to draw this out on some scrap paper!)\nSo, across our 8 beta coefficients, we’re going to need 12 numbers.\n\nRPython\n\n\n\nset.seed(22)\n\nn &lt;- 120\nb0 &lt;- -30                         # intercept\nb1 &lt;- 0.7                         # main effect of length\nb2 &lt;- c(0, 30, -10)               # main effect of pond (A, B, C)\nb3 &lt;- c(0, 20)                    # main effect of predators (no, yes)\n\nb4 &lt;- c(0, 0.2, -0.1)             # interaction of length:pond\nb5 &lt;- c(0, 0.1)                   # interaction of length:predators\nb6 &lt;- c(0, 0, 0, 0, 0.1, -0.25)   # interaction of pond:predators\n\nb7 &lt;- c(0, 0, 0, 0, 0.1, -0.2)    # interaction of length:temp:pond\n\nsdi &lt;- 6\n\n\n\n\nnp.random.seed(28)\n\nn = 120\nb0 = -30                                  # intercept\nb1 = 0.7                                  # main effect of length\nb2 = np.array([0, 30, -10])               # main effect of pond (A, B, C)\nb3 = np.array([0, 20])                    # main effect of predators (no, yes)\n\nb4 = np.array([0, 0.2, -0.1])             # interaction of length:pond\nb5 = np.array([0, 0.1])                   # interaction of length:predators\nb6 = np.array([0, 0, 0, 0, 0.1, -0.25])   # interaction of pond:predators\n\nb7 = np.array([0, 0, 0, 0, 0.1, -0.2])    # interaction of length:temp:pond\n\nsdi = 6\n\n\n\n\nIf you’re curious how we were supposed to know to put all the leading zeroes in our b6 and b7 coefficients - the answer is, by looking ahead and checking the number of columns in the design matrix!\n\nRPython\n\n\n\n# generate predictor variables\nlength &lt;- rnorm(n, 48, 3)\npond &lt;- rep(c(\"A\", \"B\", \"C\"), each = n/3)\npredators &lt;- rep(c(\"yes\", \"no\"), times = n/2)\n\n# generate response variable\navg_clutch &lt;- b0 + b1*length + model.matrix(~0+pond) %*% b2 + model.matrix(~0+predators) %*% b3 +\n  model.matrix(~0+length:pond) %*% b4 +\n  model.matrix(~0+length:predators) %*% b5 +\n  model.matrix(~0+pond:predators) %*% b6 +\n  model.matrix(~0+length:pond:predators) %*% b7\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\n# collate the dataset\ngoldentoad &lt;- tibble(length, pond, predators, clutchsize)\n\n\n\n\n# generate predictor variables\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\npredators = np.tile([\"yes\",\"no\"], reps = n//2)\n\n# create design matrices\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpred = dmatrix('0 + C(pred)', data = {'pred': predators})\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\nXlength_pred = dmatrix('0 + length:C(pred)', data = {'pred': predators, 'length': length})\nXpond_pred = dmatrix('0 + C(pond):C(pred)', data = {'pond': pond, 'pred': predators})\nXpond_length_pred = dmatrix('0 + length:C(pond):C(pred)', data = {'pond': pond, 'length': length, 'pred': predators})\n\n# generate response variable in two steps\navg_clutch = (\n  b0 + b1*length + np.dot(Xpond, b2) + np.dot(Xpred, b3) \n  + np.dot(Xpond_length, b4) + np.dot(Xlength_pred, b5) \n  + np.dot(Xpond_pred, b6) + np.dot(Xpond_length_pred, b7)\n  )\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# collate the dataset\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'temperature': temperature, 'clutchsize': clutchsize})",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html#exercises",
    "href": "materials/07-interactions.html#exercises",
    "title": "9  Simulating interactions",
    "section": "9.6 Exercises",
    "text": "9.6 Exercises\n\n9.6.1 Continuing your own simulation\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nLevel: \nIn the previous chapters, exercises (Section 7.7.2, Section 9.6.1) encouraged you to simulate a dataset that by now should contain at least one categorical and one continuous predictor.\nContinue with that simulation, by adding at least one interaction. Adapt the code from the goldentoad example to achieve this.\nTo vary the difficulty of this exercise, start with a continuous:continuous interaction, then a categorical:continuous, and then continuous:continuous.\n(If you’re super brave, try a three-way interaction!)\nAs always, remember - biological plausibility isn’t important here. No one will check this dataset.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  },
  {
    "objectID": "materials/07-interactions.html#summary",
    "href": "materials/07-interactions.html#summary",
    "title": "9  Simulating interactions",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nOnce we know how to simulate main effects, the main additional challenge for simulating interactions is to think about what the beta coefficients should look like (especially when a categorical predictor is involved).\nVisualising the dataset repeatedly, and going back to tweak the parameters, is often necessary when trying to simulate an interaction.\n\n\n\n\n\n\nKey Points\n\n\n\n\nInteractions containing only continuous predictors, only require a single constant as their beta coefficient\nWhile any interaction term containing a categorical predictor, must also be treated as categorical when simulating\nIt’s very helpful to visualise the dataset to check your simulation is as expected",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Linear models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating interactions</span>"
    ]
  }
]