{
  "hash": "075c6f09fa450b2d2dbd92752d08f948",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The central limit theorem\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nNow that you have the programming basics, we're going to use them to make sense of a famous statistical concept: the central limit theorem.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nfrom scipy.stats import t\n```\n:::\n\n\n\n\n:::\n:::\n\n## Estimating parameters from datasets\n\nThere are two main forms of statistical inference, from dataset to population.\n\n1. Estimating parameters\n2. Testing hypotheses\n\nWe'll get onto the second form later on, but for this section, we're going to focus on estimating parameters. \n\nIn other words - can we do a good job of recreating the distribution of the underlying population, just from the sample we've taken from that population?\n\nOur ability to do this well often hinges on the quality of our sample. This includes both the **size** of the sample, and whether it is **biased** in any way.\n\nIf our sample is biased, there's not much we can do about it apart from a) scrapping it and starting again, or b) acknowledging the uncertainty/narrowed focus of our conclusions.\n\nWe do, however, often have some control over the sample size. Let's look at how sample size affects our parameter estimates.\n\n### Mean\n\nWhen simulating data, we have an unusual level of knowledge and power (in the normal sense of the word, not the statistical one!): \n\nWe know exactly what the true population parameters are, because we specified them.\n\nIn the code below, we know that the actual true population mean is 4, with no uncertainty. We have set this to be the \"ground truth\".\n\nThis code is similar to the for loop introduced in @sec-exm_mean-for-loop, but we've increased the sample size.\n\nNotice how this:\n\n-   Decreases variance (our histograms are \"squished\" inward - pay attention to the x axis scale)\n-   Increases the consistency of our mean estimates (they are more similar to one another)\n-   Increases the accuracy of our mean estimates (they are, overall, closer to the true value of 4)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:3) {\n  \n  n <- 200\n  mean_n <- 4\n  sd_n <- 0.5\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  hist(data, xlim = c(1, 7)); abline(v = mean(data), col = \"red\", lwd = 3)\n\n  }\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/less simple for loop, more iterations-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/less simple for loop, more iterations-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/less simple for loop, more iterations-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfor i in range(1, 4):\n  n = 200\n  mean = 4\n  sd = 0.5\n  data = np.random.normal(mean, sd, n)\n  plt.figure()\n  plt.hist(data)\n  plt.axvline(x = statistics.mean(data), color = 'r')\n  plt.show()\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/for loop means bigger sample size-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/for loop means bigger sample size-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/for loop means bigger sample size-3.png){width=672}\n:::\n:::\n\n\n\n:::\n\nIn a larger sample size, we are able to do a better job of \"recovering\" or recreating the original parameter we specified.\n\n\n::: {.callout-tip}\n#### The average of averages\n\nWhen we take multiple samples and average across them, we do an *even better* job of recovering the true population mean. \n\nIn other words, if we sampled a bunch of datasets, and then took the average of all of their respective means, we'd probably get pretty close to 4.\n\nThe section later on in this chapter, on the central limit theorem, pushes this idea a little further.\n:::\n\n### Variance\n\nIn the code above, we also know what the true population variance is, because we set it (by setting standard deviation, the square root of variance).\n\n::: {.callout-tip}\n#### What do we mean by \"variance\"?\n\nVariance is calculated by measuring the difference from each data point and the mean, squaring them, adding those squares up, and then dividing by the number of data points. The mathematical formula looks like this:\n\n$$\n\\text{Var}(X) = \\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 } {n}\n$$\n\nHowever, when we estimate population variance from a sample, we actually tweak this formula a bit. We divide instead by $n - 1$:\n\n$$\n\\text{Var}(X) = \\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 } {n - 1}\n$$\n\n**Why?**\n\nWell, I could give you a theoretical explanation. Or, we can use the power of simulation, and see for ourselves more intuitively.\n\nFor the next section, the word variance will crop up in different forms:\n\n-   Estimated sample variance, calculated by dividing by $n - 1$, from a dataset\n-   Estimated variance, calculated by dividing by $n$, from a dataset\n-   The true population variance, which isn't calculated; it's known/specified by us\n\n:::\n\nIn the for loop below, we are using the same simulation parameters as above, and calculating (manually) two types of variance. \n\nWe'll set the mean to 4 and the variance to 1:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- data.frame(var=numeric(),\n                      sample_var=numeric())\n\nfor (i in 1:30) {\n  \n  n <- 10\n  mean_n <- 4\n  sd_n <- 1\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  v <- sum((data - mean(data))^2)/n\n  samplev <- sum((data - mean(data))^2)/(n-1)\n\n  results <- rbind(results, data.frame(var = v, sample_var = samplev))\n  \n}\n```\n:::\n\n\n\n\nTo break down this code further:\n\nWe've initialised a `results` table with our desired columns first. On each iteration of the loop (30 total), we sample a dataset, calculate the estimated variance `estv` and sample variance `samplev` using slightly different formulae, and then add them to our `results` table.\n\n## Python\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrows = []\n\nfor i in range(30):\n    n = 10\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    v = np.sum((data - np.mean(data))**2) / n\n    samplev = np.sum((data - np.mean(data))**2) / (n - 1)\n    \n    rows.append({'var': v, 'sample_var': samplev})\n\nresults = pd.DataFrame(rows)\n```\n:::\n\n\n\n\nTo break down this code further:\n\nOn each iteration of the loop (30 total), we sample a dataset, calculate the estimated variance `estv` and sample variance `samplev` using slightly different formulae. These values are collated by our `rows` object, which we finally convert to a pandas dataframe (a `results` table).\n:::\n\nNow, let's look at those results and what they show us.\n\nFirst, we'll create a new column in our `results` object that contains the difference between our estimates in each case.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- results %>%\n  mutate(diff_var = sample_var - var)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresults['diff_var'] = results['sample_var'] - results['var']\n```\n:::\n\n\n\n:::\n\nWhen we look at the final results file, we see that the estimated sample variance is, on average, a bit bigger than the estimated variance. This makes sense, because we're dividing by a smaller number when we calculate the sample variance (in other words, $n - 1 < n$).\n\nNow, let's look at the average variance and sample variance, and see which of them is doing the best job of recreating our true population variance (which we know is 1, because we built the simulation that way).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults %>%\n  summarise(mean(var), mean(sample_var), mean(diff_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean(var) mean(sample_var) mean(diff_var)\n1 0.9183643         1.020405      0.1020405\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsummary = results[['var', 'sample_var', 'diff_var']].mean()\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvar           0.918773\nsample_var    1.020859\ndiff_var      0.102086\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n:::\n\nHere, we've taken the mean of each column. This means we're looking at the average estimated variance and the average estimated sample variance, across all 10 of our random datasets.\n\n::: {.callout-tip}\n#### Remember, each of these numbers is an average\n\nYes, now we're taking the mean of the variance. Yes, we could also measure the variance of the variance if we wanted. Yes, this does start to get confusing the more you think about it. \n\nMore on this in the central limit theorem section!\n:::\n\nAs we can see from these results, when the sample size is small, our estimated variance (on average) *underestimates* the true value.\n\nThis is because, the smaller the sample, the less likely it is to contain values from the edges or tails of the normal distribution, so we don't get a good picture of the true spread.\n\nThe sample variance accounts for this by dividing by `n - 1` instead, so the estimate is larger and less of an underestimate. \n\nTo get an intuition for this, let's repeat all the code above, but with a larger sample size:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- data.frame(var=numeric(),\n                      sample_var=numeric())\n\nfor (i in 1:30) {\n  \n  n <- 20\n  mean_n <- 4\n  sd_n <- 1\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  v <- sum((data - mean(data))^2)/n\n  sample <- sum((data - mean(data))^2)/(n-1)\n\n  results <- rbind(results, data.frame(var = v, sample_var = sample))\n  \n}\n\nresults <- results %>%\n  mutate(diff_var = sample_var - var)\n\nresults %>%\n  summarise(mean(var), mean(sample_var), mean(diff_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean(var) mean(sample_var) mean(diff_var)\n1 0.9507581         1.000798      0.0500399\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrows = []\n\nfor i in range(30):\n    n = 20\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    v = np.sum((data - np.mean(data))**2) / n\n    samplev = np.sum((data - np.mean(data))**2) / (n - 1)\n    \n    rows.append({'var': v, 'sample_var': samplev})\n\nresults = pd.DataFrame(rows)\n\nresults['diff_var'] = results['sample_var'] - results['var']\n\nsummary = results[['var', 'sample_var', 'diff_var']].mean()\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvar           0.953684\nsample_var    1.003878\ndiff_var      0.050194\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWhen `n` is larger, notice how the estimated variance is now closer to the true population variance, on average?\n\nWith a larger sample size, we are more likely to sample the full \"spread\" of the distribution.\n\nThe estimated sample variance, however, is about as close to the true population variance as it was before, and so the difference between the estimated variance and estimated sample variance has shrunk.\n\n::: {.callout-tip collapse=\"true\"}\n#### Why sample variance is even cleverer than you might think\n\nDividing by `n - 1` has a bigger impact when the sample is small, where `1` will be a relatively larger fraction of `n`.\n\nThis is great, because these smaller samples are also the place where we need this adjustment most: they're less likely to contain values from the tails of the distribution, and therefore will underestimate the true population variance more.\n\nIn contrast, when our sample is much bigger, it's going to be more representative/less noisy, and we see much less of an underestimation and will need less adjustment. Happily, we will automatically get less of an adjustment anyway, since the `1` is now a smaller fraction of `n`. \n\nIn other words: the impact of dividing by `n - 1` scales naturally with both the size of `n`, and with the amount of underestimation we need to account for.\n\nIn fact, when the sample is infinitely large, we should see no difference between the estimated sample variance and the estimated variance at all, because `n-1 = n` at infinity.\n\nYou can **test this intuition** (except for the infinity part, you kinda just have to trust me on that) by continuing to mess around with the value of `n` in the code above.\n:::\n\nSince sample variance is the most effective way to estimate the true population variance, functions in R and Python will default to this.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe `var` function in R specifically calculates the sample variance. We can see that we get identical results using the function or doing it manually, by adapting the loop above:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- data.frame(var=numeric(),\n                      sample_var=numeric(),\n                      r_var=numeric())\n\nfor (i in 1:30) {\n  \n  n <- 20\n  mean_n <- 4\n  sd_n <- 1\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  v <- sum((data - mean(data))^2)/n\n  sample <- sum((data - mean(data))^2)/(n-1)\n  \n  # Add an extra column containing the results of var(data)\n  results <- rbind(results, data.frame(var = v, sample_var = sample, r_var = var(data)))\n  \n}\n\nresults %>%\n  summarise(mean(var), mean(sample_var), mean(r_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean(var) mean(sample_var) mean(r_var)\n1 0.9507581         1.000798    1.000798\n```\n\n\n:::\n:::\n\n\n\n\nNotice how `mean(sample_var)` and `mean(r_var)` are identical?\n\n## Python\n\nThe `numpy.var` function specifically calculates sample variance. We can see that we get identical results using the function or doing it manually, by adapting the loop above:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrows = []\n\nfor i in range(30):\n    n = 20\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    # Estimated variance\n    v = np.sum((data - np.mean(data))**2) / n\n    # Sample variance\n    sample = np.sum((data - np.mean(data))**2) / (n - 1)\n    # numpy sample variance\n    np_var = np.var(data, ddof=1)\n\n    rows.append({'var': v, 'sample_var': sample, 'np_var': np_var})\n\nresults = pd.DataFrame(rows)\n\nsummary = results[['var', 'sample_var', 'np_var']].mean()\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvar           0.953684\nsample_var    1.003878\nnp_var        1.003878\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\nNotice how `sample_var` and `np_var` are identical?\n:::\n\n## Central limit theorem\n\nIn the section above, we used for loops to simulate multiple datasets, measure certain statistics from them, and then averaged those statistics across the datasets.\n\nSo, in some cases, we were looking at the mean of the means, or the mean of the variances. We're going to unpack that a bit further now.\n\nSpecifically, we're going to talk about the central limit theorem: the idea that, across multiple samples taken from the same distribution, the estimates/statistics we calculate from them will themselves follow a normal distribution.\n\n### An example: the mean {#sec-exm_mean-CLT}\n\nYou will recognise all the code below from previous sections, but here we're using it to show us a slightly different distribution.\n\nInstead of producing separate histograms for each of the datasets (i.e., one per loop), we are instead simply collecting the mean value from each of our datasets.\n\nThen, we will treat the set of means as a sample in itself, and visualise its distribution.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{#lst-central-limit-template-code-r .r .cell-code}\nmeans <- c()\n\nfor (i in 1:40) {\n  \n  n <- 200\n  mean_n <- 4\n  sd_n <- 1\n  \n  means[i] <- mean(rnorm(n, mean_n, sd_n))\n\n}\n\nhist(means)\nabline(v = mean(means), col = \"red\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/central limit means demo-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{#lst-central-limit-template-code-py .python .cell-code}\nmeans = []\n\nfor i in range(40):\n  n = 200\n  mean = 4\n  sd = 1\n  \n  est_mean = np.random.normal(mean, sd, n).mean()\n\n  means.append(est_mean)\n\nplt.clf()\nplt.hist(means)\nplt.axvline(x = statistics.mean(means), color = 'r')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/CLT means demo-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\nThe set of means from our `i` datasets, follow a normal distribution. The mean of this normal distribution is approximately the true population mean (which we know to be 4).\n\nIf we increase the number of iterations/loops, we will sample more datasets, with more means.\n\nIf we think of our set of sample means as a sample in itself, then doing this is effectively increasing our sample size. And, as we know from the first section of this chapter, that means that the mean of our distribution should be a better estimate of the true population value.\n\nThis is exactly what happens:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/central limit means higher i-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/CLT means higher i-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nTo produce the plot above, the number of iterations was set to `i = 1000`.\n\nTry setting it to something between 40 and 1000, or even more than 1000, and see how that changes things.\n\n::: {.callout-tip collapse=\"true\"}\n#### Standard error of the mean\n\nYou may have come across the concept of the standard error of the mean in the past (especially when constructing error bars for plots). But now, you should be in a better position to really understand what it is.\n\nIn the histogram above, we've calculated the mean of the sample means. But around that mean of sample means, there is some spread or noise.\n\nWe can quantify that spread by measuring the standard deviation of the distribution of sample means - and if we do, we've calculated the standard error.\n\nOf course, in classical statistics we usually only have one dataset, rather than 1000, to help us figure out that standard error. So, like with everything else we calculate from a dataset, we are only ever able to access an estimate of that standard error.\n:::\n\n### It's always normal\n\nThe really quirky thing about the central limit theorem is that it doesn't actually matter what distribution you pulled the original samples from. In other words, the results we got above aren't just because we were using the normal distribution for our simulations.\n\nTo prove that, the code here has been adapted to pull each of our 1000 samples from a uniform distribution instead, and estimate the mean.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nNote the use of `runif` instead of `rnorm`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans <- c()\n\nfor (i in 1:1000) {\n  \n  n <- 200\n  min_n <- 1\n  max_n <- 7\n  \n  means[i] <- mean(runif(n, min_n, max_n))\n\n}\n\nhist(means)\nabline(v = mean(means), col = \"red\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/central limit means demo unif dist-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\nNote the use of `np.random.uniform` instead of `np.random.normal`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmeans = []\n\nfor i in range(1000):\n  n = 200\n  lower = 1\n  upper = 7\n  est_mean = np.random.uniform(mean, sd, n).mean()\n  means.append(est_mean)\n\nplt.clf()\nplt.hist(means)\nplt.axvline(x = statistics.mean(means), color = 'r')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/CLT means unif dist-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nAlthough each of the individual samples would have a flat histogram, that's not what we're plotting here. Here, we're looking at the set of means that summarise each of those individual samples.\n\nThe nature of the underlying population distribution **doesn't matter** - the distribution of the parameter estimates is still normal, which we can see clearly with a sufficient number of simulations.\n\nNo wonder the normal distribution enjoys such special status in statistics.\n\n## Exercises\n\n### t-statistic under CLT {#sec-exr_tstat-CLT}\n\nAll statistics obey the central limit theorem. This includes not just descriptive statistics like the mean, median, standard deviation etc., but the test statistics that we use for hypothesis testing.\n\n::: {.callout-exercise}\n\n\n\n\n{{< level 2 >}}\n\n\n\n\n\n\nTo demonstrate this to yourself, generate 1000 t-statistics from one-sample t-tests, and plot them on a histogram.\n\n-   What happens to the distribution as you change `mu`?\n-   How does the distribution of 1000 t-statistics, compare to the t-statistic distribution? Why are they different?\n\nYou can refer back to the code in @sec-exm_mean-CLT to help you.\n\n::: {.callout-tip collapse=\"true\"}\n#### Code tips\n\nIf you're struggling to extract the t-statistics, you might find the below code snippets to be helpful as tips/hints!\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nIf you're using the base R `t.test` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.result <- t.test(rnorm(n, mean_n, sd_n), mu = 3)\n  \nt.values[i] <- unname(t.result$statistic)\n```\n:::\n\n\n\n\nThis first method is probably quicker/easier.\n\nIf you're using `t_test` from `rstatix` (which you are likely familiar with, if you took the Core statistics course before this one):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The data must be saved as a dataframe to use t_test\ndata <- rnorm(n, mean_n, sd_n) %>%\n  data.frame()\n\nt_result <- t_test(data, .~1)\n\nt_values[i] <- unname(t_result$statistic)\n```\n:::\n\n\n\n\nIf you're trying to sample from the t-distribution, note that the function requires different parameters - specifically, we specify the degrees of freedom `df` (and optionally, a non-centrality parameter `ncp`):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt(n = 1000, df = 99)\n```\n:::\n\n\n\n\n## Python\n\nThere are a few different functions for running t-tests in Python. If you took the Core statistics course, you're likely familiar with the `ttest` function from `pingouin`.\n\nFor ease of use in this exercise, however, it's easier to extract the t-statistic on each loop by indexing the output from the `ttest_mean` from `statsmodels`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsms.DescrStatsW(data).ttest_mean()[0]\n```\n:::\n\n\n\n\nTo sample from the t-distribution, use `np.random.standard_t`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.standard_t(df = 99, size = 1000)\n```\n:::\n\n\n\n\nYou will need to provide two arguments: the degrees of freedom `df`, and the sample `size`.\n:::\n:::\n:::\n\n### Distributions all the way down {#sec-exr_multilevel-CLT}\n\nThose of you with imagination might be wondering: if one for loop can construct a single histogram, giving us the distribution of the set of sample means, why can't we run multiple for loops and look at the distribution of the set of means of the set of means?\n\nThe short answer is: we can!\n\n![It's distributions all the way down; [image source](https://en.wikipedia.org/wiki/Turtles_all_the_way_down)](images/turtles.jpg)\n\n::: {.callout-exercise}\n\n\n\n\n{{< level 3 >}}\n\n\n\n\n\n\nYour mission, should you choose to accept it, is to plot a single histogram that captures the distribution of the set of means of the set of means.\n\nAdapt the code in @sec-exm_mean-CLT by nesting one for loop inside the other, to produce a single histogram. It should represent the set of means of the set of sample means.\n\nBefore you do, consider:\n\n-   What shape will the histogram take, if you run enough iterations?\n-   If someone asks what you did on this course, how on earth will you explain *this* to them?!\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\nTo make the code a little easier to parse and eliminate any clashes in the environment, the outside for loop uses `j` for indexing instead of `i`.\n\nWe've also kept the number of iterations low-ish. You might notice, if you use a large value like 1000 for each of the loops, it takes a while to run (because you're actually asking for 1000000 total iterations!)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans_j <- c()\n\nfor (j in 1:300) {\n\n  means_i <- c()\n\n  for (i in 1:300) {\n  \n    n <- 20\n    min_n <- 1\n    max_n <- 7\n  \n    means_i[i] <- mean(runif(n, min_n, max_n))\n\n  }\n  \n  means_j[j] <- mean(means_i)\n  \n}\n\nhist(means_j)\nabline(v = mean(means_j), col = \"red\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/central limit nested-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmeans_j = []\n\nfor j in range(300):\n  \n  means_i = []\n  \n  for i in range(300):\n    n = 20\n    lower = 1\n    upper = 7\n    imean = np.random.uniform(lower, upper, n).mean()\n    means_i.append(imean)\n  \n  jmean = np.mean(means_i)\n  means_j.append(jmean)\n  \nplt.clf()\nplt.hist(means_j)\nplt.axvline(x = np.mean(means_j), color = 'r')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-central-limit-theorem_files/figure-html/CLT nested loops-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nIt's *still* normal. If this is what you guessed, well done.\n\nIf you were to take this further, and add an infinite number of nested loops, you'd get a normal distribution each time.\n\nAnd it doesn't even matter what distribution we sample our individual datasets from, as we showed above. In this worked example code we've used the uniform distribution, but you can try swapping it out for any other distribution - you'll still get a normal distribution at each \"layer\", which gets clearer and clear the more iterations you run.\n\nNow, statistics may not be considered the \"coolest\" subject in the world - but I think that's pretty awesome, don't you?\n:::\n:::\n\n## Summary\n\nSimulation is a great way to help get your head around more difficult or abstract statistical concepts, without needing to worry about the mathematical formulae.\n\nThe central limit theorem is incredibly boring to explain via equations, and much less easy to get your head around - so why not just simulate?\n\n::: {.callout-tip}\n#### Key Points\n\n-   We are more likely to accurately and precisely \"recover\" the true population parameters when our sample is large and unbiased\n-   The central limit theorem shows that, across a number of samples, the set of estimates of a given parameter will follow an approximately normal distribution \n-   When the number of samples = $\\infty$, it will be perfectly normal\n-   This is true regardless of the original distribution that the individual samples come from\n:::\n\n\n",
    "supporting": [
      "03-central-limit-theorem_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}