{
  "hash": "7372b86c3fea16c387c961ff0fb976dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulating continuous predictors\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nThis chapter follows on closely from the previous chapter on continuous predictors.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\n\n# These packages will be used for evaluating the models we fit to our simulated data\nlibrary(performance)\nlibrary(ggResidpanel)\n\n# This package is optional/will only be used for later sections in this chapter\nlibrary(MASS)\n```\n:::\n\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom patsy import dmatrix\n```\n:::\n\n\n\n\n:::\n:::\n\n\n\n## The pond variable\n\nWe're going to stick with our golden toads, and keep the continuous `length` predictor variable from the previous chapter, so that we can build up the complexity of our dataset gradually between chapters.\n\nTo add that complexity, we're going to imagine that golden toads living in different ponds produce slightly different clutch sizes, and simulate some sensible data on that basis.\n\n#### Clear previous simulation\n\nYou might find it helpful to delete variables/clear the global environment, so that nothing from your previous simulation has an unexpected impact on your new one:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls())\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel(avg_clutch, clutchsize, goldentoad, length, lm_golden, model, tempdata)\n```\n:::\n\n\n\n:::\n\n(This step is optional!)\n\n#### Simulating categorical predictors\n\nCategorical predictors are a tiny bit more complex to simulate, as the beta coefficients switch from being constants (gradients) to vectors (representing multiple means).\n\nHowever, we're still going to follow the same workflow that we used with continuous predictors in the previous chapter:\n\n-  Set parameters (seed, sample size, beta coefficients and standard deviation)\n-  Generate the values of our predictor variable\n-  Simulate average values of response variable\n-  Simulate actual values of response variable\n-  Check the dataset\n\n## Set parameters\n\nExactly as we did before, we start by setting key parameters including:\n\n-   Seed, for reproducibility\n-   Sample size `n`\n-   Individual standard deviation `sdi`\n-   Beta coefficients\n\nWe need three betas now, since we're adding a second predictor.\n\nTo generate the beta for `pond`, we need to specify a vector, instead of a single constant. Let's keep things relatively simple and stick to just three ponds, which we'll imaginatively call `A`, `B` and `C`.\n\nSince pond `A` will be our reference group, our beta does not need to contain any adjustment for that pond, hence the vector starts with a 0. The other two numbers then represent the difference in means between pond `A` and ponds `B` and `C` respectively.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n\nn <- 60\nb0 <- 175             # intercept\nb1 <- 2               # main effect of length\nb2 <- c(0, 30, -10)   # main effect of pond\n\nsdi <- 20\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(20)\n\nn = 60\nb0 = 175                        # intercept\nb1 = 2                          # main effect of length\nb2 = np.array([0, 30, -10])     # main effect of pond\n\nsdi = 20\n```\n:::\n\n\n\n:::\n\nThis means that, in the reality we are creating here, the true mean clutch size for each of our three ponds is 175, 205 (175+30) and 165 (175-10).\n\n## Generate values for predictor variable(s)\n\nNext, we need values for our `length` and `pond` predictors.\n\nWe already know how to generate `length` from the previous chapter.\n\nFor `pond`, we're not going to sample from a distribution, because we need category names instead of numeric values. So, we'll specify our category names and repeat them as appropriate:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength <- rnorm(n, 48, 3)\n\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n```\n:::\n\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### rep and c functions\n\nThe `rep` and `c` functions in R (`c` short for `concatenate`) will come up quite a lot when generating predictor variables.\n\nThe `rep` function takes two arguments each time: \n\n- First, an argument containing the thing that you want repeated (which can either be a single item, or a list/vector)\n- Second, an argument containing information about how many times to repeat the first argument (there are actually multiple options for how to phrase this second argument)\n\nThe `c` function is a little simpler: it combines, i.e., concatenates, all of the arguments you put into it into a single vector/list item.\n\nYou can then combine these two functions together in various ways to achieve what you want.\n\nFor example, these two lines of code both produce the same output:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(rep(\"A\", times = 3), rep(\"B\", times = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\"\n```\n\n\n:::\n\n```{.r .cell-code}\nrep(c(\"A\", \"B\"), each = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\"\n```\n\n\n:::\n:::\n\n\n\nThe first version asks for `\"A\"` to be repeated 3 times, `\"B\"` to be repeated 3 times, and then these two triplets to be concatenated together into one list.\n\nThe second version asks us to take a list of two items, `\"A\", \"B\"`, and repeat each element 3 times each.\n\nMeanwhile, the code below will do something very different:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrep(c(\"A\", \"B\"), times = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\"\n```\n\n\n:::\n:::\n\n\n\n\nThis line of code asks us to take the list `\"A\", \"B\"` and repeat it, as-is, 3 times. So we get a final result that alternates.\n\nThis shows that choosing carefully between `times` or `each` as your second argument for the `rep` function can be absolutely key to getting the right output!\n\nDon't forget you can use `?rep` or `?c` to get more detailed information about how these functions work, if you would like it.\n:::\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlength = np.random.normal(48, 3, n)\n\npond = np.repeat([\"A\", \"B\", \"C\"], repeats = n//3)\n```\n:::\n\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### np.repeat and np.tile functions\n\nWhen generating categorical variables, you are essentially repeating the category names multiple times.\n\nNote the difference between these two outputs, produced using two different `numpy` functions:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.repeat([\"A\", \"B\", \"C\"], repeats = n//3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',\n       'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B',\n       'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n       'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C',\n       'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'], dtype='<U1')\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.tile([\"A\", \"B\", \"C\"], reps = n//3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray(['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A',\n       'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B',\n       'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C',\n       'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A',\n       'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'], dtype='<U1')\n```\n\n\n:::\n:::\n\n\n\n\nIn both functions, the first argument is an list of category names, with no duplication. Then, you specify the number of repeats with either `repeats` or `reps` (for `np.repeats` vs `np.tile` respectively).\n\nHowever, the two functions do slightly different things:\n\n- `np.repeats` takes each element of the list and repeats it a specified number of times, before moving on to the next element\n- `np.tile` takes the entire list as-is, and repeats it as a chunk for a desired number of times\n\nIn both cases we end up with an array of the same length, but the order of the category names within that list is very different.\n:::\n:::\n\n## Simulate average values of response\n\nNow, exactly as we did with the continuous predictor, we're going to construct a linear model equation to calculate the average values of our response.\n\nHowever, including a categorical predictor in our model equation is a bit more complex. \n\nWe can no longer simply multiply our beta coefficient by our predictor, because our beta coefficient is a vector rather than a constant.\n\nInstead, we need to make use of something called a design matrix, and matrix multiplication, like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(~0 + pond)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   pondA pondB pondC\n1      1     0     0\n2      1     0     0\n3      1     0     0\n4      1     0     0\n5      1     0     0\n6      1     0     0\n7      1     0     0\n8      1     0     0\n9      1     0     0\n10     1     0     0\n11     1     0     0\n12     1     0     0\n13     1     0     0\n14     1     0     0\n15     1     0     0\n16     1     0     0\n17     1     0     0\n18     1     0     0\n19     1     0     0\n20     1     0     0\n21     0     1     0\n22     0     1     0\n23     0     1     0\n24     0     1     0\n25     0     1     0\n26     0     1     0\n27     0     1     0\n28     0     1     0\n29     0     1     0\n30     0     1     0\n31     0     1     0\n32     0     1     0\n33     0     1     0\n34     0     1     0\n35     0     1     0\n36     0     1     0\n37     0     1     0\n38     0     1     0\n39     0     1     0\n40     0     1     0\n41     0     0     1\n42     0     0     1\n43     0     0     1\n44     0     0     1\n45     0     0     1\n46     0     0     1\n47     0     0     1\n48     0     0     1\n49     0     0     1\n50     0     0     1\n51     0     0     1\n52     0     0     1\n53     0     0     1\n54     0     0     1\n55     0     0     1\n56     0     0     1\n57     0     0     1\n58     0     0     1\n59     0     0     1\n60     0     0     1\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$pond\n[1] \"contr.treatment\"\n```\n\n\n:::\n:::\n\n\n\n\nThe `model.matrix` function produces a table of 0s and 1s, which is a matrix representing the design of our experiment. In this case, that's three columns, one for each pond.\n\nThe number of columns in our model matrix matches the number of categories and the length of our `b2` coefficient. Our `b2` is also technically a matrix, just with one row. \n\nThis means we can use the `%*%` operator for matrix multiplication, to multiply these two things together:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(~0+pond) %*% b2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   [,1]\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n11    0\n12    0\n13    0\n14    0\n15    0\n16    0\n17    0\n18    0\n19    0\n20    0\n21   30\n22   30\n23   30\n24   30\n25   30\n26   30\n27   30\n28   30\n29   30\n30   30\n31   30\n32   30\n33   30\n34   30\n35   30\n36   30\n37   30\n38   30\n39   30\n40   30\n41  -10\n42  -10\n43  -10\n44  -10\n45  -10\n46  -10\n47  -10\n48  -10\n49  -10\n50  -10\n51  -10\n52  -10\n53  -10\n54  -10\n55  -10\n56  -10\n57  -10\n58  -10\n59  -10\n60  -10\n```\n\n\n:::\n:::\n\n\n\n\nThis gives us the adjustments we need to make, row by row, for our categorical predictor. For pond `A` we don't need to make any, since that was the reference group.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_clutch <- b0 + b1*length + model.matrix(~0+pond) %*% b2\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\n```\n:::\n\n\n\n\nThe `dmatrix` function from `patsy` produces a table of 0s and 1s, which is a matrix representing the design of our experiment. In this case, it has three columns, which matches the number of categories and the length of our `b2` coefficient. \n\nOur `b2` is also technically a matrix, just with one row. We use `np.dot` to multiply these matrices together:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(Xpond, b2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  30.,\n        30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,  30.,\n        30.,  30.,  30.,  30.,  30.,  30.,  30., -10., -10., -10., -10.,\n       -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,\n       -10., -10., -10., -10., -10.])\n```\n\n\n:::\n:::\n\n\n\n\nThis gives us the adjustments we need to make, row by row, for our categorical predictor. For pond `A` we don't need to make any, since that was the reference group.\n\nWe can then add all these adjustments to the rest of our model from the previous chapter, to produce our expected values:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\navg_clutch = b0 + b1*length + np.dot(Xpond, b2)\n```\n:::\n\n\n\n:::\n\nDon't worry - you don't really need to understand matrix multiplication to get used to this method. If that explanation was enough for you, you'll be just fine from here.\n\nWe'll use this syntax a few more times in this chapter, so you'll learn to recognise and repeat the syntax!\n\n## Simulate actual values of response\n\nThe last step is identical to the previous chapter.\n\nWe sample our actual values of `clutchsize` from a normal distribution with `avg_clutch` as the mean and with a standard deviation of `sdi`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ngoldentoad <- tibble(clutchsize, length, pond)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'pond': pond, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n## Checking the dataset\n\nOnce again, we'll visualise and model these data, to check that they look as we suspected they would.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length + pond, goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length + pond, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.202 -11.751  -0.899  14.254  37.039 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 262.8111    38.6877   6.793  7.6e-09 ***\nlength        0.1016     0.8108   0.125    0.901    \npondB        39.2084     5.9116   6.632  1.4e-08 ***\npondC        -5.5279     5.9691  -0.926    0.358    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.69 on 56 degrees of freedom\nMultiple R-squared:  0.5481,\tAdjusted R-squared:  0.5239 \nF-statistic: 22.64 on 3 and 56 DF,  p-value: 9.96e-10\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](06-categorical-predictors_files/figure-html/testing cat predictor dataset-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length + pond\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.354\nModel:                            OLS   Adj. R-squared:                  0.319\nMethod:                 Least Squares   F-statistic:                     10.23\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           1.80e-05\nTime:                        11:43:42   Log-Likelihood:                -263.47\nNo. Observations:                  60   AIC:                             534.9\nDf Residuals:                      56   BIC:                             543.3\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    196.9612     37.710      5.223      0.000     121.419     272.504\npond[T.B]     24.0615      6.419      3.749      0.000      11.204      36.919\npond[T.C]    -10.4082      6.426     -1.620      0.111     -23.282       2.465\nlength         1.5491      0.781      1.984      0.052      -0.015       3.114\n==============================================================================\nOmnibus:                        3.489   Durbin-Watson:                   2.017\nProb(Omnibus):                  0.175   Jarque-Bera (JB):                1.803\nSkew:                          -0.074   Prob(JB):                        0.406\nKurtosis:                       2.164   Cond. No.                         695.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n:::\n\nHas our model recreated \"reality\" very well? Would we draw the right conclusions from it?\n\n## Exercises\n\n### A second categorical predictor {#sex-exr_second-cont-pred}\n\n::: {.callout-exercise}\n\n\n\n{{< level 1 >}}\n\n\n\n\n\n\nCreate at least one additional categorical predictor for this dataset (perhaps `vegetation cover` or presence of `predators` at the time the toad was laying her eggs).\n\nRemember that you don't need to have values that are totally biologically plausible - this is just about simulation practice.\n:::\n\n\n### Continuing your own simulation {#sec-exr_another-dataset-continued}\n\n::: {.callout-exercise}\n\n\n\n{{< level 2 >}}\n\n\n\n\n\n\nLast chapter, the second exercise (@sec-exr_another-dataset) encouraged you to simulate a brand new dataset of your own, for practice.\n\nContinue with that simulation, this time adding a categorical predictor or two.\n\nYou'll need to:\n\n-   Set the seed and sample size\n-   Set sensible beta coefficients (remember how reference groups work)\n-   Construct the predictor variable (repeating instances of your group/category names)\n-   Simulate the average and then actual values of the response\n-   Visualise and model the data, to check your simulation ran as expected\n\nAdapt the code from the `goldentoad` example to achieve this.\n\nIf you used one of the inspiration examples last chapter, here's some ideas of possible categorical predictors you could add:\n\n::: {.callout-tip collapse=\"true\"}\n#### Invasive plants\n\nContinuous response variable: **Area invaded per year** (m^2^/year)\n\nThe typical range will be between ~ 10-1000 m^2^/year.\n\nCategorical predictor: **Region** (coastal, inland, mountains)\n\nOn average, the rate of invasion is probably faster in coastal regions, and slower in mountainous ones.\n\nCategorical predictor: **Species** (kudzu, knotweed, starthistle)\n\nKudzu spreads particularly fast!\n:::\n\n::: {.callout-tip collapse=\"true\"}\n#### Gut microbiome & blood glucose\n\nContinuous response variable: **Blood glucose level** (mg/dL)\n\nSomewhere in the range 70-120 mg/dL would be typical (the upper end would be considered pre-diabetic).\n\nPossible categorical predictors: **Diet type** (vegetarian, vegan, omnivore)\n\nCompared to \"normal\" omnivores, vegetarians/vegans will probably have lower blood glucose levels.\n\nCategorical predictor: **Antibiotic use** in the last 6 months (yes/no)\n\nRecent antibiotic use will probably disrupt the microbiome, and lead to poorer glucose metabolism/higher blood glucose levels.\n:::\n\n:::\n\n## Summary\n\nWhether our predictor is categorical or continuous, we still need to follow the same workflow to simulate the dataset.\n\nHowever, categorical predictors are a touch more complicated to simulate - we need a couple of extra functions, and it's conceptually a little harder to get used to.\n\n::: {.callout-tip}\n#### Key Points\n\n-   Categorical predictors require vectors for their beta coefficients, unlike continuous predictors that just have constants\n-   This means we need to use a design matrix to multiply our vector beta coefficient by our categorical variable\n-   Otherwise, the procedure is identical to simulating with a continuous predictor\n-   You can include any number of continuous and categorical predictors in the same simulation\n:::\n",
    "supporting": [
      "06-categorical-predictors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}