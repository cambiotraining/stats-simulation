{
  "hash": "a8e9f0913207a7b53db195b70f2a582a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulation basics\"\noutput: html_document\n---\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nThis introductory chapter teaches a number of key programming skills, and a handful of important functions, that will be required throughout the course.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n```\n:::\n\n\n\n:::\n:::\n\n## Drawing samples from distributions\n\nThe first thing we need to get comfortable with is random sampling, i.e., drawing a number of data points from an underlying distribution with known parameters.\n\nParameters are features of a distribution that determine its shape. For example, the normal distribution has two parameters: mean and variance (or standard deviation).\n\nThis random sampling is what we're (hopefully) doing when we collect a sample in experimental research. The underlying distribution of the response variable, i.e., the \"ground truth\" in the world, has some true parameters that we're hoping to estimate. We collect a random subset of individual observations that have come from that underlying distribution, and use the sample's statistics to estimate the true population parameters.\n\n### Sampling from the normal distribution\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThis is the `rnorm` function, which we'll be using a lot in this course. It takes three arguments; the first is the number of data points (`n`) that you'd like to draw. The second and third arguments are the two important parameters that describe the shape of the underlying distribution: the mean and the standard deviation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(n = 100, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1]  1.19413751  0.82488070 -2.03280498 -0.88162144  1.18334504 -0.39282199\n  [7] -0.12127449 -0.15390516 -0.46127790 -0.04781102 -0.44334334 -2.00062571\n [13]  1.22398257  0.67154047  1.47657080 -1.19171259  2.16088881 -0.97939882\n [19] -1.79809054 -0.44310934 -0.11891198 -1.14404607 -1.25829316 -0.72153445\n [25]  0.17065808 -0.02609742  0.36663604 -0.04358770  0.34503480 -0.39139886\n [31] -0.28992797 -0.49015842 -1.49416554 -0.48446625 -0.51228646 -1.10958688\n [37]  0.55145048  1.82860448  0.73901879  0.93363081  2.10771899 -0.69986824\n [43] -2.13965984 -1.77468761 -0.37997560  0.14708999  1.85477149 -1.55308918\n [49] -1.65166624 -0.30509286  0.47929820  1.70626061 -0.11505053 -0.79504953\n [55] -0.72939977  1.49252551  0.90405473 -0.12107089  0.56995000 -0.17964194\n [61]  0.23424090  0.38902411  1.22403197 -0.01221970 -0.27339820  0.80621331\n [67] -0.65283381  0.41752319  0.60668847 -0.72073520  0.60607566 -1.10160450\n [73] -1.94933903  0.32016798 -0.36919805  1.10603281  1.02892974  0.22290437\n [79]  1.31721136  1.32376693 -1.22220649  0.40748980 -1.55241789 -2.22279126\n [85]  1.03753667  1.00338479 -0.94600811  1.36995440 -0.23484232  0.84252324\n [91] -0.63410919 -0.13528642  1.63893883 -1.11933757 -0.47185729 -1.51330546\n [97] -0.42440094 -0.60276070  1.08890552  1.26541031\n```\n\n\n:::\n:::\n\n\n\n\nWithout any further instruction, R simply prints us a list of length `n`; this list contains numbers that have been pulled randomly from a normal distribution with mean 0 and standard deviation 1. \n\nThis is a sample or dataset, drawn from an underlying population, which we can now visualise.\n\nWe'll use the base R `hist` function for this for now, just to keep things simple:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(100, 0, 1) %>%\n  hist()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/rnorm piped to hist-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\nThe typical default method in Python for sampling from a normal distribution is via `numpy.random.normal`.\n\nIt takes three arguments: `loc` (mean), `scale` (standard deviation) and `size` (sample size):\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnorm_data = np.random.normal(loc = 0, scale = 1, size = 100)\nprint(norm_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[ 0.78042452  1.05877077 -0.34322234  1.43460346 -0.8107738  -0.56610806\n  0.29240371  1.18303492  0.09221987  0.40608544  0.70036237  0.07382069\n  0.05185122 -0.19615453  0.30460381 -0.48704251 -0.06146062  0.83327572\n  0.29220194 -0.10928813 -0.67547496  0.8103553  -1.06245438  0.67023523\n -0.0689226   1.29951556 -0.08255901  0.33776224 -0.60184521  1.58049532\n -0.95352373  1.87304687 -0.29196356  1.7634171  -0.07839978  0.35940231\n -1.39799574  0.14307504  0.07749239  0.50073757 -1.68780869  1.25269124\n -1.31596663  0.08712805 -1.28480501  1.03045276 -0.34327464  0.13857333\n -1.39290948  0.25439201  2.44242787 -1.31774668  0.85526929 -0.13559466\n -1.29102764 -0.68490814 -1.26409663  0.54079704  0.19252574  1.37421886\n -1.19498362 -1.18685347 -0.43953623 -1.60354423 -1.11632251 -0.68973399\n -1.28558718 -0.09733034 -1.28059179 -0.0505321   0.4437826   0.25695276\n -0.26818463 -0.3929834  -0.79763533 -1.50352889 -0.28198761 -0.64125615\n  0.12573249  0.68504399 -0.56023183 -1.29547165  0.75066062 -0.03600893\n  0.12794686  0.87432656  0.78940053  0.11008806  1.09901901  0.67523599\n  1.51416034 -0.72093414 -0.07365004  0.07671765 -0.27453043 -1.13162133\n  2.33548551  1.29736948  1.0842725  -0.67639435]\n```\n\n\n:::\n:::\n\n\n\n\nThe output is an array of numbers, of length `size`.\n\nThis is a sample or dataset, drawn from an underlying population, which we can now visualise.\n\nFor this first example, we'll show how to use both `matplotlib` and `plotnine` to create histograms. We'll use the `matplotlib` version for speed as we go through the course, but if you're transitioning over from R (or `ggplot`), you might find `plotnine` friendlier.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.hist(norm_data)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/plot hist matplot-1.png){width=672}\n:::\n:::\n\n\n\n\nIf plotting multiple histograms, you can use `plt.clf()` to clear the figure, or `plt.close()` to close the plotting window entirely.\n\nIf using `plotnine`, you have to convert the array to a data frame before you can visualise it:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnorm_df = pd.DataFrame({\"values\":norm_data})\n\nnorm_df_hist = (\n  ggplot(norm_df, aes(x = \"values\")) +\n  geom_histogram()\n)\n\nprint(norm_df_hist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ggplot: (640 x 480)>\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Sampling from other distributions\n\nThe normal/Gaussian distribution might be the most famous of the distributions, but it is not the only one that exists - nor the only one that we'll care about on this course.\n\nFor example, we'll also be sampling quite regularly from the uniform distribution.\n\nThe uniform distribution is flat: inside the range of possible values, all values are equally likely, and outside that range, the probability density drops to zero. This means the only parameters we need to set are the minimum and maximum of that range of possible values, like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunif(n = 100, min = 0, max = 1) %>%\n  hist(xlim = c(-0.5, 1.5))\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/runif piped to hist-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nunif_data = np.random.uniform(low = 0, high = 1, size = 100)\n\nplt.clf() # clear existing plot, if applicable\nplt.hist(unif_data)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/uniform dist histogram-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nThe underlying shape of the distribution that we just sampled from looks like this - square and boxy. The probability density is zero outside of the 0-1 range, and flat inside it:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/visualise unif dist-3.png){width=672}\n:::\n:::\n\n\n\n\nLater in this course, we will sample from the binomial, negative binomial and Poisson distributions as well.\n\n## Setting a seed\n\nWhat happens if you run this block of code over and over again?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(100, 0, 1) %>%\n  hist()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/rnorm piped to hist again-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.clf()\nplt.hist(np.random.normal(0, 1, 100))\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/hist of multiple norm-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nEach time you run the code, you are sampling a unique random subset of data points.\n\nIt's very helpful that we can do that - later on in this course, we'll exploit this to sample many different datasets from the same underlying population.\n\nHowever, sometimes it's useful for us to be able to sample the *exact* set of data points more than once.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nTo achieve this, we can use the `set.seed` function.\n\nRun the following code several times in a row, and you'll see the difference:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n\nrnorm(100, 0, 1) %>%\n  hist()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/rnorm with seed-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\nTo achieve this, we can use the `np.random.seed` function.\n\nRun the following code several times in a row, and you'll see the difference:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(20)\n\nplt.clf()\nplt.hist(np.random.normal(0, 1, 100))\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/np.norm with np.seed-1.png){width=672}\n:::\n:::\n\n\n\n\n:::\n\nNotice how each time, the exact same dataset and histogram are produced?\n\nYou can choose any number you like for the seed. All that matters is that you return to that same seed number, if you want to recreate that dataset.\n\n## Soft-coding parameters\n\nIn the code above, we have \"hard-coded\" our parameters by putting them directly inside the `rnorm` or `np.random.normal` functions.\n\nHowever, from here on, we will start \"soft-coding\" parameters or other values that we might want to change.\n\nThis is considered good programming practice. It's also sometimes called \"dynamic coding\".\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100\nmean_n <- 0\nsd_n <- 1\n\nrnorm(n = n, mean = mean_n, sd = sd_n) %>%\n  hist()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/softcode rnorm-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 100\nmean = 0\nsd = 1\n\nplt.clf()\nplt.hist(np.random.normal(loc = mean, scale = sd, size = n))\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/softcode np.random.norm-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nThis might look like we're writing out more code, but it will be helpful in more complex simulations where we use the same parameter more than once.\n\nWe're also separating out the bits of the code that might need to be edited, which are all at the top where we can more easily see them, versus the bits we can leave untouched. \n\n## Loops\n\nIn programming, a loop is a chunk of code that is run repeatedly, until a certain condition is satisfied.\n\nThere are two broad types of loop: the for loop and the while loop. For the purposes of this course, we'll only really worry about the for loop. For loops run for a pre-specified number of iterations before stopping.\n\n### For loop syntax\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nIn R, the syntax for a for loop looks like this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:5) {\n  \n  print(i)\n\n  }\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n```\n\n\n:::\n:::\n\n\n\n\nHere, `i` is our loop variable. It will take on the values in `1:5`, one at a time.\n\nFor each value of our loop variable, the code inside the loop body - defined by `{}` curly brackets in R - will run.\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfor i in range(1, 6):\n    print(i)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1\n2\n3\n4\n5\n```\n\n\n:::\n:::\n\n\n\n\nHere, `i` is our loop variable. It will take on the values in `1:5`, one at a time.\n\n(Note that `range(1, 6)` does **not** include `6`, the endpoint.)\n\nFor each value of our loop variable, the code inside the loop body - defined by indentation in Python - will run.\n:::\n\nIn this case, all we are asking our loop to do is print `i`. It'll do this 5 times, increasing the value of `i` each time for each new iteration of the loop.\n\nBut, we can ask for more complex things than this on each iteration, and we don't always have to interact with `i`. \n\n### Visualising means with for loops {#sec-exm_mean-for-loop}\n\nYou might've guessed based on context clues, but we can use for loops to perform repeated simulations using the same starting parameters (in fact, we'll do that a lot in this course).\n\nIn this loop, we sample 3 unique datasets, each made up of 20 random data points, from a normal distribution with mean 4 and standard deviation 0.5.\n\nThen, we produce a histogram for each dataset, overlaying the mean value each time.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:3) {\n  \n  n <- 20\n  mean_n <- 4\n  sd_n <- 0.5\n  \n  data <- rnorm(n, mean_n, sd_n) \n\n  hist(data, xlim = c(1, 7))\n  abline(v = mean(data), col = \"red\", lwd = 3)\n\n  }\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/less simple for loop-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/less simple for loop-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/less simple for loop-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.close()\n\nfor i in range(1, 4):\n  n = 20\n  mean = 4\n  sd = 0.5\n  data = np.random.normal(mean, sd, n)\n  plt.figure()\n  plt.hist(data)\n  plt.axvline(x = statistics.mean(data), color = 'r')\n  plt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/less simple loop with np.r.n-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/less simple loop with np.r.n-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/less simple loop with np.r.n-3.png){width=672}\n:::\n:::\n\n\n\n:::\n\nWe can see that the means in each case are mostly hovering around 4, which is reassuring, since we know that's the true population mean.\n\n## Exercises\n\n### Revisiting Shapiro-Wilk {#sec-exr_shapiro}\n\nNow that you know how to perform random sampling, let's link it back to a specific statistical test, the Shapiro-Wilk test.\n\nAs a reminder: the Shapiro-Wilk test is used to help us decide whether a sample has been drawn from a normal distribution or not. It's one of the methods we have for checking the normality assumption.\n\nHowever, it is also itself a null hypothesis test. The null hypothesis is that the underlying distribution is normal, so a significant p-value is usually interpreted as evidence that the normality assumption is violated.\n\n::: {.callout-exercise}\n\n\n\n\n{{< level 1 >}}\n\n\n\n\n\n\nIn this exercise, using the template code provided as a starting point:\n\n1. Try a variety of different seeds (hint: `20` might be interesting...)\n2. Sample from a uniform distribution instead\n3. While sampling from both normal and uniform distributions, try a variety of sample sizes (including `n < 10`)\n4. Create normal QQ plots, to compare them to the Shapiro-Wilk results (use the `qqnorm` function)\n\nTry to create:\n\n- A false positive error\n- A false negative error\n\nWhat does this teach you about the nature of the Shapiro-Wilk test, and null hypothesis significance tests in general?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nTemplate code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(200)\nn <- 100\nMean <- 0\nSD <- 1\n\ndata <- rnorm(n, Mean, SD) \n\ndata %>% hist()\n```\n\n::: {.cell-output-display}\n![](01-simulation-basics_files/figure-html/template for SW exercise-7.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>% shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.97978, p-value = 0.1279\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n## Summary\n\nThis chapter covers some key coding basics that will be important in later chapters, such as functions for random sampling, \"soft-coding\" of variables, and for loops.\n\nA key simulation concept - sampling from distributions with known parameters - was also introduced. This is central to all of the simulating we will do in the remaining chapters.\n\n::: {.callout-tip}\n#### Key Points\n\n-   There are a suite of functions for sampling data points from distributions with known parameters\n-   Each distribution has its own function, with different parameters that we need to specify\n-   Good coding practice (soft-coding of variables, setting a seed, and using loops effectively) all help us to simulate more efficiently\n:::\n\n",
    "supporting": [
      "01-simulation-basics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}