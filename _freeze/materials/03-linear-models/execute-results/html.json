{
  "hash": "c44ba1191723d4494901258ae4524632",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predictor and response variables\"\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nThis chapter shows how to simulate a variable from a distribution that is conditional on other variables: in other words, how to simulate a response variable in a linear model containing one or more predictors.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\n\n# These packages will be used for evaluating the models we fit to our simulated data\nlibrary(performance)\nlibrary(ggResidpanel)\n\n# This package is optional/will only be used for later sections in this chapter\nlibrary(MASS)\n```\n:::\n\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom patsy import dmatrix\n```\n:::\n\n\n\n\n:::\n:::\n\n## Golden toads\n\nFor the examples in the materials shown here, we're going to simulate a dataset about golden toads, an extinct species of amphibians. We'll simulate various predictor variables, and their relationship to the clutch size (number of eggs) produced by a given toad.\n\n![Here's what the fancy little guys looked like! [image source](https://en.wikipedia.org/wiki/Golden_toad#/media/File:Bufo_periglenes2.jpg)](images/goldentoad.png)\n\n### Continuous predictors\n\nWe'll start with continuous predictors, because these are a little easier than categorical ones, and simulate a dataset suitable for simple linear regression.\n\n#### Set seed and sample size\n\nFirst, we set a seed and a sample size: \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n\n# sample size\nn <- 60\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(25)\n\n# sample size\nn = 60\n```\n:::\n\n\n\n\n:::\n\n#### Generate values of predictor variable\n\nThe next step is to generate our predictor variable.\n\nThere's no noise or uncertainty in our predictor (remember that residuals are always in the y direction, not the x direction), so we can just produce the values by sampling from a distribution of our choice.\n\nOne of the things that can cause variation in clutch size is the size of the toad herself, so we'll use that as our continuous predictor. This sort of biological variable would probably be normally distributed, so we'll use `rnorm` to generate it.\n\nGoogle tells us that the average female golden toad was somewhere in the region of 42-56mm long, so we'll use that as a sensible basis for our normal distribution for our predictor variable `length`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength <- rnorm(n, 48, 3)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlength = np.random.normal(48, 3, n)\n```\n:::\n\n\n\n:::\n\n#### Simulate average values of response variable\n\nNow, we need to simulate our response variable, `clutchsize`.\n\nWe're going to do this by setting up the linear model. We'll specify a y-intercept for `clutchsize`, plus a gradient that captures how much `clutchsize` changes as `length` changes.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- 175\nb1 <- 2\n\nsdi <- 20\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nb0 = 175\nb1 = 2\n\nsdi = 20\n```\n:::\n\n\n\n\n:::\n\nWe've also added an `sdi` parameter. This captures the standard deviation *around* the model predictions that is due to other factors we're not measuring. In other words, this will determine the size of our residuals.\n\nNow, we can simulate our set of predicted values for `clutchsize`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_clutch <- b0 + b1*length\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\navg_clutch = b0 + b1*length\n```\n:::\n\n\n\n:::\n\nYou'll notice we've just written out the equation of our model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(length, avg_clutch) %>%\n  ggplot(aes(x = length, y = avg_clutch)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/visualise dataset using average response-1.png){width=672}\n:::\n:::\n\n\n\n\nWe use the `tibble` function to combine our response and predictor variables together into a single dataset.\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntempdata = pd.DataFrame({'length': length, 'avg_clutch': avg_clutch})\n\ntempdata.plot.scatter(x = 'length', y = 'avg_clutch')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/visualise data with avg response-1.png){width=672}\n:::\n:::\n\n\n\n\nWe use the `pd.DataFrame` function to stitch our arrays together into a single dataframe object with multiple columns.\n\n:::\n\nWhen we visualise `length` and `avg_clutch` together, you see they perfectly form a straight line. That's because `avg_clutch` doesn't contain the residuals - that comes next.\n\n#### Simulate actual values of response variable\n\nThe final step is to simulate the actual values of clutch size. \n\nWe'll use `rnorm` function again, and we put `avg_clutch` in as our mean. This is because the set of actual clutch size values should be normally distributed around our set of predictions.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ngoldentoad <- tibble(clutchsize, length)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n#### Checking the dataset\n\nLet's make sure our dataset is behaving the way we intended.\n\nFirst, we'll visualise it:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, y = clutchsize)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/visualise final dataset-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngoldentoad.plot.scatter(x = 'length', y = 'clutchsize')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/visualise the data-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nAnd then, we'll construct a linear model - and check that our beta coefficients have been replicated to a sensible level of precision!\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length, goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.718 -10.973   1.094  11.941  41.690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 267.1395    38.4922   6.940  3.7e-09 ***\nlength        0.1065     0.8038   0.132    0.895    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.76 on 58 degrees of freedom\nMultiple R-squared:  0.0003025,\tAdjusted R-squared:  -0.01693 \nF-statistic: 0.01755 on 1 and 58 DF,  p-value: 0.8951\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.184\nModel:                            OLS   Adj. R-squared:                  0.170\nMethod:                 Least Squares   F-statistic:                     13.07\nDate:                Tue, 03 Jun 2025   Prob (F-statistic):           0.000630\nTime:                        13:59:53   Log-Likelihood:                -256.33\nNo. Observations:                  60   AIC:                             516.7\nDf Residuals:                      58   BIC:                             520.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    157.5257     30.614      5.146      0.000      96.245     218.806\nlength         2.2928      0.634      3.615      0.001       1.023       3.562\n==============================================================================\nOmnibus:                        0.516   Durbin-Watson:                   2.307\nProb(Omnibus):                  0.773   Jarque-Bera (JB):                0.492\nSkew:                          -0.207   Prob(JB):                        0.782\nKurtosis:                       2.839   Cond. No.                         649.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n:::\n\nNot bad at all. The linear model has managed to extract beta coefficients similar to the original `b0` and `b1` that we set.\n\nIf you're looking to explore and understand this further, try exploring the following things in your simulation, and see how they affect the p-value and the precision of the beta estimates:\n\n-   Varying the sample size\n-   Varying the `sdi`\n-   Varying the `b1` parameter\n\n### Categorical predictors\n\nCategorical predictors are a tiny bit more complex to simulate, as the beta coefficients switch from being constants (gradients) to vectors (representing multiple means).\n\nLet's imagine that golden toads living in different ponds produce slightly different clutch sizes, and simulate some sensible data on that basis.\n\nYou might find it helpful to delete variables/clear the global environment, so that nothing from your previous simulation has an unexpected impact on your new one:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls())\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel(avg_clutch, clutchsize, goldentoad, length, lm_golden, model, tempdata)\n```\n:::\n\n\n\n:::\n\n(This step is optional!)\n\n#### Parameters and predictor variable\n\nThen, we'll set up the parameters and predictor variables:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n\nn <- 60\nb0 <- 175\nb1 <- 2\nb2 <- c(0, 30, -10)\n\nsdi <- 20\n\nlength <- rnorm(n, 48, 3)\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n```\n:::\n\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### rep and c functions\n\nThe `rep` and `c` functions in R (`c` short for `concatenate`) will come up quite a lot when generating predictor variables.\n\nThe `rep` function takes two arguments each time: \n\n- First, an argument containing the thing that you want repeated (which can either be a single item, or a list/vector)\n- Second, an argument containing information about how many times to repeat the first argument (there are actually multiple options for how to phrase this second argument)\n\nThe `c` function is a little simpler: it combines, i.e., concatenates, all of the arguments you put into it into a single vector/list item.\n\nYou can then combine these two functions together in various ways to achieve what you want.\n\nFor example, these two lines of code both produce the same output:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(rep(\"A\", times = 3), rep(\"B\", times = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\"\n```\n\n\n:::\n\n```{.r .cell-code}\nrep(c(\"A\", \"B\"), each = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\"\n```\n\n\n:::\n:::\n\n\n\nThe first version asks for `\"A\"` to be repeated 3 times, `\"B\"` to be repeated 3 times, and then these two triplets to be concatenated together into one list.\n\nThe second version asks us to take a list of two items, `\"A\", \"B\"`, and repeat each element 3 times each.\n\nMeanwhile, the code below will do something very different:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrep(c(\"A\", \"B\"), times = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\"\n```\n\n\n:::\n:::\n\n\n\n\nThis line of code asks us to take the list `\"A\", \"B\"` and repeat it, as-is, 3 times. So we get a final result that alternates.\n\nThis shows that choosing carefully between `times` or `each` as your second argument for the `rep` function can be absolutely key to getting the right output!\n\nDon't forget you can use `?rep` or `?c` to get more detailed information about how these functions work, if you would like it.\n:::\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(20)\n\nn = 60\nb0 = 175\nb1 = 2\nb2 = np.array([0, 30, -10])\n\nsdi = 20\n\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\", \"B\", \"C\"], repeats = n//3)\n```\n:::\n\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### np.repeat and np.tile functions\n\nWhen generating categorical variables, you are essentially repeating the category names multiple times.\n\nNote the difference between these two outputs, produced using two different `numpy` functions:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.repeat([\"A\", \"B\", \"C\"], repeats = n//3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',\n       'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B',\n       'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n       'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C',\n       'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'], dtype='<U1')\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.tile([\"A\", \"B\", \"C\"], reps = n//3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray(['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A',\n       'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B',\n       'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C',\n       'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A',\n       'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'], dtype='<U1')\n```\n\n\n:::\n:::\n\n\n\n\nIn both functions, the first argument is an list of category names, with no duplication. Then, you specify the number of repeats with either `repeats` or `reps` (for `np.repeats` vs `np.tile` respectively).\n\nHowever, the two functions do slightly different things:\n\n- `np.repeats` takes each element of the list and repeats it a specified number of times, before moving on to the next element\n- `np.tile` takes the entire list as-is, and repeats it as a chunk for a desired number of times\n\nIn both cases we end up with an array of the same length, but the order of the category names within that list is very different.\n:::\n:::\n\n#### Simulate average values of response\n\nUp to this point, we've set up a beta coefficient for our categorical predictor, which consists of three categories. The ponds have imaginatively been named `A`, `B` and `C`.\n\nNow, exactly as we did with the continuous predictor, we will simulate a set of predicted values using the model equation. We use the equation from above, but add our extra predictor/term.\n\nHowever, including a categorical predictor in our model equation is a bit more complex. We can no longer simply multiply our beta coefficient by our predictor, so we have to use slightly different syntax:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_clutch <- b0 + b1*length + model.matrix(~0+pond) %*% b2\n```\n:::\n\n\n\n\nThe `model.matrix` function produces a table of 0s and 1s, which is a matrix representing the design of our experiment. In this case, it has three columns, which matches the number of categories and the length of our `b2` coefficient. \n\nOur `b2` is also technically a matrix, just with one row. Then, `%*%` is the operator in R for matrix multiplication, to multiply these two things together.\n\nDon't worry - you don't really need to understand matrix multiplication to get used to this method. If that explanation was enough for you, you'll be just fine from here.\n\nWe'll use this syntax a few more times in this chapter, so you'll learn to recognise and repeat the syntax!\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a design matrix for the categorical predictor\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\n\n# use np.dot to multiply design matrix by beta coefficient\navg_clutch = b0 + b1*length + np.dot(Xpond, b2)\n```\n:::\n\n\n\n\nThe `dmatrix` function from `patsy` produces a table of 0s and 1s, which is a matrix representing the design of our experiment. In this case, it has three columns, which matches the number of categories and the length of our `b2` coefficient. \n\nOur `b2` is also technically a matrix, just with one row. We use `np.dot` to multiply these matrices together.\n\nDon't worry - you don't really need to understand matrix multiplication to get used to this method. If that explanation was enough for you, you'll be just fine from here.\n\nWe'll use this syntax a few more times in this chapter, so you'll learn to recognise and repeat the syntax!\n:::\n\n#### Simulate actual values of response\n\nThe last step is identical to before.\n\nWe sample our actual values of `clutchsize` from a normal distribution with `avg_clutch` as the mean and with a standard deviation of `sdi`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ngoldentoad <- tibble(clutchsize, length, pond)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'pond': pond, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n#### Checking the dataset\n\nOnce again, we'll visualise and model these data, to check that they look as we suspected they would.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length + pond, goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length + pond, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.202 -11.751  -0.899  14.254  37.039 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 262.8111    38.6877   6.793  7.6e-09 ***\nlength        0.1016     0.8108   0.125    0.901    \npondB        39.2084     5.9116   6.632  1.4e-08 ***\npondC        -5.5279     5.9691  -0.926    0.358    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.69 on 56 degrees of freedom\nMultiple R-squared:  0.5481,\tAdjusted R-squared:  0.5239 \nF-statistic: 22.64 on 3 and 56 DF,  p-value: 9.96e-10\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/testing cat predictor dataset-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length + pond\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.354\nModel:                            OLS   Adj. R-squared:                  0.319\nMethod:                 Least Squares   F-statistic:                     10.23\nDate:                Tue, 03 Jun 2025   Prob (F-statistic):           1.80e-05\nTime:                        13:59:58   Log-Likelihood:                -263.47\nNo. Observations:                  60   AIC:                             534.9\nDf Residuals:                      56   BIC:                             543.3\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    196.9612     37.710      5.223      0.000     121.419     272.504\npond[T.B]     24.0615      6.419      3.749      0.000      11.204      36.919\npond[T.C]    -10.4082      6.426     -1.620      0.111     -23.282       2.465\nlength         1.5491      0.781      1.984      0.052      -0.015       3.114\n==============================================================================\nOmnibus:                        3.489   Durbin-Watson:                   2.017\nProb(Omnibus):                  0.175   Jarque-Bera (JB):                1.803\nSkew:                          -0.074   Prob(JB):                        0.406\nKurtosis:                       2.164   Cond. No.                         695.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n:::\n\nHas our model recreated \"reality\" very well? Would we draw the right conclusions from it?\n\n### Interactions\n\nNow, let's simulate an interaction effect `length:pond`.\n\n::: {.callout-note}\nSince at least one of the variables in our interaction is a categorical predictor, requiring a vector beta coefficient and the use of the `model.matrix` syntax, the interaction will also need be the same.\n\nThink of it this way: our model with an interaction term will consist of three lines of best fit, each with a different intercept *and* gradient.\n\nThe difference in intercepts is captured by `b2`, and then the difference in gradients is captured by `b3` that we set now.\n:::\n\n#### Set up parameters and predictors\n\nTo run this simulation, we're going to need an extra categorical beta coefficient, which we include in our initial parameters:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls())\n\nset.seed(20)\n\nn <- 60\nb0 <- 175\nb1 <- 2\nb2 <- c(0, 30, -10)\nb3 <- c(0, 0.5, -0.2)\n\nsdi <- 20\n\nlength <- rnorm(n, 48, 3)\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel(length, pond, goldentoad, clutchsize, avg_clutch) # optional clean-up\n\nnp.random.seed(23)\n\nn = 60\nb0 = 175\nb1 = 2\nb2 = np.array([0, 30, -10])\nb3 = np.array([0, 0.5, -0.2])\n\nsdi = 20\n\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\n```\n:::\n\n\n\n:::\n\n#### Simulate response variable\n\nThen, we continue exactly as we did before. \n\nWe don't need to set up a new predictor for our interaction, since it uses our existing two predictors.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_clutch <- b0 + b1*length + model.matrix(~0+pond) %*% b2 + model.matrix(~0+length:pond) %*% b3\n\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ngoldentoad <- tibble(clutchsize, length, pond)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# construct design matrices for categorical predictor & interaction\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\n\n# simulate response variable in two steps\navg_clutch = b0 + b1*length + np.dot(Xpond, b2) + np.dot(Xpond_length, b3)\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# stitch variables together into a data frame\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n#### Checking the dataset\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/check interaction dataset-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length*pond, goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length * pond, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.409 -11.136  -1.377  12.863  37.095 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  271.19106   64.37630   4.213 9.64e-05 ***\nlength        -0.07503    1.35414  -0.055    0.956    \npondB         11.00129   89.05626   0.124    0.902    \npondC          8.73498  106.35392   0.082    0.935    \nlength:pondB   1.09421    1.87219   0.584    0.561    \nlength:pondC  -0.49062    2.20866  -0.222    0.825    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19 on 54 degrees of freedom\nMultiple R-squared:  0.7793,\tAdjusted R-squared:  0.7588 \nF-statistic: 38.13 on 5 and 54 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length * pond\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.788\nModel:                            OLS   Adj. R-squared:                  0.769\nMethod:                 Least Squares   F-statistic:                     40.21\nDate:                Tue, 03 Jun 2025   Prob (F-statistic):           5.01e-17\nTime:                        14:00:02   Log-Likelihood:                -264.07\nNo. Observations:                  60   AIC:                             540.1\nDf Residuals:                      54   BIC:                             552.7\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept          215.5857     78.036      2.763      0.008      59.133     372.039\npond[T.B]         -195.5107    108.751     -1.798      0.078    -413.543      22.522\npond[T.C]          -72.0805    137.728     -0.523      0.603    -348.208     204.047\nlength               1.2552      1.639      0.766      0.447      -2.030       4.541\nlength:pond[T.B]     5.1592      2.262      2.280      0.027       0.623       9.695\nlength:pond[T.C]     0.8129      2.869      0.283      0.778      -4.939       6.564\n==============================================================================\nOmnibus:                        4.125   Durbin-Watson:                   2.550\nProb(Omnibus):                  0.127   Jarque-Bera (JB):                3.455\nSkew:                           0.582   Prob(JB):                        0.178\nKurtosis:                       3.166   Cond. No.                     3.23e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.23e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n## Exercises\n\n### Including confounds\n\n::: {.callout-exercise}\n\n\n\n{{< level 1 >}}\n\n\n\n\n\n\nConfounding variables are just additional predictors that we are trying to account for (rather than interpret).\n\nThis means that modelling *and* simulating confounds is identical to any other predictor.\n\nIn this exercise:\n\n- Create a continuous covariate of no interest (perhaps `temperature` or `age` of toad)\n- Create a categorical covariate of no interest (perhaps `vegetation cover` or `presence of predators` at the time the toad was laying her eggs)\n- Compare models with and without the covariate of no interest included\n\nRemember that you don't need to have values that are totally biologically plausible, especially if you don't know anything about toads. We're focusing on the simulation here, not on perfect realism in the dataset!\n\nTo extend further:\n\n- Simulate a dataset where a continuous covariate of no interest has an interaction with a predictor of interest\n- Compare models with and without the interaction term, to see if you're able to recreate the original parameters you set\n:::\n\n### Interactions between categorial predictors\n\n::: {.callout-exercise}\n\n\n\n{{< level 3 >}}\n\n\n\n\n\n\nSimulate an interaction effect between two categorical predictors, for example `pond:predators` (or other categorical predictors, if inspiration has struck!)\n\nYou won't need any new syntax or functions, but you will need to think a little bit about what it means for two categorical variables to interact with one another. \n\n**Hint**: think about how many means are being estimated. It might help to use the `model.matrix` function to help you figure out what your beta coefficient should look like.\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\nFor this worked answer, we'll use `pond:predators` as our interaction effect.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n\nn <- 60\nb0 <- 165\nb1 <- 2               # length \nb2 <- c(0, 30, -10)   # pond (A, B, C)\nb3 <- c(0, 20)        # presence of predator (no, yes)\n\nsdi <- 20\n\nlength <- rnorm(n, 48, 3)\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n\npredators <- rep(c(\"yes\", \"no\"), times = n/2)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(23)\n\nn = 60\nb0 = 165\nb1 = 2                        # length \nb2 = np.array([0, 30, -10])   # pond (A, B, C)\nb3 = np.array([0, 20])        # presence of predator (no, yes)\n\nsdi = 20\n\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\n\npredators = np.tile([\"yes\",\"no\"], reps = n//2)\n```\n:::\n\n\n\n:::\n\nSet up your simulation as normal. Then, figure out what `b4` (the coefficient for `pond:predators`) needs to look like.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(~0 + pond:predators)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  pondA:predatorsno pondB:predatorsno pondC:predatorsno pondA:predatorsyes\n1                 0                 0                 0                  1\n2                 1                 0                 0                  0\n3                 0                 0                 0                  1\n4                 1                 0                 0                  0\n5                 0                 0                 0                  1\n6                 1                 0                 0                  0\n  pondB:predatorsyes pondC:predatorsyes\n1                  0                  0\n2                  0                  0\n3                  0                  0\n4                  0                  0\n5                  0                  0\n6                  0                  0\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nXpond_pred = dmatrix('0 + C(pond):C(predators)', data = {'pond': pond, 'predators': predators})\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.]])\n```\n\n\n:::\n:::\n\n\n\n:::\n\nThe matrix is 60 rows long (we're looking at just the top few rows) and has 6 columns.\n\nThose 6 columns represent our 6 possible subgroups, telling us that our `b4` coefficient will need to be 6 elements long.\n\nThe first 4 elements will be 0s, because our `b1`, `b2` and `b3` coefficients already contain values for 4 of our subgroups. We then need two additional unique numbers for the remaining 2 subgroups.\n\n::: {.callout-tip collapse=\"true\"}\n#### A bit more explanation on b4\n\nRemember that when fitting the model, our software will choose a group as a reference group. All the values in our beta coefficients represent the difference in the mean between that reference group, and the other levels of our categorical variable.\n\nIf there is no interaction between `pond:predators`, this means that the difference in group means between `pondA:predatorsno` and `pondB:predatorsno` is the same as the difference between `pondA:predatorsyes` and `pondB:predatorsyes`. (Along with the differences between `pondA` and `pondC`, as well.)\n\nThis means that a single value can represent the difference between `predatorsno` and `predatorsyes` in all three of our ponds, as you see in the R plot below - the black bar is the same height for each pond.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'pond'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/only 4 numbers needed-1.png){width=672}\n:::\n:::\n\n\n\n\nThis plot, with the equal differences, is therefore representing a situation where there is no interaction between `pond:predator`, but there are independent main effects of `pond` and `predator`.\n\nIf we include the interaction term, however, then that's no longer the case. Within each pond, there can be a completely unique difference between when predators were and weren't present.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'pond'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/6 numbers needed-1.png){width=672}\n:::\n:::\n\n\n\n\nSo, each of our 5 non-reference subgroups will have a completely unique difference in means from our reference subgroup. This means our simulation needs to provide 6 unique values across our beta coefficients.\n\nIn the code above, we've already specified 4 values:\n\n- `b0`, the baseline mean of our reference group `pondA:predatorsno`\n- `b2`, containing two numbers; these capture the difference between the reference group and `pondB:predatorsno`/`pondC:predatorsno`\n- `b3`, containing one number; this captures the difference between the reference group and `pondA:predatorsyes`\n\nWe still need two more numbers, to capture the difference between the reference group and `pondB:predatorsyes`/`pondC:predatorsyes`.\n\n(We can ignore `b1` for now, as it has nothing to do with this interaction. Since `length` has no interactions with other variables, we are setting things up such that all 6 subgroups have an identical `clutchsize ~ length` relationship within them.)\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb4 <- c(0, 0, 0, 0, -20, 40)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nb4 = np.array([0, 0, 0, 0, -20, 40])\n```\n:::\n\n\n\n:::\n\nSince there are 6 subgroups, and the first 4 from our model design matrix are already dealt with, we only need two additional numbers. The other groups don't need to be adjusted further.\n\nFinally, we simulate our response variable, and then we can check how well our model does at recovering these parameters.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb4 <- c(0, 0, 0, 0, -20, 40)\n\navg_clutch <- b0 + model.matrix(~0+pond) %*% b2 + model.matrix(~0+predators) %*% b3 + model.matrix(~0+pond:predators) %*% b4\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ntoads <- tibble(clutchsize, length, pond, predators)\n\nlm_toads <- lm(clutchsize ~ length + pond * predators, toads)\n\nsummary(lm_toads)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length + pond * predators, data = toads)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.890 -11.348  -0.284   9.929  37.223 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        241.8247    37.8165   6.395 4.24e-08 ***\nlength              -1.8881     0.7868  -2.400   0.0200 *  \npondB               48.0543     8.0604   5.962 2.08e-07 ***\npondC                2.5931     8.0643   0.322   0.7491    \npredatorsyes        40.9971     8.0570   5.088 4.86e-06 ***\npondB:predatorsyes -37.6927    11.4020  -3.306   0.0017 ** \npondC:predatorsyes  23.7371    11.4269   2.077   0.0426 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.01 on 53 degrees of freedom\nMultiple R-squared:  0.693,\tAdjusted R-squared:  0.6583 \nF-statistic: 19.94 on 6 and 53 DF,  p-value: 4.978e-12\n```\n\n\n:::\n\n```{.r .cell-code}\ncheck_model(lm_toads)\n```\n\n::: {.cell-output-display}\n![](03-linear-models_files/figure-html/finishing iteraction simulation goldentoads-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nb4 = np.array([0, 0, 0, 0, -20, 40])\n\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpred = dmatrix('0 + C(predators)', data = {'predators': predators})\nXpond_pred = dmatrix('0 + C(pond):C(predators)', data = {'pond': pond, 'predators': predators})\n\n# simulate response variable in two steps\navg_clutch = b0 + b1*length + np.dot(Xpond, b2) + np.dot(Xpred, b3) + np.dot(Xpond_pred, b4)\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# stitch variables together into a data frame\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n:::\n:::\n\n## Summary\n\nIn this chapter, we started by simulating two-dimensional data and have worked up to three- or four-dimensional data, suitable for use in linear modelling.\n\nIn all cases, we start by simulating our predictor variable(s). Then, we can simulate our response variable as a function of the predictor variable(s) via a linear model equation, with residuals added.\n\nBy definition, the assumptions of the linear model will be always be met, because we are in control of the nature of the underlying population. \n\nHowever, our model may or may not do a good job of \"recovering\" the original beta coefficients we specified, depending on the sample size and the amount of error we introduce in our simulation.\n\n::: {.callout-tip}\n#### Key Points\n\n-   Predictor variables can be simulated from different distributions, with no errors associated\n-   Response variables should be simulated with errors/residuals from the normal distribution\n-   Continuous predictors require a constant beta coefficient and simple multiplication to simulate them\n-   While categorical predictors require vector coefficients and a model matrix to simulate them\n-   Any interaction term containing a categorical predictor, must also be treated as categorical when simulating\n:::\n",
    "supporting": [
      "03-linear-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}