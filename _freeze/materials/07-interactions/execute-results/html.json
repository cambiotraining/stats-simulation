{
  "hash": "aeff5e7fb3ff99946b213546d2519818",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulating interactions\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\n\nlibrary(performance)\nlibrary(ggResidpanel)\n```\n:::\n\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport pingouin as pg\n\nfrom patsy import dmatrix\n```\n:::\n\n\n\n:::\n:::\n\nIn this chapter, we're going to simulate a few different possible interactions for the `goldentoad` dataset we've been building.\n\nAs with main effects, categorical interactions are a little trickier than continuous ones, so we'll work our way up.\n\n## Two continuous predictors\n\nThe easiest type of interaction to simulate is a two-way interaction between continuous predictors.\n\nWe'll use the `length` predictor from before, and add a new continuous predictor, `temperature`.\n\n(I don't know whether temperature actually does predict clutch size in toads - remember, this is made up!)\n\n#### Set up parameters and predictors\n\nFirst, we set the important parameters. This includes the beta coefficients.\n\n::: {.callout-note collapse=\"true\"}\n#### You might notice...\n\nthat we've increase the sample size from previous chapters, and tweaked beta0.\n\nThis is to give our model a fighting chance to recover some sensible estimates at the end, and also to keep the values of our final `clutchsize` variable within some sensible biological window.\n\nHowever, all of this is **optional** - the process of actually doing the simulation would work the same even with the old values!\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(22)\n\nn <- 120\nb0 <- -30             # intercept\nb1 <- 0.7               # main effect of length\nb2 <- 0.5             # main effect of temperature\n\nb3 <- 0.25             # interaction of length:temperature\n\nsdi <- 12\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(28)\n\nn = 120\nb0 = -30            # intercept\nb1 = 0.7            # main effect of length\nb2 = 0.5            # main effect of temperature\n\nb3 = 0.25           # interaction of length:temperature\n\nsdi = 12\n```\n:::\n\n\n\n:::\n\nNotice that the beta coefficient for the interaction is just a single constant - this is always true for an interaction between continuous predictors.\n\nNext, we generate the values for `length` and `temperature`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength <- rnorm(n, 48, 3)\n\ntemperature <- runif(n, 10, 32)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlength = np.random.normal(48, 3, n)\n\ntemperature = np.random.uniform(10, 32, n)\n```\n:::\n\n\n\n:::\n\nJust for a bit of variety, we've sampled `temperature` from a uniform distribution instead of a normal one. \n\nIt won't make any difference at all to the rest of the workflow, but if you'd like, you can test both ways to see whether it has an impact on the visualisation and model at the end!\n\n#### Simulate response variable\n\nThese steps should look familiar from previous chapters.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_clutch <- b0 + b1*length + b2*temperature + b3*length*temperature\n\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ngoldentoad <- tibble(length, temperature, clutchsize)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\navg_clutch = b0 + b1*length + b2*temperature + b3*length*temperature\n\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'temperature': temperature, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n#### Check the dataset\n\nFirst, let's visualise the dataset.\n\nThis isn't always easy, with two continuous variables, but one way that gives us at least some idea is to assign one of our continuous predictors to the `colour` aesthetic:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = temperature)) +\n  geom_point(size = 3) +\n  scale_colour_continuous(type = \"viridis\")\n```\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/test data twoway cont-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.clf()\nplt.scatter(goldentoad[\"length\"], goldentoad[\"clutchsize\"], c=goldentoad[\"temperature\"],  \n    cmap=\"viridis\", s=40)          # optional - set colourmap and point size                \n# add labels\nplt.xlabel(\"Length\"); plt.ylabel(\"Clutch Size\"); plt.colorbar(label=\"Temperature\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0.5, 0, 'Length')\nText(0, 0.5, 'Clutch Size')\n<matplotlib.colorbar.Colorbar object at 0x00000184E6146F90>\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/data test two way cont-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nBroadly speaking, `clutchsize` is increasing with both `length` and `temperature`, which is good - we specified positive betas for both main effects.\n\nSince we specified a positive beta for the interaction, we would expect there to be a bigger increase in `clutchsize` per unit increase in `length`, for each unit increase in `temperature`. \n\nVisually, that *should* look like the beginnings of a \"trumpet\" or \"megaphone\" shape in the data; you're more likely to see that with a larger sample size.\n\nNext, let's fit the linear model and see if we can recover those beta coefficients:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length * temperature, data = goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length * temperature, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.480  -8.079   0.191   8.796  36.987 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -60.41655   71.12790  -0.849 0.397404    \nlength               1.23202    1.49139   0.826 0.410448    \ntemperature          0.11301    3.52109   0.032 0.974452    \nlength:temperature   0.26105    0.07404   3.526 0.000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.05 on 116 degrees of freedom\nMultiple R-squared:  0.9718,\tAdjusted R-squared:  0.9711 \nF-statistic:  1332 on 3 and 116 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length * temperature\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.984\nModel:                            OLS   Adj. R-squared:                  0.984\nMethod:                 Least Squares   F-statistic:                     2370.\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):          7.23e-104\nTime:                        11:49:52   Log-Likelihood:                -458.42\nNo. Observations:                 120   AIC:                             924.8\nDf Residuals:                     116   BIC:                             936.0\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept            -16.2454     54.243     -0.299      0.765    -123.681      91.190\nlength                 0.3842      1.138      0.338      0.736      -1.870       2.639\ntemperature           -0.6613      2.488     -0.266      0.791      -5.589       4.267\nlength:temperature     0.2755      0.052      5.313      0.000       0.173       0.378\n==============================================================================\nOmnibus:                        1.282   Durbin-Watson:                   2.077\nProb(Omnibus):                  0.527   Jarque-Bera (JB):                0.818\nSkew:                           0.072   Prob(JB):                        0.664\nKurtosis:                       3.378   Cond. No.                     5.58e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.58e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n:::\n\nNot bad. Not brilliant, but not terrible!\n\nOut of interest, let's also fit a model that we know is incorrect - one that doesn't include the interaction effect:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length + temperature, data = goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length + temperature, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.767  -9.325   0.651   9.986  37.804 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -301.4146    20.5996  -14.63   <2e-16 ***\nlength         6.3030     0.4131   15.26   <2e-16 ***\ntemperature   12.5066     0.2113   59.18   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.68 on 117 degrees of freedom\nMultiple R-squared:  0.9688,\tAdjusted R-squared:  0.9682 \nF-statistic:  1815 on 2 and 117 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length + temperature\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.980\nModel:                            OLS   Adj. R-squared:                  0.980\nMethod:                 Least Squares   F-statistic:                     2872.\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):          3.65e-100\nTime:                        11:49:52   Log-Likelihood:                -471.49\nNo. Observations:                 120   AIC:                             949.0\nDf Residuals:                     117   BIC:                             957.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept    -289.7312     19.003    -15.247      0.000    -327.365    -252.097\nlength          6.1167      0.403     15.169      0.000       5.318       6.915\ntemperature    12.5306      0.179     69.955      0.000      12.176      12.885\n==============================================================================\nOmnibus:                        0.723   Durbin-Watson:                   2.145\nProb(Omnibus):                  0.697   Jarque-Bera (JB):                0.326\nSkew:                          -0.007   Prob(JB):                        0.850\nKurtosis:                       3.255   Cond. No.                         876.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWithout the interaction term, our estimates are **wildly** wrong - or least, much more wrong than they were with the interaction. \n\nThis is a really nice illustration of how important it is to check for interactions when modelling data.\n\n## One categorical & one continuous predictor\n\nThe next type of interaction we'll look at is between one categorical and one continuous predictor. This is the type of interaction you'd see in a grouped linear regression.\n\nWe'll use our two predictors from the previous chapters, `length` and `pond`. \n\n#### Set up parameters and predictors\n\nSince at least one of the variables in our interaction is a categorical predictor, the beta coefficient for the interaction will need to be a vector.\n\nThink of it this way: our model with an interaction term will consist of three lines of best fit, each with a different intercept *and* gradient. The difference in intercepts is captured by `b2`, and then the difference in gradients is captured by `b3`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls()) # optional clean-up\n\nset.seed(20)\n\nn <- 60\nb0 <- 175                 # intercept\nb1 <- 2                   # main effect of length\nb2 <- c(0, 30, -10)       # main effect of pond\nb3 <- c(0, 0.5, -0.2)     # interaction of length:pond\n\nsdi <- 12\n\nlength <- rnorm(n, 48, 3)\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel(length, pond, goldentoad, clutchsize, avg_clutch) # optional clean-up\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(23)\n\nn = 60\nb0 = 175                          # intercept\nb1 = 2                            # main effect of length\nb2 = np.array([0, 30, -10])       # main effect of pond\nb3 = np.array([0, 0.5, -0.2])     # interaction of length:pond\n\nsdi = 12\n\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\n```\n:::\n\n\n\n:::\n\nSimulating the values for `length` and `pond` themselves is no different to how we did it in previous chapters.\n\n#### Simulate response variable\n\nOnce again, we use our two-step procedure to simulate our response variable.\n\nSince our interaction is categorical (i.e., contains a categorical predictor), we will need to create a design matrix for it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(~0+length:pond)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   length:pondA length:pondB length:pondC\n1      51.48806      0.00000      0.00000\n2      46.24223      0.00000      0.00000\n3      53.35640      0.00000      0.00000\n4      44.00222      0.00000      0.00000\n5      46.66030      0.00000      0.00000\n6      49.70882      0.00000      0.00000\n7      39.33085      0.00000      0.00000\n8      45.39294      0.00000      0.00000\n9      46.61489      0.00000      0.00000\n10     46.33338      0.00000      0.00000\n11     47.93959      0.00000      0.00000\n12     47.54885      0.00000      0.00000\n13     46.11562      0.00000      0.00000\n14     51.96966      0.00000      0.00000\n15     43.43595      0.00000      0.00000\n16     46.68772      0.00000      0.00000\n17     50.91173      0.00000      0.00000\n18     48.08467      0.00000      0.00000\n19     47.74265      0.00000      0.00000\n20     49.16764      0.00000      0.00000\n21      0.00000     48.71006      0.00000\n22      0.00000     47.56668      0.00000\n23      0.00000     50.16669      0.00000\n24      0.00000     49.10972      0.00000\n25      0.00000     47.27380      0.00000\n26      0.00000     43.58381      0.00000\n27      0.00000     46.21152      0.00000\n28      0.00000     44.55990      0.00000\n29      0.00000     40.57609      0.00000\n30      0.00000     46.15947      0.00000\n31      0.00000     47.35107      0.00000\n32      0.00000     52.77044      0.00000\n33      0.00000     52.66843      0.00000\n34      0.00000     51.32535      0.00000\n35      0.00000     44.70797      0.00000\n36      0.00000     42.41818      0.00000\n37      0.00000     45.25926      0.00000\n38      0.00000     51.73671      0.00000\n39      0.00000     48.26356      0.00000\n40      0.00000     49.27045      0.00000\n41      0.00000      0.00000     45.54455\n42      0.00000      0.00000     43.37230\n43      0.00000      0.00000     49.66765\n44      0.00000      0.00000     46.89291\n45      0.00000      0.00000     44.85799\n46      0.00000      0.00000     48.05454\n47      0.00000      0.00000     50.64563\n48      0.00000      0.00000     50.64558\n49      0.00000      0.00000     51.07873\n50      0.00000      0.00000     46.85607\n51      0.00000      0.00000     51.29831\n52      0.00000      0.00000     47.90725\n53      0.00000      0.00000     48.57102\n54      0.00000      0.00000     52.00562\n55      0.00000      0.00000     50.19166\n56      0.00000      0.00000     48.16861\n57      0.00000      0.00000     51.98792\n58      0.00000      0.00000     46.77564\n59      0.00000      0.00000     45.54523\n60      0.00000      0.00000     49.07684\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$pond\n[1] \"contr.treatment\"\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\n```\n:::\n\n\n\n:::\n\nYou'll notice that this design matrix doesn't contain 0s and 1s, like the design matrix for `pond` alone does.\n\nInstead, wherever there *would* be a 1, it has been replaced with the value of `length` for that row.\n\nThis means that when we multiply our design matrix by our `b3`, the following happens:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(~0+length:pond) %*% b3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n1    0.000000\n2    0.000000\n3    0.000000\n4    0.000000\n5    0.000000\n6    0.000000\n7    0.000000\n8    0.000000\n9    0.000000\n10   0.000000\n11   0.000000\n12   0.000000\n13   0.000000\n14   0.000000\n15   0.000000\n16   0.000000\n17   0.000000\n18   0.000000\n19   0.000000\n20   0.000000\n21  24.355031\n22  23.783340\n23  25.083345\n24  24.554860\n25  23.636901\n26  21.791905\n27  23.105761\n28  22.279950\n29  20.288045\n30  23.079737\n31  23.675533\n32  26.385219\n33  26.334215\n34  25.662676\n35  22.353987\n36  21.209091\n37  22.629632\n38  25.868353\n39  24.131782\n40  24.635223\n41  -9.108910\n42  -8.674459\n43  -9.933529\n44  -9.378583\n45  -8.971597\n46  -9.610908\n47 -10.129127\n48 -10.129117\n49 -10.215746\n50  -9.371214\n51 -10.259661\n52  -9.581450\n53  -9.714204\n54 -10.401124\n55 -10.038331\n56  -9.633721\n57 -10.397583\n58  -9.355128\n59  -9.109046\n60  -9.815367\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(Xpond_length, b3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([  0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n         0.        ,   0.        ,   0.        ,   0.        ,\n        23.69723922,  25.56805692,  24.80724295,  25.218178  ,\n        24.36165945,  22.5712357 ,  23.79559987,  25.90087231,\n        24.26045047,  22.16511784,  26.12297997,  24.68656647,\n        25.09331376,  26.9526521 ,  23.17831799,  22.98087259,\n        20.24065452,  24.22044074,  24.90929324,  23.96619166,\n        -9.60805335, -10.16156694,  -9.8523736 ,  -9.84697178,\n        -9.55720565,  -9.57273745, -10.22453158,  -9.54357916,\n        -9.34749363,  -9.26880686,  -9.52734147,  -9.71408482,\n        -9.90728244,  -9.67892308,  -9.40102973,  -8.62056823,\n        -9.97146844,  -9.00445573,  -9.50319217, -10.3154426 ])\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWe get no adjustments made for any of the measurements from pond `A`. This is what we wanted, because this pond is our reference group. The gradient between 'clutchsize ~ length' in pond `A` is therefore kept equal to our `b2` value.\n\nWe do, however, get adjustments for ponds `B` and `C`. These generate a different gradient between `clutchsize ~ length` for our two non-reference ponds.\n\nWe then add this in to our model equation, like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_clutch <- b0 + b1*length + model.matrix(~0+pond) %*% b2 + model.matrix(~0+length:pond) %*% b3\n\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ngoldentoad <- tibble(clutchsize, length, pond)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\n\navg_clutch = b0 + b1*length + np.dot(Xpond, b2) + np.dot(Xpond_length, b3)\n\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n#### Check the dataset\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/check cat-cont interaction dataset-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length*pond, goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length * pond, data = goldentoad)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.0456  -6.6815  -0.8263   7.7179  22.2570 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  232.7146    38.6258   6.025 1.56e-07 ***\nlength         0.7550     0.8125   0.929    0.357    \npondB         18.6008    53.4338   0.348    0.729    \npondC          1.2410    63.8124   0.019    0.985    \nlength:pondB   0.8565     1.1233   0.762    0.449    \nlength:pondC  -0.3744     1.3252  -0.283    0.779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.4 on 54 degrees of freedom\nMultiple R-squared:  0.9009,\tAdjusted R-squared:  0.8917 \nF-statistic: 98.18 on 5 and 54 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\nWe're going to use the `seaborn` package here, instead of just `matlibplot`, for efficiency.\n\n(If you'd prefer, however, you can just use `plotnine` and copy the R code from the other tab.)\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.clf()\nsns.lmplot(data=goldentoad, x=\"length\", y=\"clutchsize\", hue=\"pond\", height=5, aspect=1.3,\n           scatter_kws={\"s\": 40}, line_kws={\"linewidth\": 2}, ci=None)\n```\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/visualise cat-cont interaction-1.png){width=350}\n:::\n\n```{.python .cell-code}\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/visualise cat-cont interaction-2.png){width=681}\n:::\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/visualise cat-cont interaction-3.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length * pond\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.900\nModel:                            OLS   Adj. R-squared:                  0.891\nMethod:                 Least Squares   F-statistic:                     97.36\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           9.33e-26\nTime:                        11:49:55   Log-Likelihood:                -233.42\nNo. Observations:                  60   AIC:                             478.8\nDf Residuals:                      54   BIC:                             491.4\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept          199.3514     46.822      4.258      0.000     105.480     293.223\npond[T.B]         -105.3064     65.251     -1.614      0.112    -236.126      25.513\npond[T.C]          -47.2483     82.637     -0.572      0.570    -212.925     118.428\nlength               1.5531      0.983      1.580      0.120      -0.418       3.524\nlength:pond[T.B]     3.2955      1.357      2.428      0.019       0.574       6.017\nlength:pond[T.C]     0.4078      1.721      0.237      0.814      -3.043       3.859\n==============================================================================\nOmnibus:                        4.125   Durbin-Watson:                   2.550\nProb(Omnibus):                  0.127   Jarque-Bera (JB):                3.455\nSkew:                           0.582   Prob(JB):                        0.178\nKurtosis:                       3.166   Cond. No.                     3.23e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.23e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWe can see that we get three separate lines of best fit, with different gradients and intercepts.\n\nHow do these map onto our beta coefficients?\n\n-   The intercept and gradient for pond `A` are captured in `b0` and `b1`\n-   The differences between the intercept of pond `A` and the intercepts of ponds `B` and `C` are captured in `b2`\n-   The differences in gradients between pond `A` and the gradients of ponds `B` and `C` are captured in `b3`\n\nUltimately, there are still 6 unique numbers; but, because of the format of the equation of a linear model, they're split across 4 separate beta coefficients.\n\n## Two categorical predictors\n\nLast but not least: what happens if we have an interaction between two categorical predictors?\n\nWe'll use a binary predictor variable, presence of `predators` (yes/no), as our second categorical predictor alongside `pond`.\n\n#### Set up parameters and predictors\n\nBy now, most of this should look familiar. We construct `predators` just like we do `pond`, by repeating the elements of a list/vector.\n\n(To keep things simple, we'll drop our continuous `length` predictor for this example.)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls()) # optional clean-up\n\nset.seed(20)\n\nn <- 60\nb0 <- 165\nb1 <- c(0, 30, -10)   # pond (A, B, C)\nb2 <- c(0, 20)        # presence of predator (no, yes)\n\nsdi <- 20\n\nlength <- rnorm(n, 48, 3)\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n\npredators <- rep(c(\"yes\", \"no\"), times = n/2)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel(length, pond, goldentoad, clutchsize, avg_clutch) # optional clean-up\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(23)\n\nn = 60\nb0 = 165\nb1 = np.array([0, 30, -10])   # pond (A, B, C)\nb2 = np.array([0, 20])        # presence of predator (no, yes)\n\nsdi = 20\n\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\n\npredators = np.tile([\"yes\",\"no\"], reps = n//2)\n```\n:::\n\n\n\n:::\n\nYou'll notice that we haven't specified `b3` yet - the next section is dedicated to this, since it gets a bit abstract.\n\n#### The interaction coefficient\n\nWhat on earth does our `b3` coefficient need to look like?\n\nWell, we know it needs to be a vector. Any interaction that contains at least one categorical predictor, requires a vector beta coefficient.\n\nLet's look at the design matrix for the `pond:predators` interaction to help us figure that out.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(~0 + pond:predators)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  pondA:predatorsno pondB:predatorsno pondC:predatorsno pondA:predatorsyes\n1                 0                 0                 0                  1\n2                 1                 0                 0                  0\n3                 0                 0                 0                  1\n4                 1                 0                 0                  0\n5                 0                 0                 0                  1\n6                 1                 0                 0                  0\n  pondB:predatorsyes pondC:predatorsyes\n1                  0                  0\n2                  0                  0\n3                  0                  0\n4                  0                  0\n5                  0                  0\n6                  0                  0\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nXpond_pred = dmatrix('0 + C(pond):C(predators)', data = {'pond': pond, 'predators': predators})\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.]])\n```\n\n\n:::\n:::\n\n\n\n:::\n\nThe matrix is 60 rows long (here we're looking at just the top few rows) and has 6 columns.\n\nThose 6 columns represent our 6 possible subgroups, telling us that our `b4` coefficient will need to be 6 elements long. Some of these elements will be 0s, but how many?\n\n**The short answer is this:**\n\nThe first 4 elements will be 0s, because our `b0`, `b1` and `b2` coefficients already contain values for 4 of our subgroups. We then need two additional unique numbers for the remaining 2 subgroups.\n\n::: {.callout-tip collapse=\"true\"}\n#### For a longer answer:\n\nRemember that when fitting the model, our software will choose a group as a reference group.\n\nIn this example, `b0` is the mean of our reference group, which here is `pondA:predatorsno` (determined alphabetically).\n\nOur other beta coefficients then represent the differences between this reference group mean, and our other group means.\n\n- `b0`, the baseline mean of our reference group `pondA:predatorsno`\n- `b2`, containing two numbers; these capture the difference between the reference group and `pondB:predatorsno`/`pondC:predatorsno`\n- `b3`, containing one number; this captures the difference between the reference group and `pondA:predatorsyes`\n\nIf we didn't include a `b3` at all, we would get a dataset that looks like this:\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'pond'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/only 4 numbers needed-1.png){width=672}\n:::\n:::\n\n\n\n\nHere, the difference in group means between `pondA:predatorsno` and `pondA:predatorsyes` is the exact same difference as we see within ponds `B` and `C` as well. That's represented by the black lines, which are all identical in height.\n\nThis means that the only information we need to recreate the 6 group means is the 4 values from our `b0`, `b1` and `b2` coefficients.\n\nIf we include the interaction term, however, then that's no longer the case. Within each pond, there can be a completely unique difference between when predators were and weren't present:\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'pond'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/6 numbers needed-1.png){width=672}\n:::\n:::\n\n\n\n\nNow, we cannot use the same `b2` value (yes vs no predators) for each of our three ponds: we need unique differences.\n\nOr, to phrase it another way: each of our 5 non-reference subgroups will have a completely unique difference in means from our reference subgroup. \n\nThis means our simulation needs to provide 6 unique values across our beta coefficients. We already have the first 4, from `b0`, `b1` and `b2`, so we just need two more.\n:::\n\nThis, therefore, is what our `b3` looks like:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb3 <- c(0, 0, 0, 0, -20, 40)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nb3 = np.array([0, 0, 0, 0, -20, 40])\n```\n:::\n\n\n\n:::\n\nSince there are 6 subgroups, and the first 4 from our model design matrix are already dealt with, we only need two additional numbers. The other groups don't need to be adjusted further.\n\n#### Simulate response variable & check dataset\n\nFinally, we simulate our response variable, and then we can check how well our model does at recovering these parameters.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb3 <- c(0, 0, 0, 0, -20, 40)    # interaction pond:predators\n\navg_clutch <- b0 + model.matrix(~0+pond) %*% b1 + model.matrix(~0+predators) %*% b2 + model.matrix(~0+pond:predators) %*% b3\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ntoads <- tibble(clutchsize, length, pond, predators)\n\n# fit and summarise model\nlm_toads <- lm(clutchsize ~ length + pond * predators, toads)\nsummary(lm_toads)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length + pond * predators, data = toads)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.890 -11.348  -0.284   9.929  37.223 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        241.8247    37.8165   6.395 4.24e-08 ***\nlength              -1.8881     0.7868  -2.400   0.0200 *  \npondB               48.0543     8.0604   5.962 2.08e-07 ***\npondC                2.5931     8.0643   0.322   0.7491    \npredatorsyes        40.9971     8.0570   5.088 4.86e-06 ***\npondB:predatorsyes -37.6927    11.4020  -3.306   0.0017 ** \npondC:predatorsyes  23.7371    11.4269   2.077   0.0426 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.01 on 53 degrees of freedom\nMultiple R-squared:  0.693,\tAdjusted R-squared:  0.6583 \nF-statistic: 19.94 on 6 and 53 DF,  p-value: 4.978e-12\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nb3 = np.array([0, 0, 0, 0, -20, 40])    # interaction pond:predators\n\n# construct design matrices\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpred = dmatrix('0 + C(predators)', data = {'predators': predators})\nXpond_pred = dmatrix('0 + C(pond):C(predators)', data = {'pond': pond, 'predators': predators})\n\n# simulate response variable in two steps\navg_clutch = b0 + np.dot(Xpond, b1) + np.dot(Xpred, b2) + np.dot(Xpond_pred, b3)\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# collate dataset\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'clutchsize': clutchsize})\n\n# fit and summarise model\nmodel = smf.ols(formula= \"clutchsize ~ pond * predators\", data = goldentoad)\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.473\nModel:                            OLS   Adj. R-squared:                  0.425\nMethod:                 Least Squares   F-statistic:                     9.704\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           1.19e-06\nTime:                        11:50:00   Log-Likelihood:                -265.96\nNo. Observations:                  60   AIC:                             543.9\nDf Residuals:                      54   BIC:                             556.5\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================================\n                                 coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nIntercept                    174.9817      6.787     25.781      0.000     161.374     188.589\npond[T.B]                     30.7008      9.599      3.198      0.002      11.457      49.945\npond[T.C]                    -25.7779      9.599     -2.686      0.010     -45.022      -6.534\npredators[T.yes]              10.4013      9.599      1.084      0.283      -8.843      29.646\npond[T.B]:predators[T.yes]   -22.1565     13.575     -1.632      0.108     -49.372       5.059\npond[T.C]:predators[T.yes]    44.0281     13.575      3.243      0.002      16.813      71.244\n==============================================================================\nOmnibus:                        1.332   Durbin-Watson:                   2.384\nProb(Omnibus):                  0.514   Jarque-Bera (JB):                1.357\nSkew:                           0.325   Prob(JB):                        0.507\nKurtosis:                       2.652   Cond. No.                         9.77\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Three-way interactions\n\nThree-way interactions are rarer than two-way interactions, at least in practice, because they require a much bigger sample size to detect and are harder to interpret.\n\nHowever, they can occur, so let's (briefly) look at how you'd simulate them.\n\n#### length:temperature:pond\n\nThis three-way interaction involves two continuous (`length` and `temperature`) and one categorical (`pond`) predictors.\n\nIt will, by default, need a vector beta coefficient.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(22)\n\nn <- 120\nb0 <- -30               # intercept\nb1 <- 0.7               # main effect of length\nb2 <- c(0, 30, -10)     # main effect of pond\nb3 <- 0.5               # main effect of temperature\n\nb4 <- 0.25              # interaction of length:temperature\nb5 <- c(0, 0.2, -0.1)   # interaction of length:pond\nb6 <- c(0, 0.1, -0.25)  # interaction of temperature:pond\n\nb7 <- c(0, 0.05, -0.1)  # interaction of length:temp:pond\n\nsdi <- 6\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(28)\n\nn = 120\nb0 = -30                        # intercept\nb1 = 0.7                        # main effect of length\nb2 = np.array([0, 30, -10])     # main effect of pond\nb3 = 0.5                        # main effect of temperature\n\nb4 = 0.25                       # interaction of length:temperature\nb5 = np.array([0, 0.2, -0.1])   # interaction of length:pond\nb6 = np.array([0, 0.1, -0.25])  # interaction of temperature:pond\n\nb7 = np.array([0, 0.05, -0.1])  # interaction of length:temp:pond\n\nsdi = 6\n```\n:::\n\n\n\n:::\n\nThere are 12 unique/non-zero values across our 8 beta coefficients.\n\nOne helpful way to think about this: within each pond, we need 4 unique numbers/constants to describe the intercept, main effect of `length`, main effect of `temperature`, and two-way interaction between `length:temperature`. \n\nSince we're allowing a three-way interaction, each of the three ponds will (or at least, can) have a completely unique set of 4 values.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate predictor variables\nlength <- rnorm(n, 48, 3)\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\ntemperature <- runif(n, 10, 32)\n\n# generate response variable in two steps\navg_clutch <- b0 + b1*length + model.matrix(~0+pond) %*% b2 + b3*temperature +\n  b4*length*temperature +\n  model.matrix(~0+length:pond) %*% b5 +\n  model.matrix(~0+temperature:pond) %*% b6 +\n  model.matrix(~0+length:temperature:pond) %*% b7\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\n# collate the dataset\ngoldentoad <- tibble(length, pond, temperature, clutchsize)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# generate predictor variables\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\ntemperature = np.random.uniform(10, 32, n)\n\n# create design matrices\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\nXpond_temp = dmatrix('0 + C(pond):temp', data = {'pond': pond, 'temp': temperature})\nXpond_temp_length = dmatrix('0 + C(pond):length:temp', data = {'pond': pond, 'length': length, 'temp': temperature})\n\n# generate response variable in two steps\navg_clutch = (\n  b0 + b1*length + np.dot(Xpond, b2) + b3*temperature \n  + b4*length*temperature + np.dot(Xpond_length, b5) \n  + np.dot(Xpond_temp, b6) + np.dot(Xpond_temp_length, b7)\n  )\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# collate the dataset\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'temperature': temperature, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\nLet's check whether these data look sensible by visualising them, and how well a linear model does at recovering the parameters:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, colour = temperature, y = clutchsize)) +\n  facet_wrap(~ pond) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/threeway length-temp-pond test data-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length * pond * temperature, data = goldentoad)\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length * pond * temperature, data = goldentoad)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.6864  -4.4287  -0.0933   4.2699  17.4106 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              -33.44912   59.94099  -0.558 0.577976    \nlength                     0.64795    1.26547   0.512 0.609683    \npondB                     26.82304   94.49979   0.284 0.777075    \npondC                    -58.61716   84.66914  -0.692 0.490230    \ntemperature                0.63337    2.98100   0.212 0.832142    \nlength:pondB               0.31095    1.96551   0.158 0.874591    \nlength:pondC               1.15449    1.79169   0.644 0.520711    \nlength:temperature         0.25152    0.06296   3.995 0.000119 ***\npondB:temperature         -0.67053    4.73427  -0.142 0.887633    \npondC:temperature          0.84409    4.13926   0.204 0.838798    \nlength:pondB:temperature   0.06567    0.09875   0.665 0.507463    \nlength:pondC:temperature  -0.13405    0.08779  -1.527 0.129706    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.51 on 108 degrees of freedom\nMultiple R-squared:  0.9972,\tAdjusted R-squared:  0.9969 \nF-statistic:  3534 on 11 and 108 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\nWe'll use `seaborn` to create the faceted plot (though, as always, if you're used to `ggplot`/`plotnine`, you can toggle over to the R code and use that as a basis instead):\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.clf()\ng = sns.FacetGrid(goldentoad, col=\"pond\", hue=\"temperature\", height=4, aspect=1.2)\ng.map_dataframe(sns.scatterplot, x=\"length\", y=\"clutchsize\")\n```\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/three way test data length-temp-pond-1.png){width=714}\n:::\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/three way test data length-temp-pond-2.png){width=1382}\n:::\n:::\n\n\n\n\nAnd let's fit and summarise the model:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length * pond * temperature\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.998\nModel:                            OLS   Adj. R-squared:                  0.998\nMethod:                 Least Squares   F-statistic:                     5432.\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):          1.02e-142\nTime:                        11:50:07   Log-Likelihood:                -370.91\nNo. Observations:                 120   AIC:                             765.8\nDf Residuals:                     108   BIC:                             799.3\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n================================================================================================\n                                   coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                      -66.5326     48.314     -1.377      0.171    -162.299      29.233\npond[T.B]                      107.8100     68.883      1.565      0.120     -28.729     244.349\npond[T.C]                       48.7665     67.082      0.727      0.469     -84.202     181.735\nlength                           1.4876      1.014      1.467      0.145      -0.523       3.498\nlength:pond[T.B]                -1.5205      1.443     -1.054      0.294      -4.381       1.340\nlength:pond[T.C]                -1.3506      1.407     -0.960      0.339      -4.139       1.438\ntemperature                      0.6124      2.123      0.288      0.774      -3.596       4.821\npond[T.B]:temperature           -0.9611      3.030     -0.317      0.752      -6.966       5.044\npond[T.C]:temperature           -1.9374      3.148     -0.615      0.540      -8.177       4.302\nlength:temperature               0.2466      0.044      5.556      0.000       0.159       0.335\nlength:pond[T.B]:temperature     0.0769      0.063      1.216      0.227      -0.048       0.202\nlength:pond[T.C]:temperature    -0.0646      0.065     -0.987      0.326      -0.194       0.065\n==============================================================================\nOmnibus:                        2.940   Durbin-Watson:                   2.011\nProb(Omnibus):                  0.230   Jarque-Bera (JB):                2.966\nSkew:                           0.065   Prob(JB):                        0.227\nKurtosis:                       3.759   Cond. No.                     2.14e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.14e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07-interactions_files/figure-html/three way model data l-t-p-5.png){width=672}\n:::\n:::\n\n\n\n:::\n\nTry fitting a purely additive model. Better or worse?\n\n#### length:pond:predators\n\nNow, let's look at a three-way interaction that contains two categorical predictors.\n\nWithin each pond, there are 4 key numbers that we need: \n\n-   The intercept for the `predator` reference group\n-   The `length` gradient for the `predator` reference group\n-   The adjustment to the intercept for the `predator` non-reference group\n-   The adjustment the `length` gradient for the `predator` non-reference group\n\n(It can really help to draw this out on some scrap paper!)\n\nSo, across our 8 beta coefficients, we're going to need 12 numbers.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(22)\n\nn <- 120\nb0 <- -30                         # intercept\nb1 <- 0.7                         # main effect of length\nb2 <- c(0, 30, -10)               # main effect of pond (A, B, C)\nb3 <- c(0, 20)                    # main effect of predators (no, yes)\n\nb4 <- c(0, 0.2, -0.1)             # interaction of length:pond\nb5 <- c(0, 0.1)                   # interaction of length:predators\nb6 <- c(0, 0, 0, 0, 0.1, -0.25)   # interaction of pond:predators\n\nb7 <- c(0, 0, 0, 0, 0.1, -0.2)    # interaction of length:temp:pond\n\nsdi <- 6\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(28)\n\nn = 120\nb0 = -30                                  # intercept\nb1 = 0.7                                  # main effect of length\nb2 = np.array([0, 30, -10])               # main effect of pond (A, B, C)\nb3 = np.array([0, 20])                    # main effect of predators (no, yes)\n\nb4 = np.array([0, 0.2, -0.1])             # interaction of length:pond\nb5 = np.array([0, 0.1])                   # interaction of length:predators\nb6 = np.array([0, 0, 0, 0, 0.1, -0.25])   # interaction of pond:predators\n\nb7 = np.array([0, 0, 0, 0, 0.1, -0.2])    # interaction of length:temp:pond\n\nsdi = 6\n```\n:::\n\n\n\n:::\n\nIf you're curious how we were supposed to know to put all the leading zeroes in our `b6` and `b7` coefficients - the answer is, by looking ahead and checking the number of columns in the design matrix!\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate predictor variables\nlength <- rnorm(n, 48, 3)\npond <- rep(c(\"A\", \"B\", \"C\"), each = n/3)\npredators <- rep(c(\"yes\", \"no\"), times = n/2)\n\n# generate response variable\navg_clutch <- b0 + b1*length + model.matrix(~0+pond) %*% b2 + model.matrix(~0+predators) %*% b3 +\n  model.matrix(~0+length:pond) %*% b4 +\n  model.matrix(~0+length:predators) %*% b5 +\n  model.matrix(~0+pond:predators) %*% b6 +\n  model.matrix(~0+length:pond:predators) %*% b7\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\n# collate the dataset\ngoldentoad <- tibble(length, pond, predators, clutchsize)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# generate predictor variables\nlength = np.random.normal(48, 3, n)\npond = np.repeat([\"A\",\"B\",\"C\"], repeats = n//3)\npredators = np.tile([\"yes\",\"no\"], reps = n//2)\n\n# create design matrices\nXpond = dmatrix('0 + C(pond)', data = {'pond': pond})\nXpred = dmatrix('0 + C(pred)', data = {'pred': predators})\nXpond_length = dmatrix('0 + C(pond):length', data = {'pond': pond, 'length': length})\nXlength_pred = dmatrix('0 + length:C(pred)', data = {'pred': predators, 'length': length})\nXpond_pred = dmatrix('0 + C(pond):C(pred)', data = {'pond': pond, 'pred': predators})\nXpond_length_pred = dmatrix('0 + length:C(pond):C(pred)', data = {'pond': pond, 'length': length, 'pred': predators})\n\n# generate response variable in two steps\navg_clutch = (\n  b0 + b1*length + np.dot(Xpond, b2) + np.dot(Xpred, b3) \n  + np.dot(Xpond_length, b4) + np.dot(Xlength_pred, b5) \n  + np.dot(Xpond_pred, b6) + np.dot(Xpond_length_pred, b7)\n  )\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\n# collate the dataset\ngoldentoad = pd.DataFrame({'pond': pond, 'length': length, 'temperature': temperature, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n## Exercises\n\n### Continuing your own simulation {#sec-exr_another-dataset-again}\n\n::: {.callout-exercise}\n\n\n\n{{< level 3 >}}\n\n\n\n\n\n\nIn the previous chapters, exercises (@sec-exr_another-dataset, @sec-exr_another-dataset-again) encouraged you to simulate a dataset that by now should contain at least one categorical and one continuous predictor.\n\nContinue with that simulation, by adding at least one interaction. Adapt the code from the `goldentoad` example to achieve this.\n\nTo vary the difficulty of this exercise, start with a continuous:continuous interaction, then a categorical:continuous, and then continuous:continuous.\n\n(If you're super brave, try a three-way interaction!)\n\nAs always, remember - biological plausibility isn't important here. No one will check this dataset.\n:::\n\n\n## Summary\n\nOnce we know how to simulate main effects, the main additional challenge for simulating interactions is to think about what the beta coefficients should look like (especially when a categorical predictor is involved).\n\nVisualising the dataset repeatedly, and going back to tweak the parameters, is often necessary when trying to simulate an interaction.\n\n::: {.callout-tip}\n#### Key Points\n\n-   Interactions containing only continuous predictors, only require a single constant as their beta coefficient\n-   While any interaction term containing a categorical predictor, must also be treated as categorical when simulating\n-   It's very helpful to visualise the dataset to check your simulation is as expected\n:::\n",
    "supporting": [
      "07-interactions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}