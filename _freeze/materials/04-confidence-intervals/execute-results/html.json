{
  "hash": "c9047664465a990cbcc5ec0c0c962d1b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Confidence intervals\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nIn this chapter, we'll use simulation to answer the deceptively simple question: what are confidence intervals, and how should they be interpreted?\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nfrom scipy.stats import t\n```\n:::\n\n\n\n\n:::\n:::\n\nConfidence intervals, like p-values, are misunderstood surprisingly often in applied statistics. It's understandable that this happens, because a single set of confidence intervals by itself might not mean very much, unless you understand more broadly where they come from.\n\nSimulation is a fantastic way to gain this understanding.\n\n## Extracting confidence intervals for a single sample\n\nLet's start by showing how we can calculate confidence intervals from just one dataset.\n\nWe're still using one-dimensional data, so our confidence intervals in this case are for the mean.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n#### Method 1\n\nMethod 1 in R uses the `t.test` function. This is simpler in our current one-dimensional situation.\n\nThe confidence intervals are a standard part of the t-test, and we can use the `$` syntax to extract them specifically from the output:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\ndata <- rnorm(n, mean_n, sd_n)\n\nconf.int <- t.test(data, conf.level = 0.95)$conf.int\n\nprint(conf.int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.647455 6.678777\nattr(,\"conf.level\")\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n\n\n#### Method 2\n\nMethod 2 in R is via the `confint` function, which takes an `lm` object as its first argument.\n\nWe've had to do a bit of extra work to be able to use the `lm` function in this case (making our data into a dataframe), but in future sections of the course where we simulate multi-dimensional data, we'll have to do this step anyway - so this method may work out more efficient later on.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\ndata <- rnorm(n, mean_n, sd_n) %>%\n  data.frame()\n\nlm_data <- lm(. ~ 1, data)\n\nconfint(lm_data, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 5.647455 6.678777\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\nThere are a few options for extracting confidence intervals in Python, but perhaps the most efficient is via `statsmodels.stats.api`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(20)\n\nn = 40\nmean = 6\nsd = 1.5\n\ndata = np.random.normal(mean, sd, n)\n\nsms.DescrStatsW(data).tconfint_mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(5.241512557961966, 6.341197862942866)\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Extracting multiple sets of confidence intervals {#sec-exm_loop-confint}\n\nNow, let's use a for loop to extract and save multiple sets of confidence intervals.\n\nWe'll stick to the same parameters we used above:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\n# Soft-code the number of iterations\niterations <- 100\n\n# Soft-code the simulation parameters\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\n# Initialise a dataframe to store the results of the iterations\nintervals <- data.frame(mean = numeric(iterations),\n                        lower = numeric(iterations),\n                        upper = numeric(iterations))\n\n# Run simulations\nfor (i in 1:iterations) {\n  \n  data <- rnorm(n, mean_n, sd_n) %>%\n    data.frame()\n\n  lm_data <- lm(. ~ 1, data)\n  \n  # Extract mean and confidence intervals as simple numeric objects\n  est_mean <- unname(coefficients(lm_data)[1])\n  est_lower <- confint(lm_data, level = 0.95)[1]\n  est_upper <- confint(lm_data, level = 0.95)[2]\n  \n  # Update appropriate row of empty intervals object, with values from this loop\n  intervals[i,] <- data.frame(mean = est_mean, lower = est_lower, upper = est_upper)\n  \n}\n\nhead(intervals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mean    lower    upper\n1 6.163116 5.647455 6.678777\n2 6.112842 5.601488 6.624196\n3 5.771928 5.313615 6.230240\n4 5.926402 5.489371 6.363432\n5 5.916349 5.481679 6.351019\n6 6.180240 5.742182 6.618298\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(30)\n\niterations = 100\n\nn = 40\nmean = 6\nsd = 1.5\n\nrows = []\n\nfor i in range(iterations):\n  data = np.random.normal(mean, sd, n)\n  \n  estmean = statistics.mean(data)\n  lower = sms.DescrStatsW(data).tconfint_mean()[0]\n  upper = sms.DescrStatsW(data).tconfint_mean()[1]\n  \n  rows.append({'mean': estmean, 'lower': lower, 'upper': upper})\n\nintervals = pd.DataFrame(rows)\n\nintervals.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       mean     lower     upper\n0  5.787213  5.223520  6.350906\n1  5.760226  5.256582  6.263870\n2  6.176261  5.716103  6.636418\n3  6.214928  5.795727  6.634128\n4  5.988544  5.452712  6.524377\n```\n\n\n:::\n:::\n\n\n\n:::\n\nJust by looking at the first few sets of intervals, we can see - as expected - that our set of estimated means are varying around the true population value (in an approximately normal manner, according to the central limit theorem, as we now know).\n\nWe can also see that our confidence intervals are approximately following the mean estimate in each case. When the mean estimate is a bit high or a bit low relative to the true value, our confidence intervals are shifted up or down a bit, such that the estimated mean sits in the middle of the confidence intervals for each individual dataset.\n\nIn other words: each confidence interval is a property of its dataset.\n\nTo get a clearer picture, let's visualise them:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintervals %>%\n  ggplot(aes(x = 1:iterations)) +\n    geom_point(aes(y = mean)) +\n    geom_segment(aes(y = lower, yend = upper)) +\n    geom_hline(yintercept = mean_n, colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](04-confidence-intervals_files/figure-html/confint forest plot-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(iterations):\n    ax.plot([i + 1, i + 1], [intervals['lower'][i], intervals['upper'][i]], color='black')\n    ax.plot(i + 1, intervals['mean'][i], 'o', color='black')\n\nax.axhline(y=mean, color='red', linestyle='--', label='True Mean')\n\nplt.show()\n```\n\n::: {.cell-output-display}\n![](04-confidence-intervals_files/figure-html/forest plot of confidence intervals-1.png){width=768}\n:::\n:::\n\n\n\n\n\n:::\n\nFrom this plot, with the true population mean overlaid, we can see that most of the confidence intervals are managing to capture that true value. But a small proportion aren't.\n\nWhat proportion of the intervals are managing to capture the true population mean?\n\nWe can check like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(mean_n <= intervals$upper & mean_n >= intervals$lower)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncontains_true = (intervals['lower'] <= mean) & (intervals['upper'] >= mean)\ncontains_true.mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.95\n```\n\n\n:::\n:::\n\n\n\n:::\n\nGiven that we set our confidence intervals at the 95% level, this is exactly what the simulation should reveal: approximately (in this case, exactly) 95 of our 100 confidence intervals contain the true population mean.\n\n::: {.callout-warning}\n#### The confidence is about the intervals, not the parameter\n\nThe very definition of 95% confidence intervals is this: we expect that the confidence intervals from 95% of the samples drawn from a given population with a certain parameter, to contain that true population parameter.\n\nThis is *not* equivalent to saying that there is a 95% chance that the true population value falls inside a given interval. This is a common misconception, but there is no probability associated with the true population value - it just is what it is (even if we don't know it).\n\nAs with p-values, the probability is associated with datasets/samples when talking about confidence intervals, not with the underlying population.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n### Calculating confidence intervals manually\n\nFor those of you who are curious about the underlying mathematical formulae for confidence intervals, and how to calculate them manually, it's done like so:\n\n1.    Calculate the sample mean\n2.    Calculate the (estimated) standard error of the mean\n3.    Find the t-score* that corresponds to the confidence level (e.g., 95%)\n4.    Calculate the margin of error and construct the confidence interval\n\n*You can use z-scores, but t-scores tend to be more appropriate for small samples.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nLet's start by simulating a simple dataset.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\ndata <- rnorm(n, mean_n, sd_n)\n```\n:::\n\n\n\n\n#### Step 1: Calculate the sample mean\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_mean <- mean(data)\nprint(sample_mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.163116\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 2: Calculate the standard error of the mean\n\nWe do this by dividing the sample standard deviation by the square root of the sample size, $\\frac{s}{\\sqrt{N}}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_se <- sd(data)/sqrt(n)\nprint(sample_se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2549382\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 3: Calculate the t-score corresponding to the confidence level\n\nThis step also gives a clue as to how the significance threshold (or $\\alpha$) is associated with confidence level (they add together to equal 1).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\n\nsample_df <- n - 1\n\nt_score = qt(p = alpha/2, df = sample_df, lower.tail = FALSE)\nprint(t_score)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.022691\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 4: Calculate the margin of error and construct the confidence interval\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many standard deviations away from the mean, is the margin of error?\nmargin_error <- t_score * sample_se\n\n# Calculate upper & lower bounds around the mean\nlower_bound <- sample_mean - margin_error\nupper_bound <- sample_mean + margin_error\n\nprint(c(lower_bound,upper_bound))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.647455 6.678777\n```\n\n\n:::\n:::\n\n\n\n\nIf we compare that to what we would've gotten, if we'd used a function to do it for us:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  data.frame() %>%\n  lm(data = ., formula = . ~ 1, ) %>%\n  confint(level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 5.647455 6.678777\n```\n\n\n:::\n:::\n\n\n\n\n... we can indeed see that we get exactly the same values.\n\n## Python\n\nLet's start by simulating a simple dataset.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(20)\n\nn = 40\nmean = 6\nsd = 1.5\n\ndata = np.random.normal(mean, sd, n)\n```\n:::\n\n\n\n\n#### Step 1: Calculate the sample mean and standard deviation\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsample_mean = np.mean(data)\nsample_sd = np.std(data)\nprint(sample_mean, sample_sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5.791355210452416 1.69762282725244\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 2: Calculate the standard error of the mean\n\nWe do this by dividing the sample standard deviation by the square root of the sample size, $\\frac{s}{\\sqrt{N}}$.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsample_se = sample_sd/np.sqrt(n)\nprint(sample_se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.26841773710061373\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 3: Calculate the t-score corresponding to the confidence level\n\nThis step also gives a clue as to how the significance threshold (or $\\alpha$) is associated with confidence level (they add together to equal 1).\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import t\n\nalpha = 0.05\nsample_df = n-1\n\nt_crit = t.ppf(1-alpha/2, sample_df)\nprint(t_crit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2.022690911734728\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 4: Calculate the margin of error and construct the confidence interval\n\nFirst, we find the margin of error: how many standard deviations away from the mean is our cut-off?\n\nThen, we use that to find the upper and lower bounds, around the mean.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmargin_error = t_crit * sample_se\n\nci = (sample_mean - margin_error, sample_mean + margin_error)\n\nprint(ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(5.248429093070603, 6.334281327834229)\n```\n\n\n:::\n:::\n\n\n\n\nIf we compare that to what we would've gotten, if we'd used a function to do it for us:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsms.DescrStatsW(data).tconfint_mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(5.241512557961966, 6.341197862942866)\n```\n\n\n:::\n:::\n\n\n\n\n... we can indeed see that we get the same interval, plus or minus some tiny differences in numerical precision from the different functions used.\n:::\n\nIf you think about things in a maths-ier way, it can be helpful to know how something is calculated - but you will probably always use existing functions when actually coding this stuff!\n\n:::\n\n## Exercises\n\n### Width of confidence intervals {#sec-exr_confint}\n\nThere are multiple factors that will affect the width of confidence intervals.\n\nIn this exercise, you'll test some of them, to get an intuition of how (and hopefully why).\n\n::: {.callout-exercise}\n\n\n\n\n{{< level 1 >}}\n\n\n\n\n\n\nUse the code in @sec-exm_loop-confint as a starting point.\n\nVary the following parameters, and look at the impact on the width of the confidence intervals:\n\n-   The sample size of each individual sample\n-   The standard deviation of the underlying population\n-   The confidence level (e.g., 95%, 99%, 50%)\n\nThink about the following questions:\n\n-   Does the confidence interval get wider or narrower as these parameters increase? Why?\n-   What would happen (theoretically) if we set our desired confidence level to 100%, or our sample size to $n = \\infty$?\n\n::: {.callout-tip}\n#### Pay attention to the y-axis!\n\nIf you keep the same seed while changing other parameters, you might get a series of plots that look identical. \n\nBut if you look more closely at the y-axis, you will sometimes notice the scale changing.\n\nTo combat this, you can manually set the y-axis limits, if you'd like.\n:::\n\n:::\n\n## Summary\n\nConfidence intervals are commonly misunderstood, which is really easy to do when you're only thinking about one dataset.\n\nHowever, simulation allows us to look at a massive number of datasets that all come from the same underlying population, meaning we can look at multiple sets of confidence intervals - which puts the real interpretation of confidence intervals into context!\n\n::: {.callout-tip}\n#### Key Points\n\n-   If you construct 100 sets of 95% confidence intervals, you should expect ~95 of them to contain the true population parameter\n-   This is **not** the same as a 95% chance of the true population parameter being contained inside any individual set of confidence intervals\n-   The probability is associated with the intervals, not with the parameter!\n:::\n\n\n",
    "supporting": [
      "04-confidence-intervals_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}