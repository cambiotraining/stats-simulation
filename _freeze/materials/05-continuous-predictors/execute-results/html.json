{
  "hash": "bf07de31d82aeed5857a8187caafd340",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulating continuous predictors\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nIn this and the next couple of chapters, we're going to simulate a dataset about golden toads, an extinct species of amphibians. \n\nWe'll simulate various predictor variables, and their relationship to the clutch size (number of eggs) produced by a given toad.\n\nIn this chapter, we'll start by looking at continuous predictor variables.\n\n![Here's what the fancy little guys looked like! [image source](https://en.wikipedia.org/wiki/Golden_toad#/media/File:Bufo_periglenes2.jpg)](images/goldentoad.png){width=50%}\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\n\n# These packages will be used for evaluating the models we fit to our simulated data\nlibrary(performance)\nlibrary(ggResidpanel)\n\n# This package is optional/will only be used for later sections in this chapter\nlibrary(MASS)\n```\n:::\n\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom patsy import dmatrix\n```\n:::\n\n\n\n\n:::\n:::\n\n## Step 1: Set seed and sample size\n\nFirst, we set a seed and a sample size: \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n\n# sample size\nn <- 60\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(25)\n\n# sample size\nn = 60\n```\n:::\n\n\n\n\n:::\n\n## Step 2: Generate values of predictor variable\n\nThe next step is to generate our predictor variable.\n\nThere's no noise or uncertainty in our predictor (remember that residuals are always in the y direction, not the x direction), so we can just produce the values by sampling from a distribution of our choice.\n\nOne of the things that can cause variation in clutch size is the size of the toad herself, so we'll use that as our continuous predictor. This sort of biological variable would probably be normally distributed, so we'll use `rnorm` to generate it.\n\nGoogle tells us that the average female golden toad was somewhere in the region of 42-56mm long, so we'll use that as a sensible basis for our normal distribution for our predictor variable `length`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength <- rnorm(n, 48, 3)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlength = np.random.normal(48, 3, n)\n```\n:::\n\n\n\n:::\n\n## Step 3: Simulate average values of response variable\n\nNow, we need to simulate our response variable, `clutchsize`.\n\nWe're going to do this by setting up the linear model. We'll specify a y-intercept for `clutchsize`, plus a gradient that captures how much `clutchsize` changes as `length` changes.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- 175\nb1 <- 2\n\nsdi <- 20\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nb0 = 175\nb1 = 2\n\nsdi = 20\n```\n:::\n\n\n\n\n:::\n\nWe've also added an `sdi` parameter. This captures the standard deviation *around* the model predictions that is due to other factors we're not measuring. In other words, this will determine the size of our residuals.\n\nNow, we can simulate our set of predicted values for `clutchsize`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_clutch <- b0 + b1*length\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\navg_clutch = b0 + b1*length\n```\n:::\n\n\n\n:::\n\nYou'll notice we've just written out the equation of our model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(length, avg_clutch) %>%\n  ggplot(aes(x = length, y = avg_clutch)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](05-continuous-predictors_files/figure-html/visualise dataset using average response-1.png){width=672}\n:::\n:::\n\n\n\n\nWe use the `tibble` function to combine our response and predictor variables together into a single dataset.\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntempdata = pd.DataFrame({'length': length, 'avg_clutch': avg_clutch})\n\ntempdata.plot.scatter(x = 'length', y = 'avg_clutch')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](05-continuous-predictors_files/figure-html/visualise data with avg response-1.png){width=672}\n:::\n:::\n\n\n\n\nWe use the `pd.DataFrame` function to stitch our arrays together into a single dataframe object with multiple columns.\n\n:::\n\nWhen we visualise `length` and `avg_clutch` together, you see they perfectly form a straight line. That's because `avg_clutch` doesn't contain the residuals - that comes next.\n\n## Step 4: Simulate actual values of response variable\n\nThe final step is to simulate the actual values of clutch size. \n\nHere, we'll be drawing from a normal distribution. We put `avg_clutch` in as our mean - this is because the set of actual clutch size values should be normally distributed around our set of predictions.\n\nOr, in other words, we want the residuals/errors to be normally distributed.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclutchsize <- rnorm(n, avg_clutch, sdi)\n\ngoldentoad <- tibble(clutchsize, length)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclutchsize = np.random.normal(avg_clutch, sdi, n)\n\ngoldentoad = pd.DataFrame({'length': length, 'clutchsize': clutchsize})\n```\n:::\n\n\n\n:::\n\n## Step 5: Checking the dataset\n\nLet's make sure our dataset is behaving the way we intended.\n\nFirst, we'll visualise it:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(goldentoad, aes(x = length, y = clutchsize)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](05-continuous-predictors_files/figure-html/visualise final dataset-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngoldentoad.plot.scatter(x = 'length', y = 'clutchsize')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](05-continuous-predictors_files/figure-html/visualise the data-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nAnd then, we'll construct a linear model - and check that our beta coefficients have been replicated to a sensible level of precision!\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_golden <- lm(clutchsize ~ length, goldentoad)\n\nsummary(lm_golden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = clutchsize ~ length, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.718 -10.973   1.094  11.941  41.690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 267.1395    38.4922   6.940  3.7e-09 ***\nlength        0.1065     0.8038   0.132    0.895    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.76 on 58 degrees of freedom\nMultiple R-squared:  0.0003025,\tAdjusted R-squared:  -0.01693 \nF-statistic: 0.01755 on 1 and 58 DF,  p-value: 0.8951\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.ols(formula= \"clutchsize ~ length\", data = goldentoad)\n\nlm_golden = model.fit()\nprint(lm_golden.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             clutchsize   R-squared:                       0.184\nModel:                            OLS   Adj. R-squared:                  0.170\nMethod:                 Least Squares   F-statistic:                     13.07\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):           0.000630\nTime:                        11:43:20   Log-Likelihood:                -256.33\nNo. Observations:                  60   AIC:                             516.7\nDf Residuals:                      58   BIC:                             520.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    157.5257     30.614      5.146      0.000      96.245     218.806\nlength         2.2928      0.634      3.615      0.001       1.023       3.562\n==============================================================================\nOmnibus:                        0.516   Durbin-Watson:                   2.307\nProb(Omnibus):                  0.773   Jarque-Bera (JB):                0.492\nSkew:                          -0.207   Prob(JB):                        0.782\nKurtosis:                       2.839   Cond. No.                         649.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n:::\n\nNot bad at all. The linear model has managed to extract beta coefficients similar to the original `b0` and `b1` that we set.\n\nIf you're looking to explore and understand this further, try exploring the following things in your simulation, and see how they affect the p-value and the precision of the beta estimates:\n\n-   Varying the sample size\n-   Varying the `sdi`\n-   Varying the `b1` parameter\n\n\n## Exercises\n\n### A second continuous predictor {#sec-exr_second-cont-pred}\n\n::: {.callout-exercise}\n\n\n\n{{< level 1 >}}\n\n\n\n\n\n\nCreate at least one additional continuous predictor for the dataset (perhaps `temperature` or `age` of toad).\n\nFollow the same procedure described above:\n\n-   Set seed & sample size\n-   Set beta coefficients\n-   Use model equation to simulate average/expected values of `clutchsize`\n-   Simulate actual values of `clutchsize`, with random noise\n-   Check the dataset\n\nDon't worry about being biologically plausible - just pick some numbers. We're focusing on the simulation here, not on perfect realism in the dataset!\n:::\n\n### A brand new dataset {#sec-exr_another-dataset}\n\n::: {.callout-exercise}\n\n\n\n{{< level 2 >}}\n\n\n\n\n\n\nTo practice this process, set up another dataset of your own to simulate, containing at least one categorical predictor and a continuous response.\n\nYou'll need to:\n\n-   Set the seed and sample size\n-   Set values of `b0` and `b1`\n-   Construct the predictor variable by sampling from some distribution\n-   Simulate the average and then actual values of the response\n-   Construct a scatterplot and run a regression, to check your simulation ran as expected\n\nAdapt the code from the `goldentoad` example above to achieve this.\n\nFeel free to pick your own example, but if you're looking for inspiration, here are a couple of ideas.\n\n(Remember - biological plausibility doesn't actually matter, so don't get too hung up on it!)\n\n::: {.callout-tip collapse=\"true\"}\n#### Invasive plants\n\nContinuous response variable: **Area invaded per year** (m^2^/year)\n\nThe typical range will be between ~ 10-1000 m^2^/year.\n\nContinuous predictor: **Annual rainfall** (mm)\n\nA sensible range would be 400-1600mm. Many invasive plants will thrive in wetter environments.\n\nContinuous predictor: **Soil pH**\n\nA sensible range would be 4.5-8.5.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n#### Gut microbiome & blood glucose\n\nContinuous response variable: **Blood glucose level** (mg/dL)\n\nSomewhere in the range 70-120 mg/dL would be typical (the upper end would be considered pre-diabetic).\n\nContinuous predictor: **Fibre intake** (g/day)\n\nWould usually be in the region of 5-50 g/day. Higher fibre would be associated with lower glucose, usually.\n\nContinuous predictor: **Abundance of bacteroides** (%)\n\nThe usual range would be 0-30%; higher levels sometimes correlate with better metabolism.\n:::\n\n:::\n\n## Summary\n\nIn this chapter, we've simulated two-dimensional data for the first time.\n\nFirst, we construct a continuous variable to act as a predictor. Then, we can simulate our response variable as a function of the predictor variable(s) via a linear model equation, with residuals added.\n\nBy definition, the assumptions of the linear model will be always be met, because we are in control of the nature of the underlying population. \n\nHowever, our model may or may not do a good job of \"recovering\" the original beta coefficients we specified, depending on the sample size and the amount of error we introduce in our simulation.\n\n::: {.callout-tip}\n#### Key Points\n\n-   Predictor variables can be simulated from different distributions, with no errors associated\n-   Response variables should be simulated with errors/residuals from the normal distribution\n-   To do this, we need to specify the equation of the straight line, i.e., the intercept and slope beta coefficients, as parameters in the simulation\n:::\n",
    "supporting": [
      "05-continuous-predictors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}