{
  "hash": "405f17998f89fb3241c2b0bd72b5b120",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Revisiting statistical concepts\"\noutput: html_document\n---\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nNow that you have the programming basics, it can help to keep using the syntax and functions to solve new problems, to get a feel for them.\n\nTo achieve this, we'll revisit a handful of statistical concepts that are much easier to understand once you can use simulation to visualise/explore them. \n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nfrom scipy.stats import t\n```\n:::\n\n\n\n\n:::\n:::\n\n## Estimating parameters from datasets\n\nThere are two main forms of statistical inference, from dataset to population.\n\n1. Estimating parameters\n2. Testing hypotheses\n\nWe'll get onto the second form later on, but for this section, we're going to focus on estimating parameters. \n\nIn other words - can we do a good job of recreating the distribution of the underlying population, just from the sample we've taken from that population?\n\nOur ability to do this well often hinges on the quality of our sample. This includes both the **size** of the sample, and whether it is **biased** in any way.\n\nIf our sample is biased, there's not much we can do about it apart from a) scrapping it and starting again, or b) acknowledging the uncertainty/narrowed focus of our conclusions.\n\nWe do, however, often have some control over the sample size. Let's look at how sample size affects our parameter estimates.\n\n### Mean\n\nWhen simulating data, we have an unusual level of knowledge and power (in the normal sense of the word, not the statistical one!): \n\nWe know exactly what the true population parameters are, because we specified them.\n\nIn the code below, we know that the actual true population mean is 4, with no uncertainty. We have set this to be the \"ground truth\".\n\nThis code is similar to the for loop introduced in @sec-exm_mean-for-loop, but we've increased the sample size.\n\nNotice how this:\n\n-   Decreases variance (our histograms are \"squished\" inward - pay attention to the x axis scale)\n-   Increases the consistency of our mean estimates (they are more similar to one another)\n-   Increases the accuracy of our mean estimates (they are, overall, closer to the true value of 4)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:3) {\n  \n  n <- 200\n  mean_n <- 4\n  sd_n <- 0.5\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  hist(data, xlim = c(1, 7)); abline(v = mean(data), col = \"red\", lwd = 3)\n\n  }\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/less simple for loop, more iterations-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/less simple for loop, more iterations-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/less simple for loop, more iterations-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfor i in range(1, 4):\n  n = 200\n  mean = 4\n  sd = 0.5\n  data = np.random.normal(mean, sd, n)\n  plt.figure()\n  plt.hist(data)\n  plt.axvline(x = statistics.mean(data), color = 'r')\n  plt.show()\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/for loop means bigger sample size-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/for loop means bigger sample size-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/for loop means bigger sample size-3.png){width=672}\n:::\n:::\n\n\n\n:::\n\nIn a larger sample size, we are able to do a better job of \"recovering\" or recreating the original parameter we specified.\n\n\n::: {.callout-tip}\n#### The average of averages\n\nWhen we take multiple samples and average across them, we do an *even better* job of recovering the true population mean. \n\nIn other words, if we sampled a bunch of datasets, and then took the average of all of their respective means, we'd probably get pretty close to 4.\n\nThe section later on in this chapter, on the central limit theorem, pushes this idea a little further.\n:::\n\n### Variance\n\nIn the code above, we also know what the true population variance is, because we set it (by setting standard deviation, the square root of variance).\n\n::: {.callout-tip}\n#### What do we mean by \"variance\"?\n\nVariance is calculated by measuring the difference from each data point and the mean, squaring them, adding those squares up, and then dividing by the number of data points. The mathematical formula looks like this:\n\n$$\n\\text{Var}(X) = \\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 } {n}\n$$\n\nHowever, when we estimate population variance from a sample, we actually tweak this formula a bit. We divide instead by $n - 1$:\n\n$$\n\\text{Var}(X) = \\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2 } {n - 1}\n$$\n\n**Why?**\n\nWell, I could give you a theoretical explanation. Or, we can use the power of simulation, and see for ourselves more intuitively.\n\nFor the next section, the word variance will crop up in different forms:\n\n-   Estimated sample variance, calculated by dividing by $n - 1$, from a dataset\n-   Estimated variance, calculated by dividing by $n$, from a dataset\n-   The true population variance, which isn't calculated; it's known/specified by us\n\n:::\n\nIn the for loop below, we are using the same simulation parameters as above, and calculating (manually) two types of variance. \n\nWe'll set the mean to 4 and the variance to 1:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- data.frame(var=numeric(),\n                      sample_var=numeric())\n\nfor (i in 1:30) {\n  \n  n <- 10\n  mean_n <- 4\n  sd_n <- 1\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  v <- sum((data - mean(data))^2)/n\n  samplev <- sum((data - mean(data))^2)/(n-1)\n\n  results <- rbind(results, data.frame(var = v, sample_var = samplev))\n  \n}\n```\n:::\n\n\n\n\nTo break down this code further:\n\nWe've initialised a `results` table with our desired columns first. On each iteration of the loop (30 total), we sample a dataset, calculate the estimated variance `estv` and sample variance `samplev` using slightly different formulae, and then add them to our `results` table.\n\n## Python\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrows = []\n\nfor i in range(30):\n    n = 10\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    v = np.sum((data - np.mean(data))**2) / n\n    samplev = np.sum((data - np.mean(data))**2) / (n - 1)\n    \n    rows.append({'var': v, 'sample_var': samplev})\n\nresults = pd.DataFrame(rows)\n```\n:::\n\n\n\n\nTo break down this code further:\n\nOn each iteration of the loop (30 total), we sample a dataset, calculate the estimated variance `estv` and sample variance `samplev` using slightly different formulae. These values are collated by our `rows` object, which we finally convert to a pandas dataframe (a `results` table).\n:::\n\nNow, let's look at those results and what they show us.\n\nFirst, we'll create a new column in our `results` object that contains the difference between our estimates in each case.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- results %>%\n  mutate(diff_var = sample_var - var)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresults['diff_var'] = results['sample_var'] - results['var']\n```\n:::\n\n\n\n:::\n\nWhen we look at the final results file, we see that the estimated sample variance is, on average, a bit bigger than the estimated variance. This makes sense, because we're dividing by a smaller number when we calculate the sample variance (in other words, $n - 1 < n$).\n\nNow, let's look at the average variance and sample variance, and see which of them is doing the best job of recreating our true population variance (which we know is 1, because we built the simulation that way).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults %>%\n  summarise(mean(var), mean(sample_var), mean(diff_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean(var) mean(sample_var) mean(diff_var)\n1 0.9183643         1.020405      0.1020405\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsummary = results[['var', 'sample_var', 'diff_var']].mean()\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvar           0.918773\nsample_var    1.020859\ndiff_var      0.102086\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n:::\n\nHere, we've taken the mean of each column. This means we're looking at the average estimated variance and the average estimated sample variance, across all 10 of our random datasets.\n\n::: {.callout-tip}\n#### Remember, each of these numbers is an average\n\nYes, now we're taking the mean of the variance. Yes, we could also measure the variance of the variance if we wanted. Yes, this does start to get confusing the more you think about it. \n\nMore on this in the central limit theorem section!\n:::\n\nAs we can see from these results, when the sample size is small, our estimated variance (on average) *underestimates* the true value.\n\nThis is because, the smaller the sample, the less likely it is to contain values from the edges or tails of the normal distribution, so we don't get a good picture of the true spread.\n\nThe sample variance accounts for this by dividing by `n - 1` instead, so the estimate is larger and less of an underestimate. \n\nTo get an intuition for this, let's repeat all the code above, but with a larger sample size:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- data.frame(var=numeric(),\n                      sample_var=numeric())\n\nfor (i in 1:30) {\n  \n  n <- 20\n  mean_n <- 4\n  sd_n <- 1\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  v <- sum((data - mean(data))^2)/n\n  sample <- sum((data - mean(data))^2)/(n-1)\n\n  results <- rbind(results, data.frame(var = v, sample_var = sample))\n  \n}\n\nresults <- results %>%\n  mutate(diff_var = sample_var - var)\n\nresults %>%\n  summarise(mean(var), mean(sample_var), mean(diff_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean(var) mean(sample_var) mean(diff_var)\n1 0.9507581         1.000798      0.0500399\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrows = []\n\nfor i in range(30):\n    n = 20\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    v = np.sum((data - np.mean(data))**2) / n\n    samplev = np.sum((data - np.mean(data))**2) / (n - 1)\n    \n    rows.append({'var': v, 'sample_var': samplev})\n\nresults = pd.DataFrame(rows)\n\nresults['diff_var'] = results['sample_var'] - results['var']\n\nsummary = results[['var', 'sample_var', 'diff_var']].mean()\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvar           0.953684\nsample_var    1.003878\ndiff_var      0.050194\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWhen `n` is larger, notice how the estimated variance is now closer to the true population variance, on average?\n\nWith a larger sample size, we are more likely to sample the full \"spread\" of the distribution.\n\nThe estimated sample variance, however, is about as close to the true population variance as it was before, and so the difference between the estimated variance and estimated sample variance has shrunk.\n\n::: {.callout-tip collapse=\"true\"}\n#### Why sample variance is even cleverer than you might think\n\nDividing by `n - 1` has a bigger impact when the sample is small, where `1` will be a relatively larger fraction of `n`.\n\nThis is great, because these smaller samples are also the place where we need this adjustment most: they're less likely to contain values from the tails of the distribution, and therefore will underestimate the true population variance more.\n\nIn contrast, when our sample is much bigger, it's going to be more representative/less noisy, and we see much less of an underestimation and will need less adjustment. Happily, we will automatically get less of an adjustment anyway, since the `1` is now a smaller fraction of `n`. \n\nIn other words: the impact of dividing by `n - 1` scales naturally with both the size of `n`, and with the amount of underestimation we need to account for.\n\nIn fact, when the sample is infinitely large, we should see no difference between the estimated sample variance and the estimated variance at all, because `n-1 = n` at infinity.\n\nYou can **test this intuition** (except for the infinity part, you kinda just have to trust me on that) by continuing to mess around with the value of `n` in the code above.\n:::\n\nSince sample variance is the most effective way to estimate the true population variance, functions in R and Python will default to this.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe `var` function in R specifically calculates the sample variance. We can see that we get identical results using the function or doing it manually, by adapting the loop above:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- data.frame(var=numeric(),\n                      sample_var=numeric(),\n                      r_var=numeric())\n\nfor (i in 1:30) {\n  \n  n <- 20\n  mean_n <- 4\n  sd_n <- 1\n  \n  data <- rnorm(n, mean_n, sd_n) \n  \n  v <- sum((data - mean(data))^2)/n\n  sample <- sum((data - mean(data))^2)/(n-1)\n  \n  # Add an extra column containing the results of var(data)\n  results <- rbind(results, data.frame(var = v, sample_var = sample, r_var = var(data)))\n  \n}\n\nresults %>%\n  summarise(mean(var), mean(sample_var), mean(r_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean(var) mean(sample_var) mean(r_var)\n1 0.9507581         1.000798    1.000798\n```\n\n\n:::\n:::\n\n\n\n\nNotice how `mean(sample_var)` and `mean(r_var)` are identical?\n\n## Python\n\nThe `numpy.var` function specifically calculates sample variance. We can see that we get identical results using the function or doing it manually, by adapting the loop above:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrows = []\n\nfor i in range(30):\n    n = 20\n    mean = 4\n    sd = 1\n\n    data = np.random.normal(mean, sd, n)\n\n    # Estimated variance\n    v = np.sum((data - np.mean(data))**2) / n\n    # Sample variance\n    sample = np.sum((data - np.mean(data))**2) / (n - 1)\n    # numpy sample variance\n    np_var = np.var(data, ddof=1)\n\n    rows.append({'var': v, 'sample_var': sample, 'np_var': np_var})\n\nresults = pd.DataFrame(rows)\n\nsummary = results[['var', 'sample_var', 'np_var']].mean()\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvar           0.953684\nsample_var    1.003878\nnp_var        1.003878\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\nNotice how `sample_var` and `np_var` are identical?\n:::\n\n## Central limit theorem\n\nIn the section above, we used for loops to simulate multiple datasets, measure certain statistics from them, and then averaged those statistics across the datasets.\n\nSo, in some cases, we were looking at the mean of the means, or the mean of the variances. We're going to unpack that a bit further now.\n\nSpecifically, we're going to talk about the central limit theorem: the idea that, across multiple samples taken from the same distribution, the estimates/statistics we calculate from them will themselves follow a normal distribution.\n\n### An example: the mean {#sec-exm_mean-CLT}\n\nYou will recognise all the code below from previous sections, but here we're using it to show us a slightly different distribution.\n\nInstead of producing separate histograms for each of the datasets (i.e., one per loop), we are instead simply collecting the mean value from each of our datasets.\n\nThen, we will treat the set of means as a sample in itself, and visualise its distribution.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{#lst-central-limit-template-code-r .r .cell-code}\nmeans <- c()\n\nfor (i in 1:40) {\n  \n  n <- 200\n  mean_n <- 4\n  sd_n <- 1\n  \n  means[i] <- mean(rnorm(n, mean_n, sd_n))\n\n}\n\nhist(means)\nabline(v = mean(means), col = \"red\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/central limit means demo-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{#lst-central-limit-template-code-py .python .cell-code}\nmeans = []\n\nfor i in range(40):\n  n = 200\n  mean = 4\n  sd = 1\n  \n  est_mean = np.random.normal(mean, sd, n).mean()\n\n  means.append(est_mean)\n\nplt.clf()\nplt.hist(means)\nplt.axvline(x = statistics.mean(means), color = 'r')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/CLT means demo-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\nThe set of means from our `i` datasets, follow a normal distribution. The mean of this normal distribution is approximately the true population mean (which we know to be 4).\n\nIf we increase the number of iterations/loops, we will sample more datasets, with more means.\n\nIf we think of our set of sample means as a sample in itself, then doing this is effectively increasing our sample size. And, as we know from the first section of this chapter, that means that the mean of our distribution should be a better estimate of the true population value.\n\nThis is exactly what happens:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/central limit means higher i-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/CLT means higher i-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nTo produce the plot above, the number of iterations was set to `i = 1000`.\n\nTry setting it to something between 40 and 1000, or even more than 1000, and see how that changes things.\n\n::: {.callout-tip collapse=\"true\"}\n#### Standard error of the mean\n\nYou may have come across the concept of the standard error of the mean in the past (especially when constructing error bars for plots). But now, you should be in a better position to really understand what it is.\n\nIn the histogram above, we've calculated the mean of the sample means. But around that mean of sample means, there is some spread or noise.\n\nWe can quantify that spread by measuring the standard deviation of the distribution of sample means - and if we do, we've calculated the standard error.\n\nOf course, in classical statistics we usually only have one dataset, rather than 1000, to help us figure out that standard error. So, like with everything else we calculate from a dataset, we are only ever able to access an estimate of that standard error.\n:::\n\n### It's always normal\n\nThe really quirky thing about the central limit theorem is that it doesn't actually matter what distribution you pulled the original samples from. In other words, the results we got above aren't just because we were using the normal distribution for our simulations.\n\nTo prove that, the code here has been adapted to pull each of our 1000 samples from a uniform distribution instead, and estimate the mean.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nNote the use of `runif` instead of `rnorm`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans <- c()\n\nfor (i in 1:1000) {\n  \n  n <- 200\n  min_n <- 1\n  max_n <- 7\n  \n  means[i] <- mean(runif(n, min_n, max_n))\n\n}\n\nhist(means)\nabline(v = mean(means), col = \"red\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/central limit means demo unif dist-3.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\nNote the use of `np.random.uniform` instead of `np.random.normal`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmeans = []\n\nfor i in range(1000):\n  n = 200\n  lower = 1\n  upper = 7\n  est_mean = np.random.uniform(mean, sd, n).mean()\n  means.append(est_mean)\n\nplt.clf()\nplt.hist(means)\nplt.axvline(x = statistics.mean(means), color = 'r')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/CLT means unif dist-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nAlthough each of the individual samples would have a flat histogram, that's not what we're plotting here. Here, we're looking at the set of means that summarise each of those individual samples.\n\nThe nature of the underlying population distribution **doesn't matter** - the distribution of the parameter estimates is still normal, which we can see clearly with a sufficient number of simulations.\n\nNo wonder the normal distribution enjoys such special status in statistics.\n\n## Confidence intervals\n\nIn this section, we'll try to answer the deceptively simple question: what are confidence intervals, and how should they be interpreted?\n\nConfidence intervals, like p-values, are misunderstood surprisingly often in applied statistics. It's understandable that this happens, because a single set of confidence intervals by itself might not mean very much, unless you understand more broadly where they come from.\n\nSimulation is a fantastic way to gain this understanding.\n\n### Extracting confidence intervals for a single sample\n\nLet's start by showing how we can calculate confidence intervals from just one dataset.\n\nWe're still using one-dimensional data, so our confidence intervals in this case are for the mean.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n#### Method 1\n\nMethod 1 in R uses the `t.test` function. This is simpler in our current one-dimensional situation.\n\nThe confidence intervals are a standard part of the t-test, and we can use the `$` syntax to extract them specifically from the output:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\ndata <- rnorm(n, mean_n, sd_n)\n\nconf.int <- t.test(data, conf.level = 0.95)$conf.int\n\nprint(conf.int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.647455 6.678777\nattr(,\"conf.level\")\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n\n\n#### Method 2\n\nMethod 2 in R is via the `confint` function, which takes an `lm` object as its first argument.\n\nWe've had to do a bit of extra work to be able to use the `lm` function in this case (making our data into a dataframe), but in future sections of the course where we simulate multi-dimensional data, we'll have to do this step anyway - so this method may work out more efficient later on.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\ndata <- rnorm(n, mean_n, sd_n) %>%\n  data.frame()\n\nlm_data <- lm(. ~ 1, data)\n\nconfint(lm_data, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 5.647455 6.678777\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\nThere are a few options for extracting confidence intervals in Python, but perhaps the most efficient is via `statsmodels.stats.api`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(20)\n\nn = 40\nmean = 6\nsd = 1.5\n\ndata = np.random.normal(mean, sd, n)\n\nsms.DescrStatsW(data).tconfint_mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(5.241512557961966, 6.341197862942866)\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Extracting multiple sets of confidence intervals {#sec-exm_loop-confint}\n\nNow, let's use a for loop to extract and save multiple sets of confidence intervals.\n\nWe'll stick to the same parameters we used above:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\n# Soft-code the number of iterations\niterations <- 100\n\n# Soft-code the simulation parameters\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\n# Initialise a dataframe to store the results of the iterations\nintervals <- data.frame(mean = numeric(iterations),\n                        lower = numeric(iterations),\n                        upper = numeric(iterations))\n\n# Run simulations\nfor (i in 1:iterations) {\n  \n  data <- rnorm(n, mean_n, sd_n) %>%\n    data.frame()\n\n  lm_data <- lm(. ~ 1, data)\n  \n  # Extract mean and confidence intervals as simple numeric objects\n  est_mean <- unname(coefficients(lm_data)[1])\n  est_lower <- confint(lm_data, level = 0.95)[1]\n  est_upper <- confint(lm_data, level = 0.95)[2]\n  \n  # Update appropriate row of empty intervals object, with values from this loop\n  intervals[i,] <- data.frame(mean = est_mean, lower = est_lower, upper = est_upper)\n  \n}\n\nhead(intervals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mean    lower    upper\n1 6.163116 5.647455 6.678777\n2 6.112842 5.601488 6.624196\n3 5.771928 5.313615 6.230240\n4 5.926402 5.489371 6.363432\n5 5.916349 5.481679 6.351019\n6 6.180240 5.742182 6.618298\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(30)\n\niterations = 100\n\nn = 40\nmean = 6\nsd = 1.5\n\nrows = []\n\nfor i in range(iterations):\n  data = np.random.normal(mean, sd, n)\n  \n  estmean = statistics.mean(data)\n  lower = sms.DescrStatsW(data).tconfint_mean()[0]\n  upper = sms.DescrStatsW(data).tconfint_mean()[1]\n  \n  rows.append({'mean': estmean, 'lower': lower, 'upper': upper})\n\nintervals = pd.DataFrame(rows)\n\nintervals.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       mean     lower     upper\n0  5.787213  5.223520  6.350906\n1  5.760226  5.256582  6.263870\n2  6.176261  5.716103  6.636418\n3  6.214928  5.795727  6.634128\n4  5.988544  5.452712  6.524377\n```\n\n\n:::\n:::\n\n\n\n:::\n\nJust by looking at the first few sets of intervals, we can see - as expected - that our set of estimated means are varying around the true population value (in an approximately normal manner, according to the central limit theorem, as we now know).\n\nWe can also see that our confidence intervals are approximately following the mean estimate in each case. When the mean estimate is a bit high or a bit low relative to the true value, our confidence intervals are shifted up or down a bit, such that the estimated mean sits in the middle of the confidence intervals for each individual dataset.\n\nIn other words: each confidence interval is a property of its dataset.\n\nTo get a clearer picture, let's visualise them:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintervals %>%\n  ggplot(aes(x = 1:iterations)) +\n    geom_point(aes(y = mean)) +\n    geom_segment(aes(y = lower, yend = upper)) +\n    geom_hline(yintercept = mean_n, colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/confint forest plot-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(iterations):\n    ax.plot([i + 1, i + 1], [intervals['lower'][i], intervals['upper'][i]], color='black')\n    ax.plot(i + 1, intervals['mean'][i], 'o', color='black')\n\nax.axhline(y=mean, color='red', linestyle='--', label='True Mean')\n\nplt.show()\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/forest plot of confidence intervals-1.png){width=768}\n:::\n:::\n\n\n\n\n\n:::\n\nFrom this plot, with the true population mean overlaid, we can see that most of the confidence intervals are managing to capture that true value. But a small proportion aren't.\n\nWhat proportion of the intervals are managing to capture the true population mean?\n\nWe can check like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(mean_n <= intervals$upper & mean_n >= intervals$lower)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncontains_true = (intervals['lower'] <= mean) & (intervals['upper'] >= mean)\ncontains_true.mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.95\n```\n\n\n:::\n:::\n\n\n\n:::\n\nGiven that we set our confidence intervals at the 95% level, this is exactly what the simulation should reveal: approximately (in this case, exactly) 95 of our 100 confidence intervals contain the true population mean.\n\n::: {.callout-warning}\n#### The confidence is about the intervals, not the parameter\n\nThe very definition of 95% confidence intervals is this: we expect that the confidence intervals from 95% of the samples drawn from a given population with a certain parameter, to contain that true population parameter.\n\nThis is *not* equivalent to saying that there is a 95% chance that the true population value falls inside a given interval. This is a common misconception, but there is no probability associated with the true population value - it just is what it is (even if we don't know it).\n\nAs with p-values, the probability is associated with datasets/samples when talking about confidence intervals, not with the underlying population.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n### Calculating confidence intervals manually\n\nFor those of you who are curious about the underlying mathematical formulae for confidence intervals, and how to calculate them manually, it's done like so:\n\n1.    Calculate the sample mean\n2.    Calculate the (estimated) standard error of the mean\n3.    Find the t-score* that corresponds to the confidence level (e.g., 95%)\n4.    Calculate the margin of error and construct the confidence interval\n\n*You can use z-scores, but t-scores tend to be more appropriate for small samples.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nLet's start by simulating a simple dataset.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\nn <- 40\nmean_n <- 6\nsd_n <- 1.5\n\ndata <- rnorm(n, mean_n, sd_n)\n```\n:::\n\n\n\n\n#### Step 1: Calculate the sample mean\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_mean <- mean(data)\nprint(sample_mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.163116\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 2: Calculate the standard error of the mean\n\nWe do this by dividing the sample standard deviation by the square root of the sample size, $\\frac{s}{\\sqrt{N}}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_se <- sd(data)/sqrt(n)\nprint(sample_se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2549382\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 3: Calculate the t-score corresponding to the confidence level\n\nThis step also gives a clue as to how the significance threshold (or $\\alpha$) is associated with confidence level (they add together to equal 1).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\n\nsample_df <- n - 1\n\nt_score = qt(p = alpha/2, df = sample_df, lower.tail = FALSE)\nprint(t_score)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.022691\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 4: Calculate the margin of error and construct the confidence interval\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many standard deviations away from the mean, is the margin of error?\nmargin_error <- t_score * sample_se\n\n# Calculate upper & lower bounds around the mean\nlower_bound <- sample_mean - margin_error\nupper_bound <- sample_mean + margin_error\n\nprint(c(lower_bound,upper_bound))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.647455 6.678777\n```\n\n\n:::\n:::\n\n\n\n\nIf we compare that to what we would've gotten, if we'd used a function to do it for us:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  data.frame() %>%\n  lm(data = ., formula = . ~ 1, ) %>%\n  confint(level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 5.647455 6.678777\n```\n\n\n:::\n:::\n\n\n\n\n... we can indeed see that we get exactly the same values.\n\n## Python\n\nLet's start by simulating a simple dataset.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(20)\n\nn = 40\nmean = 6\nsd = 1.5\n\ndata = np.random.normal(mean, sd, n)\n```\n:::\n\n\n\n\n#### Step 1: Calculate the sample mean and standard deviation\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsample_mean = np.mean(data)\nsample_sd = np.std(data)\nprint(sample_mean, sample_sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5.791355210452416 1.69762282725244\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 2: Calculate the standard error of the mean\n\nWe do this by dividing the sample standard deviation by the square root of the sample size, $\\frac{s}{\\sqrt{N}}$.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsample_se = sample_sd/np.sqrt(n)\nprint(sample_se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.26841773710061373\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 3: Calculate the t-score corresponding to the confidence level\n\nThis step also gives a clue as to how the significance threshold (or $\\alpha$) is associated with confidence level (they add together to equal 1).\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import t\n\nalpha = 0.05\nsample_df = n-1\n\nt_crit = t.ppf(1-alpha/2, sample_df)\nprint(t_crit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2.022690911734728\n```\n\n\n:::\n:::\n\n\n\n\n#### Step 4: Calculate the margin of error and construct the confidence interval\n\nFirst, we find the margin of error: how many standard deviations away from the mean is our cut-off?\n\nThen, we use that to find the upper and lower bounds, around the mean.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmargin_error = t_crit * sample_se\n\nci = (sample_mean - margin_error, sample_mean + margin_error)\n\nprint(ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(5.248429093070603, 6.334281327834229)\n```\n\n\n:::\n:::\n\n\n\n\nIf we compare that to what we would've gotten, if we'd used a function to do it for us:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsms.DescrStatsW(data).tconfint_mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(5.241512557961966, 6.341197862942866)\n```\n\n\n:::\n:::\n\n\n\n\n... we can indeed see that we get the same interval, plus or minus some tiny differences in numerical precision from the different functions used.\n:::\n\nIf you think about things in a maths-ier way, it can be helpful to know how something is calculated - but you will probably always use existing functions when actually coding this stuff!\n\n:::\n\n## Exercises\n\n### Width of confidence intervals\n\nThere are multiple factors that will affect the width of confidence intervals.\n\nIn this exercise, you'll test some of them, to get an intuition of how (and hopefully why).\n\n::: {.callout-exercise}\n\n\n\n\n{{< level 1 >}}\n\n\n\n\n\n\nUse the code in @sec-exm_loop-confint as a starting point.\n\nVary the following parameters, and look at the impact on the width of the confidence intervals:\n\n-   The sample size of each individual sample\n-   The standard deviation of the underlying population\n-   The confidence level (e.g., 95%, 99%, 50%)\n\nThink about the following questions:\n\n-   Does the confidence interval get wider or narrower as these parameters increase? Why?\n-   What would happen (theoretically) if we set our desired confidence level to 100%, or our sample size to $n = \\infty$?\n\n::: {.callout-tip}\n#### Pay attention to the y-axis!\n\nIf you keep the same seed while changing other parameters, you might get a series of plots that look identical. \n\nBut if you look more closely at the y-axis, you will sometimes notice the scale changing.\n\nTo combat this, you can manually set the y-axis limits, if you'd like.\n:::\n\n:::\n\n### t-statistic under CLT {#sec-exr_tstat-CLT}\n\nAll statistics obey the central limit theorem. This includes not just descriptive statistics like the mean, median, standard deviation etc., but the test statistics that we use for hypothesis testing.\n\n::: {.callout-exercise}\n\n\n\n\n{{< level 2 >}}\n\n\n\n\n\n\nTo demonstrate this to yourself, generate 1000 t-statistics from one-sample t-tests, and plot them on a histogram.\n\n-   What happens to the distribution as you change `mu`?\n-   How does the distribution of 1000 t-statistics, compare to the t-statistic distribution? Why are they different?\n\nYou can refer back to the code in @sec-exm_mean-CLT to help you.\n\n::: {.callout-tip collapse=\"true\"}\n#### Code tips\n\nIf you're struggling to extract the t-statistics, you might find the below code snippets to be helpful as tips/hints!\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nIf you're using the base R `t.test` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.result <- t.test(rnorm(n, mean_n, sd_n), mu = 3)\n  \nt.values[i] <- unname(t.result$statistic)\n```\n:::\n\n\n\n\nThis first method is probably quicker/easier.\n\nIf you're using `t_test` from `rstatix` (which you are likely familiar with, if you took the Core statistics course before this one):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The data must be saved as a dataframe to use t_test\ndata <- rnorm(n, mean_n, sd_n) %>%\n  data.frame()\n\nt_result <- t_test(data, .~1)\n\nt_values[i] <- unname(t_result$statistic)\n```\n:::\n\n\n\n\nIf you're trying to sample from the t-distribution, note that the function requires different parameters - specifically, we specify the degrees of freedom `df` (and optionally, a non-centrality parameter `ncp`):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt(n = 1000, df = 99)\n```\n:::\n\n\n\n\n## Python\n\nThere are a few different functions for running t-tests in Python. If you took the Core statistics course, you're likely familiar with the `ttest` function from `pingouin`.\n\nFor ease of use in this exercise, however, it's easier to extract the t-statistic on each loop by indexing the output from the `ttest_mean` from `statsmodels`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsms.DescrStatsW(data).ttest_mean()[0]\n```\n:::\n\n\n\n\nTo sample from the t-distribution, use `np.random.standard_t`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.standard_t(df = 99, size = 1000)\n```\n:::\n\n\n\n\nYou will need to provide two arguments: the degrees of freedom `df`, and the sample `size`.\n:::\n:::\n:::\n\n### Distributions all the way down {#sec-exr_multilevel-CLT}\n\nThose of you with imagination might be wondering: if one for loop can construct a single histogram, giving us the distribution of the set of sample means, why can't we run multiple for loops and look at the distribution of the set of means of the set of means?\n\nThe short answer is: we can!\n\n![It's distributions all the way down; [image source](https://en.wikipedia.org/wiki/Turtles_all_the_way_down)](images/turtles.jpg)\n\n::: {.callout-exercise}\n\n\n\n\n{{< level 3 >}}\n\n\n\n\n\n\nYour mission, should you choose to accept it, is to plot a single histogram that captures the distribution of the set of means of the set of means.\n\nAdapt the code in @sec-exm_mean-CLT by nesting one for loop inside the other, to produce a single histogram. It should represent the set of means of the set of sample means.\n\nBefore you do, consider:\n\n-   What shape will the histogram take, if you run enough iterations?\n-   If someone asks what you did on this course, how on earth will you explain *this* to them?!\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\nTo make the code a little easier to parse and eliminate any clashes in the environment, the outside for loop uses `j` for indexing instead of `i`.\n\nWe've also kept the number of iterations low-ish. You might notice, if you use a large value like 1000 for each of the loops, it takes a while to run (because you're actually asking for 1000000 total iterations!)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans_j <- c()\n\nfor (j in 1:300) {\n\n  means_i <- c()\n\n  for (i in 1:300) {\n  \n    n <- 20\n    min_n <- 1\n    max_n <- 7\n  \n    means_i[i] <- mean(runif(n, min_n, max_n))\n\n  }\n  \n  means_j[j] <- mean(means_i)\n  \n}\n\nhist(means_j)\nabline(v = mean(means_j), col = \"red\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/central limit nested-1.png){width=672}\n:::\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmeans_j = []\n\nfor j in range(300):\n  \n  means_i = []\n  \n  for i in range(300):\n    n = 20\n    lower = 1\n    upper = 7\n    imean = np.random.uniform(lower, upper, n).mean()\n    means_i.append(imean)\n  \n  jmean = np.mean(means_i)\n  means_j.append(jmean)\n  \nplt.clf()\nplt.hist(means_j)\nplt.axvline(x = np.mean(means_j), color = 'r')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](02-statistical-concepts_files/figure-html/CLT nested loops-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nIt's *still* normal. If this is what you guessed, well done.\n\nIf you were to take this further, and add an infinite number of nested loops, you'd get a normal distribution each time.\n\nAnd it doesn't even matter what distribution we sample our individual datasets from, as we showed above. In this worked example code we've used the uniform distribution, but you can try swapping it out for any other distribution - you'll still get a normal distribution at each \"layer\", which gets clearer and clear the more iterations you run.\n\nNow, statistics may not be considered the \"coolest\" subject in the world - but I think that's pretty awesome, don't you?\n:::\n:::\n\n## Summary\n\nSimulation is a great way to help get your head around more difficult or abstract statistical concepts, without needing to worry about the mathematical formulae.\n\nIf you're ever keen to test an intuition or query, you don't need to puzzle through a bunch of probability theory. You can just put together a simulation and look at the outcome.\n\n::: {.callout-tip}\n#### Key Points\n\n-   We are more likely to accurately and precisely \"recover\" the true population parameters when our sample is large and unbiased\n-   The central limit theorem shows that, across a number of samples, the set of estimates of a given parameter will follow an approximately normal distribution \n  +   When the number of samples = $\\infty$, it will be perfectly normal\n  +   This is true regardless of the original distribution that the individual samples come from\n-   Confidence intervals don't tell us about the probability of the true parameter value; instead, they tell us about the probability of our intervals in a given dataset containing the true value\n:::\n\n\n",
    "supporting": [
      "02-statistical-concepts_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}